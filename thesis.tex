%@+leo-ver=5-thin
%@+node:gcross.20110314174620.1274: * @file thesis.tex
%@@language latex

%@+<< Prelude >>
%@+node:gcross.20110314174620.1275: ** << Prelude >>
\documentclass{book}

\usepackage{cite}
%@-<< Prelude >>

\begin{document}

%@+others
%@+node:gcross.20110314174620.1276: ** Introduction
\chapter{Introduction}

When physicists discovered the laws of quantum mechanics, they were both excited and disappointed.  On the one hand, this new theory did a fantastic job of modeling all of the bizarre microscopic phenomena that they had been observing in ther labs.  But on the other hand, the theory was fundamentally \emph{nondeterministic}, postulating that reality was generally not in a single observable state but rather existed in many states at once; upon measurement one state is selected at random and the rest discarded.  This made many physicists uncomfortable, as most famously expressed by Albert Einstein:

\begin{quote}
Quantum mechanics is certainly imposing. But an inner voice tells me that it is not yet the real thing. The theory says a lot, but does not really bring us any closer to the secret of the `old one.' I, at any rate, am convinced that \emph{He is} not playing at dice{\cite{Born2004}}.
\end{quote}

In addition to this, the nondeterministic nature of quantum mechanics necessarily implies that in the general case the amount of information needed to describe a system grows \emph{exponentially} with the number of parts in the system;  this is because the addition of each part \emph{multiplies} the total number of possible states for the system by the number of possible states for the part, and in general each state in the system will have an independent non-trivial amplitude assigned to it.  This is in stark contrast with classical (deterministic\footnote{It is worth noting that although classical systems are fundamentally deterministic, it is often useful to use non-deterministic models to describe them.  For example, in the theory of thermodynamics one models systems with very large numbers of particles and so in order to make the theory tractible it is necessary to use statistical models of the behaviour of the system as a whole rather than modeling the state of every individual particle in the system.}) systems which only require an amount of information that grows \emph{linearly} with the number of parts in the system.  The exponential information needed to represent quantum systems in general makes it intractible to use relatively straight-forward methods to model systems with non-trivial size, which can make it very difficult to study such systems.

Fortunately, in the last few decades there has been an increasing appeaciation that the supposed difficulties introduced by quantum mechanics can be reinterpreted as \emph{features} that could potentially be harnessed to perform some computations much faster than classical computers, turning the proverbial glass half-empty into a glass half-full.  One of the earliest people to bring up the possibility of a quantum computer was Richard Feynman, though his interest was mainly in a machine that could accurately simulate quantum systems rather than a general-purpose machine\cite{springerlink:10.1007/BF02650179}.  David Deutsch (often dubbed the ``father of quantum computing'') is typically credited as the first person to demonstrate that quantum computers could be advantagous for general computations by providing the first example of a problem that a quantum computer can intrinsically solve using fewer operations (specifically, queries to an binary function) than a classical computer\cite{Deutsch08071985}.  The improvement of his quantum algorithm over the classical algorithm was not very impressive --- one operation instead of two --- but a generalization of the algorithm by himself and Richard Jozsa (and hence known as the \emph{Deutsch-â€“Jozsa} algorithm) demonstrated a more respectable \emph{exponential} speedup over the classical algorithm\cite{Deutsch1992}.  This speedup was only present, however, if one was unwilling to accept any chance of getting a wrong answer;  if one were willing to tolerate an arbitrarily small chance of failure, then there is a randomized algorithm for the classical computer over which the quantum computer only provides a \emph{constant} speedup, so it was not yet clear whether quantum computers truly were able to offer an advantage in performance over classical computers.  Ethan Bernstein and Umesh Vazirani finally settled this open question by providing an example of an problem where a quantum computer offered a super-polynomial ($O(n^{\log n})$) speedup over even a randomized classical computer with a chance of failure\cite{Bernstein:1993:QCT:167088.167097};  Daniel Simon later improved on this bound by presenting a problem where the speedup was \emph{exponential}\cite{10.1109/SFCS.1994.365701}.

Up to this point the speedups that a quantum computer were able to offer existed only in artifical problems that had been carefully contrived to exhibit such speedups, and so quantum computers were relegated to the status of being little more than an academic curiosity.  This changed dramatically when Peter Shor discovered a quantum algorithm for factoring integers that is exponentially faster than the best known classical algorithm, which is an important application that has significant repricussions for the modern cryptosecurity infrastructure.  Now it had finally been shown that quantum computers could solve problems of practical interest, and suddenly there was a great deal of interest\footnote{especially in the cryptosecurity communities, of course} in figuring out whether and how a quantum computer could be built.  However, building a quantum computer is a very difficult task, and so even today the largest quantum computers can only compute using a handful of quantum bits (qubits).

There are many challanges posed in building a quantum computer, but at the core of the problem are two fundamental ways in which quantum information differs from classical information.  First, quantum information cannot be observed nondestructively;  measuring the value of a qubit (in the computational basis) causes the qubit to stop existing as a superposition of 0 and 1 and instead to pick either 0 or 1 at random.  Second, quantum information cannot be copied --- that is, although it is possible to initialize many quantum systems into a known state, it is not possible to take a system in an unknown state and to ``clone'' that state exactly in additional systems\footnote{This result is known as the No-Cloning Theorem, and is a consequence of the requirement that the evolution of quantum systems is restricted to the action of unitary operators\cite{Wootters1982}.}.  These two differences deprive us of the ability to draw upon most of the tools that we use in modern electronics to robustly store and manipulate classical information;  for example, we can't use a straightforward generalization of dynamic random-access memory (DRAM) for quantum information because we can't perform a continuous measure-and-amplify procedure without collapsing the qubit, and we can't use a straightforward generalization of hard drives because we can't store a qubit in a bulk of matter by copying a representation of its value into every particle in the bulk.  Thus, in order to robustly store and manipulate quantum information, we will need to develop methods that are more advanced than mere straightforward generalizations of our classical tools.
%@+node:gcross.20110316123044.1278: ** Bibliography
\bibliography{thesis}
\bibliographystyle{plain}
%@-others

\end{document}
%@-leo
