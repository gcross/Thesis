%@+leo-ver=5-thin
%@+node:gcross.20110314174620.1274: * @file thesis.tex
%@@language latex

%@+<< Prelude >>
%@+node:gcross.20110314174620.1275: ** << Prelude >>
\documentclass[12pt]{amsbook}

\usepackage{
    amsmath,
    amssymb,
    amsthm,
    booktabs,
    cite,
    clrscode,
    comment,
    graphicx,
    mathrsfs,
    subfig
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newcommand{\lst}{\vec}
\newcommand{\set}{\tilde}

\newcommand{\genfun}{\tilde{\mathcal{G}}}
\newcommand{\pauligroup}{{\set{\mathfrak{P}}}}
\newcommand{\powerset}{\set{\mathcal{P}}}
\newcommand{\centralizer}{\set{\mathcal{C}}}
\newcommand{\pseudoproduct}{\set\Pi}
\newcommand{\unpack}{\set U}
\newcommand{\optimizer}{\lst{\mathcal{O}}}

\newcommand{\bmat}[4]{
\begin{bmatrix}
#1 & #2\\
#3 & #4\\
\end{bmatrix}
}
\newcommand{\cip}[2]{\left<#1|#2\right>}
\newcommand{\coip}[3]{\left<#1\left|#2\right|#3\right>}
\newcommand{\eqn}[2][]{\begin{equation}\label{#1}#2\end{equation}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\ket}[1]{\left|#1\right>}
\newcommand{\bra}[1]{\left<#1\right|}
\newcommand{\ketbra}[2]{\ket{#1}\!\!\bra{#2}}
\newcommand{\om}{\omega}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\tr}{\text{tr}\,}

\newcommand{\mexp}[1]{\exp{\mathcal{#1}}}
%@-<< Prelude >>

\begin{document}

%@+others
%@+node:gcross.20110318151522.1291: ** Title Page
\begin{center}

\textsc{\LARGE Thesis Title}\\[1.5cm]

\textsc{\Large Gregory Milton Crosswhite}\\[1.5cm]

A thesis submitted in partial fulfillment of the requirements for the degree of\\[1.5cm]

Doctorate of Philosophy in Physics\\[1.5cm]

University of Washington\\[1.5cm]

2011\\[1.5cm]

Program Authorized to Offer Degree:
Physics Department

\end{center}

\newpage
%@+node:gcross.20110318151522.1295: ** Table of Contents
\tableofcontents
%@+node:gcross.20110318151522.1293: ** List of Tables
\listoftables
%@+node:gcross.20110314174620.1276: ** Introduction
\part{Introduction}

When physicists discovered the laws of quantum mechanics, they were both excited and disappointed.  On the one hand, this new theory did a fantastic job of modeling all of the bizarre microscopic phenomena that they had been observing in ther labs.  But on the other hand, the theory was fundamentally \emph{nondeterministic}, postulating that reality was generally not in a single observable state but rather existed in many states at once; upon measurement one state is selected at random and the rest discarded.  This made many physicists uncomfortable, as most famously expressed by Albert Einstein:

\begin{quote}
Quantum mechanics is certainly imposing. But an inner voice tells me that it is not yet the real thing. The theory says a lot, but does not really bring us any closer to the secret of the `old one.' I, at any rate, am convinced that \emph{He is} not playing at dice{\cite{Born2004}}.
\end{quote}

In addition to this, the nondeterministic nature of quantum mechanics necessarily implies that in the general case the amount of information needed to describe a system grows \emph{exponentially} with the number of parts in the system;  this is because the addition of each part \emph{multiplies} the total number of possible states for the system by the number of possible states for the part, and in general each state in the system will have an independent non-trivial amplitude assigned to it.  This is in stark contrast with classical (deterministic\footnote{It is worth noting that although classical systems are fundamentally deterministic, it is often useful to use non-deterministic models to describe them.  For example, in the theory of thermodynamics one models systems with very large numbers of particles and so in order to make the theory tractible it is necessary to use statistical models of the behaviour of the system as a whole rather than modeling the state of every individual particle in the system.}) systems which only require an amount of information that grows \emph{linearly} with the number of parts in the system.  The exponential information needed to represent quantum systems in general makes it intractible to use relatively straight-forward methods to model systems with non-trivial size, which can make it very difficult to study such systems.

Fortunately, in the last few decades there has been an increasing appeaciation that the supposed difficulties introduced by quantum mechanics can be reinterpreted as \emph{features} that could potentially be harnessed to perform some computations much faster than classical computers, turning the proverbial glass half-empty into a glass half-full.  One of the earliest people to bring up the possibility of a quantum computer was Richard Feynman, though his interest was mainly in a machine that could accurately simulate quantum systems rather than a general-purpose machine\cite{springerlink:10.1007/BF02650179}.  David Deutsch (often dubbed the ``father of quantum computing'') is typically credited as the first person to demonstrate that quantum computers could be advantagous for general computations by providing the first example of a problem that a quantum computer can intrinsically solve using fewer operations (specifically, queries to an binary function) than a classical computer\cite{Deutsch08071985}.  The improvement of his quantum algorithm over the classical algorithm was not very impressive --- one operation instead of two --- but a generalization of the algorithm by himself and Richard Jozsa (and hence known as the \emph{Deutsch-â€“Jozsa} algorithm) demonstrated a more respectable \emph{exponential} speedup over the classical algorithm\cite{Deutsch1992}.  This speedup was only present, however, if one was unwilling to accept any chance of getting a wrong answer;  if one were willing to tolerate an arbitrarily small chance of failure, then there is a randomized algorithm for the classical computer over which the quantum computer only provides a \emph{constant} speedup, so it was not yet clear whether quantum computers truly were able to offer an advantage in performance over classical computers.  Ethan Bernstein and Umesh Vazirani finally settled this open question by providing an example of an problem where a quantum computer offered a super-polynomial ($O(n^{\log n})$) speedup over even a randomized classical computer with a chance of failure\cite{Bernstein:1993:QCT:167088.167097};  Daniel Simon later improved on this bound by presenting a problem where the speedup was \emph{exponential}\cite{10.1109/SFCS.1994.365701}.

Up to this point the speedups that a quantum computer were able to offer existed only in artifical problems that had been carefully contrived to exhibit such speedups, and so quantum computers were relegated to the status of being little more than an academic curiosity.  This changed dramatically when Peter Shor discovered a quantum algorithm for factoring integers that is exponentially faster than the best known classical algorithm, which is an important application that has significant repricussions for the modern cryptosecurity infrastructure.  Now it had finally been shown that quantum computers could solve problems of practical interest, and suddenly there was a great deal of interest\footnote{especially in the cryptosecurity communities, of course} in figuring out whether and how a quantum computer could be built.  However, building a quantum computer is a very difficult task, and so even today the largest quantum computers can only compute using a handful of quantum bits (qubits).

There are many challanges posed in building a quantum computer, but at the core of the problem are two fundamental ways in which quantum information differs from classical information.  First, quantum information cannot be observed nondestructively;  measuring the value of a qubit (in the computational basis) causes the qubit to stop existing as a superposition of 0 and 1 and instead to pick either 0 or 1 at random.  Second, quantum information cannot be copied --- that is, although it is possible to initialize many quantum systems into a known state, it is not possible to take a system in an unknown state and to ``clone'' that state exactly in additional systems\footnote{This result is known as the No-Cloning Theorem, and is a consequence of the requirement that the evolution of quantum systems is restricted to the action of unitary operators\cite{Wootters1982}.}.  These two differences deprive us of the ability to draw upon most of the tools that we use in modern electronics to robustly store and manipulate classical information;  for example, we can't use a straightforward generalization of dynamic random-access memory (DRAM) for quantum information because we can't perform a continuous measure-and-amplify procedure without collapsing the qubit, and we can't use a straightforward generalization of hard drives because we can't store a qubit in a bulk of matter by copying a representation of its value into every particle in the bulk.  Thus, in order to robustly store and manipulate quantum information, we will need to develop methods that are more advanced than mere straightforward generalizations of our classical tools.

In Part 1 of this thesis, we discuss one such method known as \emph{quantum subsystem codes}.  The basic idea is to look to classical error-correcting codes for inspiration, and in particular codes that are based on parity measurements since they allow us to learn about whether an error has occurred in the stored information and how to fix it without gaining any knowledge about the encoded information itself.  Unfortunately, it can take a great deal of mental work to find measurements that not only have good error-correcting properties but also can realistically be engineered.  The main result presented in Part 1 is a technique for automating much of this mental work by way of an algorithm that computes the error-correcting properties of the code implemented by a given set of measurements;  this algorithm can be applied to perform a (relatively) fast brute-force search through spaces of choices of measurements that can be realistically engineered, so that the output consists of choices of measurements that have both of the desired properties.

Of course, even better than having to constantly perform an error-correction procedure would be to engineer a system where there was a physical barrier against errors occuring.  This is the idea behind \emph{ground state quantum computing}, first proposed by Ari Mizel, M. W. Mitchell, and Marvin L. Cohen~\cite{PhysRevA.63.040302}.  The idea is to construct a Hamiltonian whose ground state is a representation of the result of the desired calculation, so that the computation can be performed by annealing the system to very low temperatures and then performing a measurement.  The biggest problem with this approach, though, is that the act of annealing is not guaranteed to bring a system into its ground state since it might get stuck in an excited state which has no way of transitioning directly to a lower energy state.  This is solved by \emph{adiabatic quantum computing}, which starts with a Hamiltonian which has the property that cooling down to the ground state is easy (i.e., there are no forbidden transitions that could cause it to get stuck) and then proceeds by adiabatically evolving the original Hamiltonian to a new Hamiltonian whose ground state represents the result of the calculation;  this idea was introduced by Edward Farhi et al.~\cite{Farhi2000} to provide a quantum algorithm to solve the classical Boolean Satisfiability, and was shown to provide a basis for \emph{universal} quantum computation by Dorit Aharonov et al.~\cite{Aharonov2007}.  The fact that the evolution is \emph{adiabatic} (that is, taking place so slowly that the system barely notices that anything is changing) is what provides the guarantee that the system will end in the ground state of the new Hamiltonian rather than in an excited state;  this result is known as the Adiabatic Theorem~\cite{JPSJ.5.435}.

For both ground state quantum computing and adiabatic quantum computing, the key quantity that determines the speed and robustness of the computation is the energy gap between the ground state and the first excited state;  the larger this gap, the less time needed to perform the computation and the smaller the chance of an error.  Thus, it is important to be able to compute this quantity in order to evaluate the viability of a particular ground state or adiabatic computing scheme.  Although in principle one might think that this were a simple task since it is nothing more than the problem of computing eigenvalues, in practice this is actually very difficult to do because in general the dimension of the linear state space grows \emph{exponentially} with the size (i.e., the total number of parts, particles, or subsystems) in a quantum system.  This means that one cannot simply write down the Hamiltonian in matrix from and feed it into a standard eigensolver, but instead one has to be more clever.  In particular, one needs to employ a representation of states and operators that are exponentially more compact than the naive representation.  Of course,  this condition is necessary but not sufficient;  for example, the sentence ``the ground state of the Hamiltonian'' satisfies it, but obviously this does not provide us with useful information;  we also need our representation to have the property that we can compute its average energy in order to get an estimate of the energy eigenvalue.  Futhermore, the fact that it is possible to represent the ground state using a given scheme is not helpful unless we can actually \emph{find} this respresentation, so our representation has to be amenable to some numerical method that allows us to fit it to the ground and lowest lying excited states.

In Part 2 of this thesis, we discuss a representation that meets all of these criteria known as ``matrix product states'' that have been shown to work very well in practice.  We shall present our own important insight that they are deeply connected to the formalism of \emph{finite state automata} (a fundamental construction in formal language theory) and show how this connection can be leveraged to simulate non-trivial Hamiltonians, including those with long-range interactions.  Finally, we shall present a simulation code that was written using these ideas, and show how it has been used to compute energy gaps in systems relevent to ground state and adiabatic computing, hence giving us insight into the viability of these methods.
%@+node:gcross.20110318151522.1287: ** CodeQuest
\part{CodeQuest}
\label{part:CodeQuest}
%@+node:gcross.20110318151522.1297: *3* Prelude
Quantum computers are a technological possibility because there exist methods for building these computers out of physical components that fail to operate in an error-free manner.  The theory behind achieving this makes up the field of quantum error correction~\cite{Shor:95a,Steane:96a,Steane:96b,Steane:96c,Knill:97a,Gottesman:97a} and fault-tolerant quantum computing~\cite{Shor:96a,Aharonov:97a,Knill:98a,Knill:98b,Preskill:98a,Aliferis:05a}. Of particular note is the threshold theorem for fault-tolerant quantum computing~\cite{Aharonov:97a,Knill:98a,Knill:98b,Aliferis:05a}.  This theorem says that if a quantum system decoheres slowly enough, and sufficiently precise control is maintained over the system, then effectively arbitrary error-free quantum computations can be performed.  The way that this is achieved is through the use of quantum information which is encoded across multiple quantum subsystems into a quantum error correcting code.

Different quantum codes have different advantages and disadvantages for implementation in a fault-tolerant device~\cite{Cross:07a}.  In this paper we undertake a study of an important class of quantum codes, quantum stabilizer subsystem codes~\cite{Poulin:05a,Kribs:05a,Kribs:05b,Kribs:06a} generated by measurements that are tensor products of Pauli operators.  Part of the significance of this class of codes is that they can be used to implement \emph{passive} fault tolerance by turning the measurement operators into interaction terms forming a Hamiltonian that provide energetic protection against errors;  the first example of such an approach was the toric code and related models due to Kitaev~\cite{Kitaev:97c,Kitaev:03a}, and a plethora of related approaches have now been investigated~\cite{Barnes:00a,Bacon:01b,Jordan:05a,Weinstein:05b,Bacon:06a,Bacon:08b,Nayak:08a,Bombin:09a,Chesi:10a}.

Previous approaches for studying quantum subsystem codes have focused on using theoretical analysis to find and investigate new quantum subsystem codes.  While powerful, theoretical analysis has some disadvantages:  it is limited to the `cleverness' of the analyst, and it can be prohibitively expensive to perform systematic searches of large parameter spaces to pick out the gems in the dust.  In this paper, we present an alternative approach that uses \emph{computational} analysis to accomplish the same goals.  The advantage of this approach is that one becomes limited by the power of the computer rather than the brain of analyst\footnote{Of course, this is also the main \emph{disadvantage} of this approach.}.

In this part we present an algorithm that computes the optimal subsystem code for a given set of measurements consisting of tensors products of Pauli operators.  In the process of doing this we also develop a formalism that allows us to prove that the algorithm is correct and that the code it compute is indeed the optimal code for the given measurements.  We also prove bounds on the running time of the algorithm that show that the algorithm terminates (relatively) quickly when the optimal code is not very robust to errors.  Because of this property, the algorithm can be applied to sift through a class of possible measurements to determine which (if any) result in a robust code.

To demonstrate the use of this algorithm, we focus on classes of measurement operators where each measurement is limited in action to two qubits --- that is, to operators taking the form $P_i \cdot Q_j$, where $P_i$ and $Q_j$ are Pauli operators acting on respectively the $i^{\text{th}}$ and $j^{\text{th}}$ qubit of the system;  examples of previous subsystem codes that have been constructed with this structure are the quantum compass model subsystem code~\cite{Bacon:06a} (including generalizations~\cite{Bacon:06b,Bravyi:10a}) and topological subsystem codes~\cite{Bombin:10a}.  In particular we focus on systems where the measurement operators only couple qubits that are neighboring on a periodic lattice arising from the convex uniform tilings of the plane.  We perform a systematic study of the codes on lattices arising from nine of the eleven such tilings, and present the results of this search.
%@+node:gcross.20110318151522.1300: *3* Introduction
\chapter{Introduction}

We begin by a brief review of the notion of quantum error correcting codes and in particular the subsystem stabilizer codes~\cite{Poulin:05a}.

In quantum computation we seek to reliably store and manipulate quantum information.  Unfortunately, real quantum systems are open systems that couple to their environment and quickly lose their coherence through the process of decoherence.  Even more troubling, when one wishes to manipulate quantum information one can only do this with a fixed precision, which means that additional error is introduced at every step of the computation.  While considerable progress has been made in finding systems with long coherence times, inevitably current quantum computers will fail before they achieve anything close to the amount of computation needed, for example, to break a public key cryptosystem~\cite{Shor:94a}.  However it turns out that one can generally repair damage to quantum information as long as one knows the form that the damage took.  Furthermore one can build a `trap' --- that is to say, a
\emph{quantum code} --- that tricks nature into giving up the information about what damage has occurred to the quantum system.

The nature of codes is that they separate the space in which our computation lives from the space in which the physical information is
stored; that is to say, although we design our quantum circuits to operate on some Hilbert space of qubits $\mathscr{C}$, each of these qubits
does \emph{not} directly correspond to a physical qubit, but rather there is some isomorphism that relates the entire Hilbert space $\mathscr{C}$
to the Hilbert space of physical qubits, $\mathscr{P}$.  To distinguish between these two Hilbert spaces, we call the Hilbert space of qubits in whose terms the computation is expressed the \emph{computational space} (or \emph{logical space}), and the space of qubits which have physically been built the
\emph{physical space}.

Merely building an isomorphism between these two spaces is not enough to allow us to correct errors.  For one thing, we need to add extra qubits to the computational space that contain a record of the damage that we can read out; thus, we shall say that the full computational space is $\mathscr{C}:=\mathscr{R}\times\mathscr{Q}$, where the qubits that live in $\mathscr{R}$ have the role of keeping a
record of the errors that have been introduced by the environment, and the qubits that live in $\mathscr{Q}$ are the qubits in whose terms our quantum
algorithm is expressed.  

We have to pick a strategy for reading out the information in $\mathscr{R}$ about the errors that have occurred on our system.  One natural choice is to perform a single-qubit Pauli $Z$ operator measurement on each qubit on $\mathscr{R}$.  In order to build the `trap' element into our system, we need to ensure that whenever nature strikes at the physical space $\mathscr{P}$ and produces errors in a form that we intend to correct, this action must be isomorphic to a strike on the computational space that leaves a \emph{measurable} record in $\mathscr{R}$.  For our choice of measuring Pauli $Z$ errors, these are errors that are isomorphic to any operator that \emph{anti-commutes} with the $Z$ operator of at least one of the qubits in $\mathscr{R}$.  Note that although we speak of measuring the qubits in $\mathscr{R}$, the measurement operator of interest in $\mathscr{R}$ is mapped to an operator in the physical space $\mathscr{P}$; this isomorphic operator is referred to as a \emph{stabilizer}, and the full set of operators on $\mathscr{P}$ which are isomorphic to our chosen measurement operators on $\mathscr{R}$ are referred to as the \emph{stabilizers} of the code.

Up to this point, the formalism we have described is known as \emph{stabilizer codes}~\cite{Gottesman:96a,Gottesman:97a,Calderbank:97a,Calderbank:97b} and its essential characteristic is that in determining the syndrome of the physical error, one makes a measurement of all of the qubits in $\mathscr{R}$. What if, however, we relaxed this constraint and only measured some of the qubits in $\mathscr{R}$?  That is to say, what if we split the qubits in $\mathscr{R}$ into two categories: \emph{stabilizer
qubits} whose states we care about and which we measure to obtain an error syndrome, and \emph{gauge
qubits} whose states we do not care about.  (The latter get their name
from the fact that they provide a `gauge' degree of freedom, i.e. a
degree of freedom that is irrelevant to us.)  Then we would have that
$\mathscr{R}=\mathscr{S}\times \mathscr{G}$, where $\mathscr{S}$ is
the space in which the stabilizer qubits live, and $\mathscr{G}$ is
the space in which the so-called gauge qubits live; such a scheme is
known as a \emph{stabilizer subsystem code}~\cite{Poulin:05a}.  In this case, we shall use the term
\emph{stabilizers} to denote the set of operators in $\mathscr{P}$ which are isomorphic to our chosen measurement operators of interest in $\mathscr{S}$.

At first there might not seem to be an advantage to this approach, since it essentially means adding qubits to our code that are
`wasted'; however, in practice subsystem codes have many advantages.  The first advantage is that since we do not care about what happens to the gauge qubits, some quantum errors on the system will neither result in detectable errors nor destroy the information in the logical qubits~\cite{Poulin:05a,Kribs:05a,Kribs:05b,Nielsen:05a,Kribs:06a,Bacon:06a}.  A second advantage is that we no longer need our error-correcting measurements on the physical system to commute with each other, as long as they all commute with the stabilizers and logical qubit operators, since then the fact that they do not commute only affects the gauge qubits, which we do not care about~\cite{Aliferis:07a}.  This sometimes allows one to effectively measure a stabilizer which is a non-trivial $k$-qubit measurement by using a series of two qubit measurements~\cite{Aliferis:07a}.  The individual measurements in this series do not commute (so they cannot be simultaneously measured), however the stabilizer syndrome can nonetheless be reconstructed from these measurements.  A third advantage arises from the fact that subsystem codes often require {\em fewer} measurements to diagnose errors than similar non-subsystem codes, which results in improved performance~\cite{Aliferis:07a,Cross:07a};  counterintuitively, turning stabilizer codes into subsystem stabilizer codes often results in higher thresholds for fault-tolerant quantum computing.  Finally, subsystem codes can often be implemented in a more local manner than non-subsystem codes as exemplified by the quantum compass model code~\cite{Bacon:06a,Aliferis:07a}.

There are now many examples of stabilizer subsystem codes in the literature.  One of the first non-trivial subsystem codes to be described is a code related to the quantum compass model in two-dimensions~\cite{Bacon:01a,Dorier:05a,Bacon:06a}.  In the quantum model one considers a Hamiltonian on a two-dimensional square lattice where nearest horizontal neighbors couple the $x$ component of their spins and nearest vertical neighbors couple the $z$ component of their spin, so that the Hamiltonian is given by
\begin{equation}
H=-\Delta \sum_{i,j} (X_{i,j} X_{i+1,j} +Z_{i,j} Z_{i,j+1}),
\end{equation}
where $P_{i,j}$ represents the Pauli operator $P$ acting on qubit at location $(i,j)$.  This model is interesting for a few reasons.  The first is that the energy levels of this system can be best thought of as elements of a quantum error correcting subsystem code.  The second reason is that the model provides some amount of protection from quantum errors because errors are energetically unfavored\footnote{Unfortunately, in this particular system the protection vanishes as the size of the lattice goes to infinity~\cite{Dorier:05a}, but for small lattice sizes there is some protection from errors due to the energy level structure of the system~\cite{Bacon:01a}.}.  Many other examples of systems which have energy protecting properties are also known: the most famous being Kitaev's toric code in two and four-spatial dimensions~\cite{Kitaev:97c,Kitaev:03a,Dennis:02a}.  The study of such systems is still in its infancy and one central question is whether there exist Hamiltonians with reasonable physical parameters (such as existing in three or fewer spatial dimensions and involving 2-body interactions~\cite{Bravyi:09a,Bravyi:10b}) whose physics enact quantum error correction on the system when the system is in contact with a thermal reservoir at sufficiently low temperature; such systems are called \emph{self-correcting} quantum computers~\cite{Bacon:06a,Bombin:09a}.  In this paper we will talk about quantum subsystem codes from the perspective of active error correction where error syndromes are identified through carefully engineered measurements, but it shall be understood that this formalism can equivalently be seen from the perspective of passive error correction where errors are guarded against by carefully engineered interactions.  That is, measurement operators in the active error correction picture are equivalent to interactions in the passive error correction picture.

Because we ultimately want to build a system implementing our measurements, physical considerations typically constrain our measurements to be \emph{local}, which means that they can be expressed in the physical space as a tensor product of single-qubit Pauli operators --- i.e, for each measurement operator $o$ we have that
$o := \bigotimes_i P_i$ where $P_i$ is the Pauli operator $P$ acting on the $i^{\text{th}}$ qubit.  An important question then is which sets of local measurements give rise to useful quantum error correcting subsystem codes.

Approaches to answering this question typically involve applying theoretical analysis with varying degrees of cleverness.  In this paper we present an alternative approach.  In section~\ref{sec:algorithm}, we present an algorithm which for every set of local measurement operators computes a quantum subsystem code that arises from the algebra of these operators\footnote{We say that we compute `a' code rather than `the' code because there is almost never a unique solution, since among other transformations one can multiple every gauge and logical qubit operator by an element from the stabilizers and end up with an equivalent code.}.  Along the way we develop a formalism that allows us to prove not only that this algorithm is correct, but also that the code that it computes is \emph{optimal} in the sense that there exists no other code arising from the same set of measurements for which the distance of any of the logical qubits has been increased.  This property makes this algorithm useful for analyzing the properties of codes arising from measurements that are too overwhelming to analyze by hand.  

We shall also show that an important property of this algorithm is that it terminates (relatively) quickly when the distance of the code is small, which allows it to be used not only to solve for individual codes, but also to search through entire classes of sets of measurements to see if any have high-distance qubits.  Motivated by previous results demonstrating the utility of codes implemented using systems on a lattice, we undertake a systematic investigation of codes where the measurement operators are restricted to the 2-body interactions arising from the edges of periodic lattices derived from the 11 regular tilings.  In section~\ref{sec:lattice} we discuss our approach for applying the algorithm to perform a systematic search for codes that can be implemented on these tilings, and we the present numerical results that we obtained. In section~\ref{sec:conclusion} we present our conclusions.
%@+node:gcross.20110318151522.1301: *4* Notation
\section{Notation}

In this paper we adopt the following conventions for notation:

\begin{itemize}
\item \emph{sets} are denoted by a symbol with a tilde, e.g. $\tilde A$;
\item \emph{sequences} are denoted by a symbol with an arrow, e.g. $\vec A$;
\item \emph{operators} and \emph{integers} are denoted by using lower-case letters, e.g. $o$ and $i$;
\item \emph{collections} of \emph{operators} and \emph{pairs of operators} are denoted by using upper-case letters with either a tilde or an arrow above them, e.g. $\tilde O$ and $\vec O$;
\item \emph{collections} of \emph{integers} are denoted by using lower-case letters with either a tilde or an arrow above them, e.g. $\tilde k$ and $\vec k$;  and
\item \emph{collections} of \emph{other kinds} of objects are typically denoted by capital letters in a fancy script.
\end{itemize}
%@+node:gcross.20110318151522.1370: *3* Theory
\chapter{Theory} \label{sec:algorithm}
%@+node:gcross.20110318151522.1371: *4* Stabilizers and gauge qubits
\section{Construction of the subsystem code}

\begin{remark}
This subsection describes by way of a constructive proof an algorithm that given a set of measurement operators computes a quantum code that can be implemented by these operators.  For a listing of pseudo-code that implements this algorithm, see Table \ref{table:algo1} near the end of this subsection.
\end{remark}
%@+node:gcross.20110318151522.1372: *5* Introduce main theorem
Although conceptually a subsystem code is an isomorphism $T$ such that  $\mathscr{P}\approx^T \mathscr{S}\times\mathscr{G}\times\mathscr{Q}$ --- that is, an isomorphism between the \emph{physical} space of qubits and the \emph{computational} space of qubits in whose terms our computation is actually expressed --- we do not need to actually construct this isomorphism in order to be able to use the code.  Since all of our work will be done on the physical system anyway, it suffices to know the operators in the physical space $\mathscr{P}$ that are isomorphic to the qubit measurement operators of interest in the computational space $\mathscr{S}\times\mathscr{G}\times\mathscr{Q}$, and it is exactly the operators on $\mathscr{P}$ that the algorithm we present shall compute\footnote{If one really wanted to, one could explicitly construct the isomorphism $\mathscr{T}$ from these operators by computing the unitary operator which simultaneously diagonalizes a the maximal subset of commuting measurements from this set of operators on $\mathscr{P}$, but in practice this is not particularly useful.}.

When one wants to define a qubit in terms of its measurement operators, it suffices to define two operators that anti-commute with each other but which commute with all of the others measurement operators that have been defined, since this gives us the $X$ and $Z$ measurements on the qubit which are sufficient to generate the full $Pauli$ group (minus phases).  Since working with such pairs of operators shall be a common theme in this algorithm, we shall introduce the following definition in order to simplify the language used to describe them.

\begin{definition} A pair of operators is a \emph{conjugal pair in relation to the set} $\set X$ when each of the operators in the pair commutes with every operator in $\set X$ except for its \emph{conjugal partner} --- that is, the other operator in the conjugal pair --- should its conjugal partner be a member of $\set X$.
\label{conjugal-pair-definition}
\end{definition}

Note that we have explicitly not required that the operators in the conjugal pair be members of $\set X$ in order to be a conjugal pair in relation to it.  However, should both operators be members of $\set X$, then neither operator can belong to a different conjugal pair with respect to $\set X$, since in that case there would be an operator in $\set X$ (namely, its original conjugal partner) with which it anti-commutes that was not its conjugal partner in the new pair, leading to a contradiction.

For convenience, we introduce the following additional definitions:

\begin{definition}

\begin{enumerate}
\item $\pauligroup$ is the group of Pauli operators --- that is, the group of tensor products of the (unnormalized) Pauli matrices --- acting on the physical space $\mathscr{P}$, \emph{modulo phases};
\item $\powerset(\set{S})$ is the power set of $\set S$, i.e. the set of all subsets of $\set S$; and
\item $\centralizer_\mathfrak{G}(\set S)$ is the centralizer of $\set S$, that is the subgroup of elements in $\mathfrak{G}$ which commute with $\set S$;
\item the function $\genfun:\powerset(\pauligroup)\to\powerset(\pauligroup)$ is defined such that $\genfun(\set S)$ is the set of all possible products of operators in $\set S$ --- that is, it is the set \emph{generated} by $\set S$.
\end{enumerate}

\end{definition}

We now introduce the main theorem of this subsection.

\begin{theorem} \label{theorem-SG} Suppose we are given a sequence of Pauli operators, $\lst O$.  Then there exist sets of Pauli operators $\set S\subseteq\pauligroup$, $\set G\subseteq\pauligroup$, and $\set L\subseteq\pauligroup$ such that
\begin{enumerate}
\item each of the operators in $\set S \cup \set G \cup \set L$ is independent from the rest --- i.e., no operator in this (unioned) set can be written as a product of other operators in the set;
\item each operator in $\set L \cup \set G$ is a member of a conjugal pair in relation to $\set S \cup \set G \cup \set L$;
\item $\genfun(\set S \cup \set G)=\genfun\paren{\{\lst O_i\}}$;\footnote{Here we use the notation $\{\vec{O}_i\}$ to refer to the set of elements in the sequence $\vec{O}$.}
\item and $\genfun(\set S \cup \set G \cup \set L)=\centralizer_\pauligroup(\set S )$
\end{enumerate}
\end{theorem}

\begin{remark}
This theorem follows, at least implicitly, from prior work on stabilizer codes~\cite{Gottesman:97a}, the definitions of stabilizer subsystem codes given by Poulin~\cite{Poulin:05a}, and the constructive approach to finding such codes as exemplified in~\cite{Bacon:06a}.  Because we wish to be constructive, however, we will present a full proof of this theorem and show how it gives rise to an algorithm for finding sets of Pauli operators which satisfy Theorem~\ref{theorem-SG}.  To be explicit, we note that $\set S$ will be a set of stabilizers (or equivalently, generators for the stabilizer group), $\set G$ will be a set of gauge qubit operators, and $\set L$ will be a set of logical qubit operators (i.e., those on which the computation is performed).

The main work in the proof of this theorem will be performed by proving several related propositions.  First we shall show how the set $\set G$ and a sequence $\lst S$ are constructed from the sequence of operators $\lst O$.  Since we want our stabilizers to form an independent set of operators, we shall then show that through a Gaussian elimination procedure it is possible to extract a list of independent operators from a sequence $\lst S$ resulting in a set $\set S$.  Finally, we shall show how using this same Gaussian elimination procedure we can transform a subset of the operators of $\set S\cup\set G$ into a form that makes it trivial to compute the logical qubit operators $\set L$.
\end{remark}
%@+node:gcross.20110318151522.1373: *5* Construction of sequences
\begin{proposition} \label{proposition-SG} Suppose that we are given a sequence of Pauli operators $\lst O\subseteq \pauligroup$.  Then there exists a sequence of Pauli operators $\lst S\subseteq\pauligroup$ and a set of Pauli operators $\set G\subseteq\pauligroup$ such that
\begin{enumerate}
\item all of the operators in $\lst S$ commute with each other and also all of the operators in $\lst G$; \label{stabs-commute-with-G}
\item each operator in $\set G$ is a member of a \emph{conjugal pair} (Definition \ref{conjugal-pair-definition}) in relation to $\{\lst S_i\} \cup \set G $ \label{conjugal-pairs-commute-with-SAG}; and
\item $\genfun\paren{\{\lst S_i\}\cup \set G}=\genfun\paren{\{\lst O_i\}}$ \label{SAG-spans-all}.
\end{enumerate}
\end{proposition}

\begin{proof}
Proof by induction.  For the base case, note that if $\lst O$ is empty then $\lst S:=\emptyset$ and $\set G:=\emptyset$ trivially satisfy all properties.

Now assume that the proposition holds for a sequence of length $n-1$, and consider a sequence of operators $\lst O$ of length $n$.  By the inductive hypothesis, we know that there is a sequence $\lst S'$ and a set $\set G'$ satisfying the properties above for the subsequence of $\lst O$ consisting of the first $n-1$ operators.  Let $o:=\lst O_n\cdot \prod_{g\in \set G, \{\lst O_n,g\}=0} \text{conj}_{\set G}(g)$ --- that is, the product of $\lst O_n$ with the conjugal partner of every operator in $\set G$ with which $\lst O_n$ anti-commutes.  This definition guarantees that $o$ commutes with every operator in $\set G$;  furthermore, we can obtain $\lst O_n$ back from $o$ since every operator in $\set G$ squares to the identity and thus $\lst O_n=o\cdot \prod_{g\in \set G, \{\lst O_n,g\}=0} \text{conj}_{\set G}(o)$; therefore we conclude that $\genfun\paren{\{\lst S'_i\} \cup \set G' \cup \{o\}}=\genfun\paren{\{\lst O_i\}}$.

If $o$ commutes with every operator in $\lst S'$, then set
$$\lst S_i :=
\begin{cases}
\lst S'_i & i \le n-1 \\
o & i = n
\end{cases}
$$
and $\set G := \set G'$, and we are done.  Otherwise, let $s$ be some operator in $\lst S'$ that anti-commutes with $o$, $\set G:=\set G'\cup \{s,o\}$
\footnote{Observe that neither $o$ nor $s$ can be present in $\set G'$ since they commute with every operator in $\set G'$, so the new set $\set G:=\set G'\cup \{s,o\}$ gives us a strictly larger set.  This fact is irrelevant far as the proof is concerned, but it has the important consequence that a computer code implementing the algorithm described by this proof can append $s$ and $o$ to a list of gauge operators and assume that this list continues to form a set (i.e., a sequence without duplicates) without having to explicitly check for this.}, $\lst S_i'' := f(\lst S'_i)$, and $\lst S$ be the subsequence of $\lst S''$ with the identity operators removed, where
$$
f(s') :=
\begin{cases}
s'\cdot s & \{s',o\}=0\\
s' & \text{otherwise}
\end{cases}.
$$
Observe that by this definition, all of the operators in $\lst S$ commute with every operator in $\set G$, so property \ref{stabs-commute-with-G} is satisfied.  Since the only difference between $\set G'$ and $\set G$ is the addition of $s$ and $o$, which form a conjugal pair with respect to $\{\lst S_i\} \cup \set G$, we conclude that property \ref{conjugal-pairs-commute-with-SAG} is satisfied.
Lastly, since $s\in \set G$, we can form any operator in $\lst S'$ with products of operators in $\lst S$ and $\set G$, so therefore $\genfun\paren{\{\lst S_i\} \cup \set G}=\genfun\paren{\{\lst S'_i\} \cup G' \cup \{s,o\}}=\genfun\paren{\{\lst O_i\}}$, and so the final property is satisfied.

We conclude by noting that since all of the operators in $\lst S$ and $\set G$ were formed from products of operators in $\lst O$, which are Pauli operators (i.e., members of the group $\pauligroup$), they are Pauli operators themselves.
\end{proof}
%@+node:gcross.20110318151522.1374: *5* Making them independent
\begin{remark}
A consequence of not requiring independence of the operators in $\lst O$ is that the operators $\lst S$ given by Proposition \ref{proposition-SG} are not necessarily independent.  Happily, since all of these operators can be expressed as tensor products of Pauli operators, we can construct a set of independent operators by performing an analog of Gaussian elimination.
\end{remark}

\begin{proposition}
\label{make-independent-using-elimination}
Suppose that we have been given a sequence of Pauli operators which commute with each other, $\lst R$.  Then there exists
\begin{enumerate}
\item a sequence $\lst S$ of $n$ independent operators such that $\genfun\paren{\{\lst S_i\}}=\genfun\paren{\{\lst R_i\}}$,
\item a sequence of $n$ integers without duplicates in the inclusive range $1\dots n$,
\item and a map $p:\{1\dots n\} \to \{0,1\}$ such that $\lst S_i$ is the only operator in $\lst S$ that anti-commutes with $P_{k_i}^{[p(i)]}$, where $P_k^{[0]}:=X_k$ and $P_k^{[1]}:=Z_k$.
\end{enumerate}
\end{proposition}

\begin{proof}
Proof by induction.  For the base case, we observe that if $\lst R$ is empty, then the trivial sequences $\lst S:=\emptyset$ and $\lst k :=\emptyset$ and the trivial function $p:\emptyset\to\emptyset$ satisfy the requirements.

Now suppose that we know the proposition holds for sequences of length $N-1$, and we are given a sequence $\lst S$ of length $N$.  By our inductive hypothesis, we can apply the proposition to the first $N-1$ operators in $\lst R$ obtain sequences $\lst S'$ and $\lst k'$ of length $n-1$\footnote{Note that $n\ne N$ in general, since some of the first $N-1$ operators might not have been independent.}, and a map $p':\{1\dots n-1\}\to \{0,1\}$ which all satisfy the respective properties of the theorem.  Let $$s:=\lst R_N\cdot \prod_{i=1\dots n-1, \,\,\left\{\lst R_N,P_{k'_i}^{[p(i)]}\right\}=0} \lst S'_i.$$  We know that $s$ commutes with every operator in $\lst S'$ because both $s$ and every operator in $\lst S'$ are equal to products of operators in $\lst R$, which all commute with each other.  Furthermore, since $s$ is a product of $\lst R_N$ and a factor of $\lst S'_i$ for every $i$ such that $\lst R_N$ and $P_{k'_i}^{[p'(i)]}$ anti-commute, and we know that $\lst S_i'$ is the only operator in $\lst S'$ that anti-commutes with $P_{k'_i}^{[p'(i)]}$ for $i=1\dots n-1$, it is therefore the case that $s$ commutes with every member of the set $\{P_{k'_i}^{[p'(i)]}\}_{i=1\dots n-1}$.  Finally, since $s$ is a product of $\lst R_N$ and operators in $\lst S'$, we can obtain $\lst R_N$ entirely from products of operators in $\{\lst S'_i\} \cup \{s\}$, and so $\genfun\paren{\{\lst S'_i\} \cup \{s\}}=\genfun\paren{\{\lst R_i\}}$.

If $s$ is the identity operator, then let $\lst S:=\lst S'$ and $p:=p$ and we are done.  Otherwise, we shall now show that there must exist integers $j\in\{1,\dots,N\}\backslash\{\lst k'_i\}$ and $l\in\{0,1\}$ such that $s$ anti-commutes with $P_{j}^{[l]}$, by demonstrating that if this were not the case then $s$ would have to anti-commute with some element in $\lst S'$, leading to a contradiction.

Assume that $s$ commutes with every operator in the set $$\left\{P_j^{[l]}:\quad j\in\{1,\dots,N\}\backslash\{\lst k'_i\}, \quad l\in\{0,1\}\right\}.$$  Recalling that $s$ is a member of the Pauli group and thus a tensor product of single-particle Pauli spin matrices, and also that $s$ commutes with every member of the set $\{P_{\lst k'_i}^{[p'(i)]}\}_{i=1\dots n-1}$, we see therefore that $s$ must be a product of elements from this set --- that is, there is some subset $\emptyset \ne \set F \subseteq \{P_{\lst k'_i}^{[p'(i)]}\}_{i=1\dots n-1}$ such that $s=\prod_{o\in \set F} o$.  However, from our inductive hypothesis we know that for every operator $f\in\set F$ there is an operator $s'\in\lst S'$ that anti-commutes with $f$ but commutes with the operators in $\set F\backslash\{f\}$.  Since $s$ is therefore a product of a single operator that anti-commutes with $s'$ and more operators that commute with $s'$, we conclude that $s$ and $s'$ anti-commute, which contradicts our earlier conclusion that $s$ commutes with every operator in $\lst S'$.

Now that we have shown that there exist integers $j\in\{1,\dots,N\}\backslash\{\lst k'_i\}$ and $l\in\{0,1\}$ such that $s$ anti-commutes with $P_{j}^{[l]}$, in terms of these integers we define
$$
\begin{aligned}
\lst S_i &:= 
\begin{cases}
\begin{cases}
\lst S'_i \cdot s & \{\lst S_i',P_j^{[l]}\}=0 \\
\lst S'_i & \text{otherwise}
\end{cases} & 1\le i\le n-1 \\
S' & i=n
\end{cases}, \\
\lst k_i &:=
\begin{cases}
\lst k'_i & 1 \le i \le n-1 \\
j & i=n
\end{cases},\quad \text{and} \\
p(i) &:=
\begin{cases}
p'(i) & 1 \le i \le n-1\\
l & i=n
\end{cases},
\end{aligned}
$$ and we are done.
\end{proof}
%@+node:gcross.20110318151522.1375: *5* Construct the logical operators
\begin{remark}
Proposition \ref{make-independent-using-elimination} is good for more than computing an independent set of generators from a commuting list of operators;  it is also the key ingredient in computing the logical qubit operators.
\end{remark}

\begin{proposition}
\label{construction-of-logicals}
Suppose that we have been given the objects described in 1-3 of Proposition \ref{make-independent-using-elimination}.  Let $\set S := \{\vec S_i\}_i.$  Then there exists a set of operators $\set L$ such that
\begin{enumerate}
\item \label{L-are-independent} the operators in $\set S\cup\set L$ are independent;
\item \label{L-are-conjugal-pairs} every operator in $\set L$ is a member of a conjugal pair with respect to $\set S\cup\set L$;
\item \label{L-completes-the-generators} $\genfun\paren{\set S\cup\set L}=\centralizer_\pauligroup(\set S)$ --- that is, the set generated by $\set S\cup\set L$ is equal to the set of Pauli operators that commute with $\set S$.
\end{enumerate}
\end{proposition}

\begin{proof}
Recalling that $n$ is the number of elements in $\lst S$ (and $\set S$), let $\lst l$ be some sequential ordering of $\{1 \dots N\}\backslash\{\vec k_i\}_i$, and then let $\set L:=\{\lst A_i\}_i\cup\{\lst B_i\}_i$ where
$$
\begin{aligned}
\lst A_i &:= P_{\lst l_i}^{[1]}\cdot \paren{\prod_{j=1\dots n,\,\,\{P_{l_i}^{[1]},\lst S_j\}=0} P_{\lst k_j}^{[s(j)]}},\\
\lst B_i &:= P_{\lst l_i}^{[0]}\cdot \paren{\prod_{j=1\dots n,\,\,\{P_{l_i}^{[0]},\lst S_j\}=0} P_{\lst k_j}^{[s(j)]}}.\\
\end{aligned}
$$

To see that property \ref{L-are-independent} is satisfied, observe the following.  First, the operators in $\set L$ are independent from the operators in $\set S$ since none of them is the identity operator and they all commute with every operator in $\{P_{\lst k_i}^{[s(i)]}\}_{i=1 \dots n}$.  Second, they are independent from each other since for every $i=1 \dots |\lst l|$ we have that $\vec A_i$ is the only operator that anti-commutes with $P_{\lst l_i}^{[0]}$ and $\lst B_i$ is the only operator that anti-commutes with $P_{\lst l_i}^{[1]}$.  Thus we conclude that all of the operators in $\set S\cup\set L$ are independent.

Next, to see that property \ref{L-are-conjugal-pairs} holds, observe that for every choice of operators $\lst A_i$ and $\lst S_j$ we have (by intentional construction) that $\lst S_j$ either anti-commutes with two of the operators in the product forming $\lst A_i$ or none at all, and so $[\lst S_i,\lst A_j]=0$ for all $i=1\dots n$ and $j=1\dots |\lst l|$;  by the same reasoning we see also that $[\lst S_i,\lst B_j]=0$ for all $i=1\dots n$ and $j=1\dots |\lst l|$.  Furthermore, each operator $\lst A_i$ commutes with every operator in $\set L$ except for its conjugal partner $\lst B_i$, since the only factor in $\lst A_i$ that could anti-commute with a factor contained within another operator in $\set L$ is $P_{l_i}^{[1]}$, and $\lst B_i$ is the only operator in $\set L$ that contains a factor $P_{l_i}^{[0]}$ that anti-commutes with $\lst X_{l_i}$;  reversing this argument, we also see that $\lst B_i$ commutes with every operator in $\set L$ except for $\lst A_i$.  Thus, every operator in $\set L$ is a member of a conjugal pair with respect to $\set L\cup\set S$.

Finally, to see that property \ref{L-completes-the-generators} holds, observe that since the operators in $\set S$ commute they can therefore be simultaneously diagonalized, which means that there is an automorphism on $\pauligroup$ that takes $\lst S_i\mapsto P_i^{[1]}$ for every $i=1 \dots n$.  The only operators that commute with every such $P_i^{[1]}$ are those which do not contain any factor of $P_i^{[0]}$ for $i=1 \dots n$, and so $$\centralizer_\pauligroup\paren{\{P_i^{[0]}\}_{i=1\dots n}} = \genfun\paren{\{P_i^{[1]}\}_{i=1 \dots n}\cup \{P_i^{[l]}\}_{i=n+1 \dots N, \,\, l=0,1}},$$ which has $2N-n$ generators.  Since the automorphism preserves the number of generators in the centralizer, we thus conclude that $\centralizer_\pauligroup(\set S)$ has exactly $2N-n$ generators.  Since $\set S\cup\set L$ contains independent operators which commute with every member of $\set S$, and furthermore $|\set S\cup\set L|=2N-n$, we thus conclude that $\genfun\paren{\set S\cup\set L}=\centralizer_\pauligroup(\set S)$.
\end{proof}
%@+node:gcross.20110318151522.1376: *5* Prove main theorem
With these building blocks in place, we now prove the main theorem:

\begin{proof}[Proof of Theorem \ref{theorem-SG}]
By Proposition \ref{proposition-SG}, we know that there exists a list of operators $\lst S$ and a set of independent operators $\set G$ satisfying the properties that are listed there.  By Proposition \ref{make-independent-using-elimination}, we know that there is an independent set of operators $\set S$ that generate the same subgroup as $\lst S$.  

Now let $\set F$ be a maximal subset of commuting operators in $\set G$ --- i.e., for each conjugal pair in $\set G$ take one of the two operators --- and then let $\set O := \set F \cup \set S$.  Since all of the operators in $\set O$ commute, we apply Proposition \ref{make-independent-using-elimination} again to conclude the existence of the objects listed there, and then we immediately apply Proposition \ref{construction-of-logicals} to show that a set $\set M$ exists with the properties listed there.  We are not done yet, however, since there might be operators in $\set G$ with which operators in $\set M$ anti-commute, so we let
$$\set L := \left\{m\cdot\paren{\prod_{f\in \set F,\,\,\{m,\text{conj}_{\set G}(f)\}=0} f }:\quad m \in\set M\right\}$$
where $\text{conj}_{\set G}(F)$ is the conjugal partner of $F$ in the set $\set G$.  This guarantees that the operators in $\set L$ commute with every operator in $\set S\cup\set G$, and so we are done.
\end{proof}
%@+node:gcross.20110318151522.1377: *5* Pseudo-code
\begin{remark}
A pseudo-code representation of the algorithm described by Theorem \ref{theorem-SG} is given in Table \ref{table:algo1}.
\end{remark}

%@+others
%@+node:gcross.20110318151522.1378: *6* Compute-Subsystem-Code
\begin{table}
{\scriptsize
\begin{codebox}
\Procname{$\proc{Compute-Subsystem-Code}(\lst O)$}
\li $\lst S \gets []$
\li $\lst G\gets []$
\li \For $o \gets \lst O$ \label{li:csg-main-loop-start}
\li \Do
\li      \For $(g_X, g_Z) \gets \lst G$ %\Comment i.e., iterate over conjugal pairs in $\lst G$
\li      \Do
\li        \kw{if}  $\func{anti}(o,g_X)$ \kw{then} $o \gets o \cdot g_Z$
\li        \kw{if} $\func{anti}(o,g_Z)$ \kw{then} $o \gets o \cdot g_X$
          \End 
\li      \kw{if} \text{$o$ is identity} \kw{then} \Goto \ref{li:csg-main-loop-start}
\li      \For $s \gets \lst S$
\li      \Do
\li        \kw{if} $\func{anti}(o,s)$ \kw{then} \Goto \ref{li:make-into-gauge}
          \End
\li      \Goto \ref{li:csg-main-loop-start}
\li      $\lst G \gets \lst G \cup [(o,s)]$ \label{li:make-into-gauge}
\li      $i \gets 1$ 
\li      \For $s' \gets \lst S$ \label{li:kill-redundant-stabs}
\li      \Do
\li            \kw{if} $s'=s$ \kw{then} \Goto \ref{li:kill-redundant-stabs}
\li            \If $\func{anti}(s',o)$
\li            \Then
\li                  $\lst S[i] \gets s' \cdot s$
\li            \Else
\li                  $\lst S[i] \gets s$
                 \End
\li            $i \gets i+1$
             \End
\li        delete $\lst S[i\dots|\lst S|]$
      \End
\li $\lst I \gets []$
\li $\lst P \gets []$
\li \kw{call} $\proc{Gaussian-Elimination}(\lst S,1,\lst I,\lst P)$ \emph{(Table \ref{table:gaussian-elimination})}
\li $\lst T \gets \lst S \cup [g_X | (g_X,g_Z) \in \lst G]$
\li \kw{call} $\proc{Gaussian-Elimination}(\lst T,|\lst S|+1,\lst I,\lst P)$ \emph{(Table \ref{table:gaussian-elimination})}
\li $\lst L \gets []$
\li \For $i\gets 1$ \To $\text{number of physical qubits}$ \label{li:compute-logicals-loop}
\li \Do
\li     \kw{if} $i\in\lst I$ \kw{then} \Goto \ref{li:compute-logicals-loop}
\li     $l_X \gets X_i$
\li     $l_Z \gets Z_i$
\li     \For $(j,p,t)\gets (\lst I,\lst P,\lst T)$
\li     \Do
\li        \If $p=0$
\li        \Then
\li            \kw{if} $\func{anti}(t,X_j)$ \kw{then} $l_X \gets l_X \cdot Z_j$
\li            \kw{if} $\func{anti}(t,Z_j)$ \kw{then} $l_Z \gets l_Z \cdot Z_j$
\li        \Else
\li            \kw{if} $\func{anti}(t,X_j)$ \kw{then} $l_X \gets l_X \cdot X_j$
\li            \kw{if} $\func{anti}(t,Z_j)$ \kw{then} $l_Z \gets l_Z \cdot X_j$
             \End
          \End 
\li     \For $(g_X,g_Z)\in\lst G$
\li     \Do
\li         \kw{if} $\func{anti}(l_X,g_Z)$ \kw{then} $l_X \gets l_X \cdot g_X$
\li         \kw{if} $\func{anti}(l_Z,g_Z)$ \kw{then} $l_Z \gets l_Z \cdot g_X$
          \End
      \End
\li \Return $(\lst S,\lst G,\lst L)$
\end{codebox}
}
\caption[Algorithm \proc{Compute-Subsystem-Code}]{\small Algorithm which computes the subsystem code generated by a given list of measurement operators $\lst O$.  The subroutine \textsc{Gaussian-Elimination} is listed in Table \ref{table:gaussian-elimination}.} \label{table:algo1}
\end{table}
%@+node:gcross.20110318151522.1379: *6* Gaussian-Elimination
\begin{table}
{\footnotesize
\begin{codebox}
\Procname{$\proc{Gaussian-Elimination}(\lst S,i,\lst I,\lst P)$}
\li \While $i < |\lst S|$ \label{li:while-loop}
\li \Do
\li   $s \gets \lst S[i]$
\li   \For $j \gets 0$ \kw{to} $i-1$
\li   \Do
\li     $(n,z)\gets (\lst I[j],\lst P[j])$
\li     \If $z = 0$
\li     \Then
\li       \If $\func{anti}(s,X_n)$
\li       \Then $s \gets s \cdot \lst S[j]$
          \End
\li     \Else
\li       \If $\func{anti}(s,Z_n)$
\li       \Then $s \gets s \cdot \lst S[j]$
          \End
        \End
      \End
\li   \If $s$ is identity
\li   \Then
\li     delete $\lst S[i]$
\li     \Goto \ref{li:while-loop}
      \End
\li   \For $n \gets 0$ \kw{to} number of physical qubits \label{li:next-physical-qubit}
\li   \Do
\li     \kw{if} $n\in\lst I$ \kw{then} \Goto \ref{li:next-physical-qubit}
\li     \If $\func{anti}(s,X_n)$
\li     \Then
\li         $z\gets 0$
\li         \Goto \ref{li:found-the-pauli}
        \End
\li     \If $\func{anti}(s,Z_n)$
\li     \Then
\li         $z\gets 1$
\li         \Goto \ref{li:found-the-pauli}
        \End
      \End
\li   \If $z = 0$ \label{li:found-the-pauli}
\li   \Then
\li     \For $j \gets 0$ \kw{to} $i-1$
\li     \Do
\li       \If $\func{anti}(\lst S[j],X_n)$
\li       \Then $\lst S[j] \gets \lst S[j] \cdot s$
          \End
        \End
\li   \Else
\li     \For $j \gets 0$ \kw{to} $i-1$
\li     \Do
\li       \If $\func{anti}(\lst S[j],Z_n)$
\li       \Then $\lst S[j] \gets \lst S[j] \cdot s$
          \End
        \End
      \End
\li   append $n$ to $\lst I$
\li   append $z$ to $\lst P$
\li   $\lst S[i] \gets s$
\li   $i \gets i + 1$
    \End
\end{codebox}
}
\caption[Algorithm \proc{Gaussian-Elimination}]{\small Subroutine which performs a procedure analogous to Gaussian-elimination on $\vec S$ to distill a set of independent operators from a possible dependent set of operators.  This subroutine is called by the main subsystem code algorithm in Table \ref{table:algo1}.}
\label{table:gaussian-elimination}
\end{table}
%@-others
%@+node:gcross.20110318151522.1380: *4* Optimization
\section{Optimization of the logical qubits}

\begin{remark}
A pseudo-code representation of the algorithm that will be described in this section is presented in Table \ref{table:algorithm-optimization}.
\end{remark}

%@+<< Main body >>
%@+node:gcross.20110318151522.1381: *5* << Main body >>
%@+others
%@+node:gcross.20110318151522.1382: *6* Lemma:  Recombining generators
In general there are multiple sets of operators that satisfy the properties of \ref{theorem-SG}, as is illustrated by the following Lemma:

\begin{lemma}
\label{combining-pairs}
Given conjugal pairs $Q:=(a,b)$ and $R:=(c,d)$ in relation to some set $\set X$ such that either $a\ne c$ or $b\ne d$, we have that
\begin{enumerate}
\item the pairs $Q':=(a\cdot c,b)$ and $R':=(c,d\cdot b)$ are conjugal pairs with respect to $\set X \backslash \{Q,R\} \cup \{Q',R'\}$; and
\item $\genfun\paren{\{a,b,c,d\}}=\genfun\paren{\{a\cdot c,b,c,d\cdot b\}}$.
\end{enumerate}
\end{lemma}

\begin{proof}
$\,$

\begin{enumerate}
\item Since $[a,c]=[a,d]=[b,c]=[b,d]=\{a,b\}=\{c,d\}=0$, we see therefore that $[a\cdot c,c]=[a\cdot c,d\cdot b]=[b,c]=[b,d\cdot b]=\{a\cdot c,b\}=\{c,d\cdot b\}=0$.  Furthermore, since $a$, $b$, $c$ and $d$ commute with every operator in $\set X\backslash \{Q,R\}$, so do $a\cdot c$ and $d\cdot b$.
\item Since $b$ and $c$ are Pauli operators and thus square to the identity, we have that $a\cdot c\cdot c=a$ and $d\cdot b\cdot b=d$, and so $\genfun\paren{\{a,b,c,d\}}=\genfun\paren{\{a\cdot c,b,c,d\cdot b\}}$.
\end{enumerate}
\end{proof}
%@+node:gcross.20110318151522.1383: *6* Definition: Undetectable error
As a result of this lemma, we see that we can take pairs of arbitrary conjugal pairs from sets $\set G$ and $\set L$ of Theorem \ref{theorem-SG} and replace them with different pairs per the recipe in Lemma \ref{combining-pairs} such that the properties of the theorem still hold.  So given that these sets are not unique, the natural question is:  What is the best choice of $\set G$ and $\set L$?  To answer this, we observe that another criteria we would like for our code to satisfy is that it be as robust to errors as possible;  in particular, we seek to maximize the difficulty of \emph{undetectable errors}, which is defined as follows:

\begin{definition}
Given a set $\set S\subseteq\pauligroup$ and operators $l\in\pauligroup$ and $e\in\centralizer_\pauligroup(\set S)$ which anti-commute (i.e., $\{l,e\}=0$), we say that $e$ is an \emph{undetectable error with respect to} $\set S$ \emph{acting on} $l$.
\end{definition}
%@+node:gcross.20110318151522.1384: *6* Definition: Weight
We assume that the `difficulty' of an interaction between our physical system and its environment is related to the number of physical qubits in our system that are participating in the interaction.  Thus, the natural metric for measuring the relative difficulty of an error is given by its weight, which recall is defined as follows:

\begin{definition}
Given an operator $p\in\pauligroup$---which recalls means that $p$ is the tensor product of single-qubit Pauli unnormalized spin matrices---the \emph{weight} of $p$ is the number of single-qubit operators in the product which are non-trivial (i.e., not the identity).  So for example, the weight of $I\otimes I\otimes I$ is 0, the weight of $I\otimes Z\otimes I\otimes X$ is 2, and the weight of $Z\otimes X\otimes Y$ is 3.
\end{definition}
%@+node:gcross.20110318151522.1385: *6* Definition: Additional notation
For convenience, we introduce the following additional notation:

\begin{definition}
$\quad$

\begin{itemize}
\item the function $\Pi:\powerset(\pauligroup)\to\pauligroup$ is defined such that $\Pi(\set X):=\prod_{x\in \set X} x$ --- that is, it is the product of the operators in $\set X$.
\item assuming we have a set of independent operators, $\set Q$, the function $\set G_{\set Q}:\genfun(\set Q)\to\powerset(\set Q)$ is defined (uniquely) such that for every $q\in\set Q$ we have that $q=\prod_{o\in\set G_{\set Q}(q)} o$;
\item the function $w:\pauligroup\to \mathscr{N}$ is defined such that $w(o)$ gives the weight of $o$;
\item the function $e_{\set S}:\centralizer_\pauligroup\paren{\set S}\to \paren{\powerset\circ\centralizer_\pauligroup}\paren{\set S}$ is defined such that $\set e_{\set S}(l)$ is the set of minimizers of $w$ over the set $$\left\{o: o\in \centralizer_\pauligroup\paren{\set S}, \{o,l\}=0\right\}$$ --- that is, it gives the undetectable errors with respect to $\set S$ acting on $l$ that are of minimum weight;
\item the function $\om_{\set S}:\centralizer_\pauligroup\paren{\set S}\to\mathscr{N}$ is defined such that $\om_{\set S}:=w(o)$ for an arbitrarily chosen $o\in\circ \set e_{\set S}$ --- note that function is well-defined since all operators in the set $\set e_{\set S}$ have the same weight;
\item the function $m_{\set S}:\centralizer_\pauligroup\paren{\set S}\times \centralizer_\pauligroup\paren{\set S} \to \mathscr{N}$ is defined such that $m_{\set S}(l,l'):=\min \{\om_{\set S}(l),\om_{\set S}(l')\}$ --- that is, it gives the smaller of the weights of the smallest weight errors acting on respectively $\set L$ and $\set L'$;
\item the function $\lst M_{\set S}$ is defined such that $\lst M_{\set S}\paren{\lst P}$ is the sequence of $|\lst P|$ integers such that $\lst M_{\set S}\paren{\lst P}_i := m_{\set S}\paren{\lst P_i}$;
\item the functions $p_1$ and $p_2$ are defined such that, given $(a,b):=x$, we have that $p_1(x):=a$ and $p_2(x):=b$.
\item the function $\set U:\powerset(\pauligroup\times\pauligroup)\to\powerset(\pauligroup)$ is defined (for convenience) such that $\set U\paren{\set X}:=\bigcup_{x\in\set X} \{p_1(x),p_2(x)\}$ --- that is, it `unpacks' a set of pairs of operators into a set of operators;  in an abuse of notation, we shall also allow $\set U$ to apply to sequences, so that $\set U(\lst P) := \set U\paren{\{\lst P_i\}_i}$, and to individual pairs, so that if $X$ is a single pair then $\set U(X) := \set U(\{X\})$;
\item finally, a \emph{choice of qubits stabilized by $\set S$}, $\lst P$, is a sequence of pairs of operators from the Pauli group such that 
\begin{enumerate}
\item no operator in $\set U(\lst P)$ appears in more than one pair in $\lst P$;
\item $\set U(\lst P)\subseteq \centralizer_\pauligroup(\set S)$
\item every pair in $\lst P$ is a conjugal pair with respect to $\set S \cup \set U(\lst P)$;
\item $(\om_{\set S}\circ p_1)(\lst P_i)=m_{\set S}(\lst P_i)$; and
\item $\lst M_{\set S}(\lst P)$ is an ordered sequence.
\end{enumerate}
\end{itemize}

\end{definition}
%@+node:gcross.20110318151522.1386: *6* Definition: Total ordering => Optimal choice of qubits
Given the notation above, we now precisely define what we mean by the ``best choice'' of logical qubits.

\begin{definition}
An \emph{optimal choice of qubits stabilized by $\set S$} is any choice of qubits, $\lst P$ stabilized by $\set S$, such that given any other choice of qubits, $\lst P'$, that is also stabilized by $\set S$ and which satisfies $(\genfun\circ \set U)(\lst P)=(\genfun\circ \set U)(\lst P')$, we have that $\lst M(\lst P)_i \ge \lst M(\lst P')_i$ for all $1\le i \le |\lst P|=|\lst P'|$.\footnote{Note that since $\lst P$ and $\lst P'$ are sequences of conjugal pairs without duplicates they are therefore independent, and so if $(\genfun\circ \set U)(\lst P)=(\genfun\circ \set U)(\lst P')$ then we know automatically that $|\lst P|=|\lst P'|$.}
\end{definition}
%@+node:gcross.20110318151522.1387: *6* Definition: Optimization procedure
We now present an algorithm for computing the optimal choice of logical qubits from a set of input qubits.  The key insight upon which the algorithm is built is that undetectable errors acting on the space of logical qubits can never be eliminated entirely, so there will always be \emph{some} operator on which they act.  Thus, the goal of the optimization procedure is not to eliminate errors, but rather to \emph{contain} them, so that they act on as few operators as possible.

The optimization algorithm works by starting with an empty (and therefore automatically optimal) choice of qubits and a set of `unoptimized' qubits, and making progress by gradually moving qubits from the unoptimized set into the choice in such a way that preserves the optimality of the choice.  The trick is that we want to delay as long as possible moving a qubit into the choice, until we have had every chance to improve it.  Thus, we additionally keep track of a subset of pairs in the choice whose second members have yet to be used to contain an error, and then use them as much as possible to fix errors.  That is, at every step in the algorithm, we scan for the minimal weight undetectable error acting on the set of operators consisting of both the second member of the pairs in this subset and all of the operators in the unoptimized set of qubits.  If the minimal weight error acts on an operator in the first category, then we remove the pair from the subset and use this operator to fix this error wherever it occurs in both the second members of pairs in the subset and the unoptimized qubits.  Otherwise, we pull out a qubit from the unoptimized set on which the error acts, use the first member in the pair to fix the error in the qubits remaining in the unoptimized subset, add the pair to the subset of qubits whose second members have yet to be used to contain an error, and then add it to the end of the choice.  At this point our choice turns out to still be optimal because if there had been a way to make the qubit we just added any better by recombining it with other qubits in the choice then we would have already done so by now.

This procedure is presented formally by means of the following inductive definition.

\begin{definition}
Let the function $\optimizer$ be a map from a tuple of the form tuple $(\set S,\set L)$ to a sequence of tuples each of the form $(\set Q,\lst P,\lst s)$, where
\begin{itemize}
\item $\set S$ is a set of commuting Pauli operators;
\item $\set L$ is a set of Pauli operators that are conjugal in relation to $\set L\cup\set S$;
\item $\set Q$ is a set of pairs of Pauli operators;
\item $\lst P$ is a sequence of pairs of Pauli operators; and
\item $\lst s$ is a sequence of integers from $\{0,1\}$ with the same length as $\lst P$.
\end{itemize}

The sequence is defined inductively.  For convenience, we let the first index of this sequence be zero, and define $\optimizer(\set S,\set L)_0=(\set L',\lst\emptyset,\lst\emptyset)$, where $\set L'$ is the set of pairs such that $\set U(\set L')$ and no operator appears in more than one pair in $\set L'$, and $\lst\emptyset$ is the empty sequence.  Now assume that $\optimizer(\set S,\set L)_i$ is defined and that $\optimizer(\set S,\set L)_i=(\set Q,\lst P,\lst s)$.  If $\set Q$ is the empty set, then $\optimizer(\set S,\set L)_i$ is the last element of the sequence so that $|\optimizer(\set S,\set L)|=i+1$.  Otherwise, $\optimizer(\set S,\set L)_{i+1}:=(\set Q',\lst P',\lst s'),$ where $\set Q'$, $\lst P'$ and $\lst s'$ are defined as follows.

Let $\set R:=\{p_2(\lst P_i):i\in\{1\dots |\lst P|\},\lst s'=1\}$ and $\set X:=\set U(\set Q)\cup\set R$.  Note that since $\set Q\ne\emptyset$ that therefore $\set X\ne\emptyset$.  Let $h$\footnote{An observant reader may have noticed that we do not specify exactly how one goes about computing $h$.  This was an intentional omission since the details are quite technical and fortunately they are irrelevant for proving that this algorithm works correctly as long as we can assume that $h$ \emph{can} be computed.  Thus, the discussion of how to compute $h$ will be deferred until Section \ref{subsubsection:running time analysis} when we analyze bounds on the running time of the algorithm.} be the minimal weight error with respect to $\set S$ acting on any operator in $\set X$.  There are two cases: either $h$ acts on some operator in $\set R$, or it doesn't and so must act on some operator in $\set U(\set Q)$.  The definition of $\set Q'$, $\lst P'$ and $\lst a'$ depends on which of these cases holds.

\begin{center}
\textbf{Case 1:} $h$ acts on some operator in $\set R$
\end{center}

Let $k$ be the smallest index such that $h$ acts on $p_2(\lst P_k)$, and $o:=p_2(\lst P_k)$.  Define
$$
f(x) :=
\begin{cases}
x & [h,x] = 0 \\
x\cdot o & \{h,x\} = 0, \\
\end{cases}
$$
and
$$
g(a,b) :=
\begin{cases}
I & [h,a] = 0 \\
b & \{h,b\} = 0, \\
\end{cases}
$$
Let $s':=p_1(\lst P_k)\cdot \alpha \cdot \beta$ where
$$\alpha := \Pi\paren{\{g(p_2(\lst P_i),p_1(\lst P_i)\}:i\in\{k+1\dots |\lst P|\},\lst a_i=0}$$
and
$$\beta := \Pi\paren{\{g(a,b)\cdot g(b,a):(a,b)\in\set Q\}}.$$

Then we define
$$
\begin{aligned}
\set Q'&:= \{(f(a),f(b)):(a,b)\in\set Q\},\\
\lst P'_i&:=
\begin{cases}
\lst P_i & i < k \\
(a',o) & i = k \\
\lst P_i & i > k, \lst a_i=0 \\
\lst P_i & i > k, [h,p_2(\lst P_i)]=0 \\
(p_1(\lst P_i),p_2(\lst P_i)\cdot o) & \text{otherwise} \\
\end{cases}\\
\lst s'_i&:=
\begin{cases}
\lst s_i & i\ne k\\
0 & i =k
\end{cases}
\end{aligned}
$$

\begin{center}
\textbf{Case 2:} $h$ does not act on some operator in $\set R$
\end{center}

Let $q\in\set Q$ be a pair such that $h$ acts on one of its members, and without loss of generality assume that $h$ acts on the first member since otherwise we can swap the members of the pair.  Let $o:=p_1(q)$.
Define
$$
f(x) :=
\begin{cases}
x & [h,x] = 0 \\
x\cdot o & \{h,x\} = 0, \\
\end{cases}
$$
and
$$
g(a,b) :=
\begin{cases}
I & [h,a] = 0 \\
b & \{h,b\} = 0, \\
\end{cases}
$$
Let $$b'':=p_2(q)\cdot\Pi\paren{\{g(a,b)\cdot g(b,a):(a,b)\in\set Q\backslash\{q\}\}},$$ and
$$b':=
\begin{cases}
b'' & [h,b'']=0 \\
b''\cdot o & \{h,b''\}=0 \\
\end{cases}
$$

Then we define
$$
\begin{aligned}
\set Q'&:= \{(f(a),f(b)):(a,b)\in\set Q\backslash\{q\}\},\\
\lst P'_i&:=
\begin{cases}
\lst P_i & i \le |\lst P| \\
(o,b') & i = |\lst P|+1
\end{cases} \\
\lst s'_i&:=
\begin{cases}
\lst s_i & i \le |\lst s|\\
1 & i = |\lst s|+1
\end{cases}
\end{aligned}
$$
\end{definition}

Table \ref{table:algorithm-optimization} contains a listing of pseudo-code that uses the above algorithm to compute the optimal choice of qubits.  For the sake of completeness, it includes additional steps that pertain to the details of how the minimal weight operator is computed, which will be discussed in more detail in Section \ref{subsubsection:running time analysis}.

%@+node:gcross.20110318151522.1388: *6* Pseudo-code
%@+node:gcross.20110318151522.1389: *7* Optimize-Logical-Qubits
\begin{table}
{\scriptsize
\begin{codebox}
\Procname{$\proc{Optimize-Logical-Qubits}(\lst S,\lst G,\lst L)$}
\li $N \gets |\lst L|$
\li $k \gets 0$
\li $\lst m \gets [\const{true}]*|\lst L|$
\li \kw{nested function} $\proc{Query-Function}(o)$
\li \Do
\li     \For $i \gets 0$ \To $k$, $(l_X,l_Z) \gets \lst L[i]$
\li     \Do
\li         \If $\lst m[i]$ and $\func{anti}(o,l_Z)$
\li         \Then
\li           \Return $(\const{true},(1,i))$
            \End
        \End
\li     \For $i \gets k+1$ \To $N$, $(l_X,l_Z) \gets \lst L[i]$
\li     \Do
\li         \If $\func{anti}(o,l_X)$ or $\func{anti}(o,l_Z)$
\li         \Then
\li           \Return $(\const{true},(2,i))$
            \End
        \End
\li     \Return $(\const{false},\const{undefined})$
    \End
\li $\lst O \gets \func{copy}(\lst S)$
\li \For $(g_X, g_Z) \gets \lst G$
\li \Do
\li     append $g_X$ and $g_Z$ to $\lst O$
    \End
\li \For $(l_X, l_Z) \gets \lst L$
\li \Do
\li     append $l_X$ and $l_Z$ to $\lst O$
    \End
\li $\lst P \gets \proc{Compute-Pseudogenerators}(\lst O)$ \emph{(Table \ref{table:compute-pseudo-generators})}
\li \While $k < N$
\li \Do
\li     $(e,(c,l))\gets$ $\proc{Find-Weight-Minimizer}$ 
        \Indentmore
        \Indentmore
\zi         $(\proc{Query-Function},\lst P)$ \emph{(Table \ref{table:find-weight-minimizer})}
        \End
        \End
\li     $(q_X,q_Z)\gets \lst L[l]$
\li     \If $c = 1$
\li     \Then
\li         $\lst m[l] \gets \const{false}$
\li         \For $i\gets l+1$ \To $k$,  $(l_X,l_Z) \gets \lst L[i]$
\li         \Do
\li             \If $m[i]$ and $\{e,l_Z\}$
\li             \Then
\li                 $q_X \gets q_X \cdot l_X$
\li                 $\lst L[i] \gets (l_X,l_Z\cdot q_Z)$
                \End
            \End
\li         \kw{call} $\proc{Fix-Logical-Qubits}$ 
            \Indentmore
\zi         $(\lst L,k,e,q_Z,q_X)$ \emph{(Table \ref{table:fix-logical-qubits})}
            \End
\li         $L[l] \gets (q_X,q_Z)$
\li     \Else
\li         \If $\func{commute}(e,q_X)$
\li         \Then swap $q_X$ and $q_Z$
            \End
\li         $(q_X,q_Z) \gets \lst L[l]$
\li         $\lst L[l] \gets \lst L[k]$
\li         $k \gets k+1$
\li         \kw{call} $\proc{Fix-Logical-Qubits}$
            \Indentmore
\zi         $(\lst L,k,e,q_X,q_Z)$ \emph{(Table \ref{table:fix-logical-qubits})}
            \End
\li         $L[k] \gets (q_X,q_Z)$
        \End
    \End
\end{codebox}
}
\caption[Algorithm \proc{Optimize-Logical-Qubits}]{\small Algorithm which optimizes the logical qubits for a given subsystem code.} \label{table:algorithm-optimization}
\end{table}
%@+node:gcross.20110318151522.1390: *7* Fix-Logical-Qubits
\begin{table}
\begin{codebox}
\Procname{$\proc{Fix-Logical-Qubits}(\lst L,k,e,q_A,q_B)$}
\li \For $i\gets k$ \To $|\lst L|$,  $(l_X,l_Z) \gets \lst L[i]$
\li \Do
\li     \If $\func{anti}(e,l_Z)$ and $\func{anti}(e,l_X)$
\li     \Then
\li         $q_A \gets q_A \cdot l_X \cdot l_Z$
\li         $\lst L[i] \gets (l_X\cdot q_B,l_Z\cdot q_B)$
\li     \ElseIf $\func{anti}(e,l_Z)$
\li     \Then
\li         $q_A \gets q_A \cdot l_X$
\li         $\lst L[i] \gets (l_X,l_Z\cdot q_B)$
\li     \ElseIf $\func{anti}(e,l_X)$
\li     \Then
\li         $q_A \gets q_A \cdot l_Z$
\li         $\lst L[i] \gets (l_X\cdot q_B,l_Z)$
        \End
        \End
\li \If $\proc{anti}(e,q_A)$
\li \Then
        $q_A \gets q_A \cdot q_B$
    \End
\li \Return $(q_A,q_B)$
    \End
\end{codebox}
\caption[Algorithm \proc{Fix-Logical-Qubits}]{Algorithm which `fixes' a subset of the logical qubits so that they are robust to a given error.} \label{table:fix-logical-qubits}
\end{table}
%@+node:gcross.20110318151522.1391: *6* Definition: Running time
In addition to proving that the above algorithm successfully constructs an optimal choice of qubits, we shall also provide a bound on its running time.  In order to do this, we first need to precisely define what we mean by the running time for the purposes of this section.

\begin{definition}
We say that a computation can be performed \emph{in time $x$} if the computation requires taking $x$ products of Pauli operators.
\end{definition}

Of course, the number of products of Pauli operators is not the only metric that could serve as the gauge for the running time, but it suffices for our purposes.
%@+node:gcross.20110318151522.1392: *6* Introduce Theorem
We now present the main result of this section.

\begin{theorem}
\label{theorem:optimization procedure}
Suppose we are given
\begin{itemize}
\item a set of commuting Pauli operators, $\set S$, acting on $N$ physical qubits;
\item a set of pairs, $\set L\subseteq\centralizer_\pauligroup(\set S)$, conjugal with respect to $\set U(\set Q)\cup\set S$;
\item and a set of Pauli operators $\set C$ such that $\genfun(\set C)=\centralizer_\pauligroup(\set S)$;
\end{itemize}
then $\optimizer(\set S,\set L)$ is a sequence of finite length, and if $(\set Q,\lst P,\lst s)$ is the last element in the sequence then $\lst P$ is an optimal choice of qubits such that $\genfun(\lst P)=\genfun(\set L)$, and furthermore it can be computed in a time that is in the set $$O\paren{|\set C|^2+|\lst L|^2d3^d\choose{N}{d}}\footnote{A function $f$ is said to be in the set $O(g)$ if $f$ is asymptotically bounded by some fixed constant times $g$;  formally $f\in O(g)$ if and only if there exists constants $c$ and $x_0$ such that $f(x)<c g(x)$ for all $x>x_0$.},$$ where $d:=\lst M(\lst P)_{|\lst P|}$\footnote{Recall that $\lst M(\lst P)_{|\lst P|}$ is the distance of the best qubit in the (optimized) code.}.
\end{theorem}

The proof of this Theorem is rather technical and shall be split into several subsections.  First we shall prove the existence of a condition that suffices to prove that a choice of logical qubits is optimal.   Second we shall prove that the algorithm above constructs a choice satisfying this condition.  Third we shall prove that the running time of the algorithm has the claimed bound.  Finally we shall tie these results together to prove the Theorem above.
%@-others
%@-<< Main body >>

%@+others
%@+node:gcross.20110318151522.1393: *5* Optimality condition
\subsection{Optimality condition}

\label{optimal-generators}

%@+others
%@+node:gcross.20110318151522.1394: *6* Definition: Unimprovable set
How do we know that a choice of qubits is optimal?  Intuitively, it should be sufficient to prove that a choice of qubits is optimal if we can show that there is no way that we can recombine qubits in the choice to form one or more qubits that are more robust than their component factors --- that is, there is no way that any qubit can be ``improved'' by its involvement in such a product.  This condition is stated formally in the following definition of \emph{unimprovable sets}:

\begin{definition}
An \emph{unimprovable set with respect to $\set S$} is a set of Pauli operators, $\set O$, such that for any subset, $\set X\subseteq \set O$, we have that $(\om_{\set S}\circ\prod)(\set X) = \min_{x\in\set X}\om_{\set S}(x)$.  We say that an unimprovable set $\set O$ \emph{extends to $\lst Q$} if for all subsets $\set X \subseteq \set O\cup\set Q$ such that $\set X\cap \set O \ne \emptyset$ we have that $(\om_{\set S}\circ\prod)(\set X) \le \min_{x\in\set X\cap \set O}\om_{\set S}(x)$.
\end{definition}
%@+node:gcross.20110318151522.1395: *6* Introduce Theorem
The following Theorem is the main result of this subsection that proves that this condition is indeed sufficient to show that a choice of logical qubits is optimal.

\begin{theorem}
\label{theorem:optimality-condition}
If $\lst P$ is a choice of $N$ logical qubits stabilized by $\set S$ such that $\{p_1(\lst P_i)\}_i$ is an unimprovable set with respect to $\set S$ that extends to $\set U(\lst P)$, then $\lst P$ is an optimal choice of qubits.
\end{theorem}

\begin{remark}
The intuition behind the proof of this Theorem is that because the set of first members of pairs is unimprovable and extends to the set of all members of pairs, we know that no qubit can be ``improved'' by recombining it with one or more other qubits.  Thus, the only way one could construct a better choice would by forming $n+k$ independent qubits from products of $n$ qubits (where $k>0$), which intuitively should be impossible.  Thus, we conclude that it is not possible for there to be a choice of qubits generated by the same qubits in this choice that is ``better'' than this choice.

To assist us in proving this Theorem, we shall first prove a number of useful Lemmas and Propositions.  We start with a simple Lemma that proves that taking a product of operators results in an operator that is no ``worse'' (with respect to its robustness to errors) than the worst operator in the product.
\end{remark}
%@+node:gcross.20110318151522.1396: *6* Lemma: Combinations can't make things worse
\begin{lemma}
\label{combinations-can't-make-things-worse}
For any set of operators $\set O$, we have that $(\om_{\set S}\circ \prod)(\set O) \ge \text{min}_{o\in \set O}\,\om_{\set S}\paren{o}$.
\end{lemma}

\begin{proof}[Proof of Lemma]
Any undetectable error with respect to $\set S$ acting on $(\om_{\set S}\circ \prod)(\set O)$ must also act at least one of the operators in $\set O$ since otherwise it cannot anti-commute with the product. \end{proof}
%@+node:gcross.20110318151522.1397: *6* Lemma: The lesser operator wins
\begin{remark}
In general, taking products of operators might result in an operator that is better than the worst operator in the product because errors will cancel each other out --- i.e., if two operators in the product anti-commute with an error then their product commutes with the error.  Thus, it is useful to state a condition under which we can be certain that this will not happen, so that the product is exactly as bad as the worst operator, which we do in the following Lemma.
\end{remark}

\begin{lemma}
\label{lesser-operator-wins}
Suppose we are given two operators $a,b\in\pauligroup$ such that $\om_{\set S}(a)<\om_{\set S}(b)$;  then $\om_{\set S}(a\cdot b) = \om_{\set S}(a)$.
\end{lemma}

\begin{proof}[Proof of Lemma]
Since $\om_{\set S}(a)<\om_{\set S}(b)$, there must be an undetectable error with respect to $\set S$ that acts on $a$ but not on $b$;  thus, it must anti-commute with and hence act on the product $a\cdot b$, so that $\om_{\set S}(a\cdot b)\le \om_{\set S}(a)$.  Since $\om_{\set S}(a\cdot b)\ge \om_{\set S}(a)$ by Lemma \ref{lesser-operator-wins}, we conclude that $\om_{\set S}(a\cdot b) = \om_{\set S}(a)$.
\end{proof}
%@+node:gcross.20110318151522.1398: *6* Proposition: Bound on recombinations
\begin{remark}
Intuitively we should expect that it is not possible to take $n$ qubits and recombine them to form $n+k$ independent qubits where $k>0$.  To state this intuition in other terms, suppose we are given a set of conjugal pairs $\set C$ that are generated from some other set of conjugal pairs $\set D$.  We know that every pair in $\set C$ must have a member that includes a factor that is a first member of a pair in $\set D$ (since otherwise the members of the pair cannot anti-commute), so let $\set A$ be the set of first members of pairs in $\set D$.  Our intuition then tells us that $|\set C|\le |\set A|=|\set D|$.  The following Proposition states this fact formally:
\end{remark}
%@+node:gcross.20110318151522.1399: *7* Statement
\begin{proposition}
\label{bound-on-recombinations}
Suppose we are given
\begin{enumerate}
\item sets of independent Pauli operators $\set Q$ and $\set S$;
\item a non-empty set of conjugal pairs, $\set C$, with respect to $\set U(\set C) \cup \set S$, such that $U(\set C)\subseteq \genfun(\set Q)$; and
\item a set $\set A$ of independent Pauli operators with the property that for any conjugal pair $X:=(a,b)$ such that $\{a,b\}\in\genfun(\set C)$, we must have that $\set G_{\set Q}(X) \cap \set A \ne \emptyset$.
\end{enumerate}
Then $|\set C|\le|\set A|$.
\end{proposition}
\begin{remark}
The basic idea behind the proof of this Proposition is that an analogue of Gaussian elimination can be used on the conjugal pairs to eliminate the presence of members of $\set A$ from them;  when we are done with this process, we can see that unless $|\set C|\le |\set A|$ we would have eliminated \emph{all} members of $\set A$ from some of the qubits, which contradicts the assumptions of this Proposition.

The formal proof is somewhat technical and so we first introduce several Lemmas.  First we prove a small helper Lemma that shows that it is possible to take a conjugal pair in which a given generator appears and rearrange it so that the generator only appears in the first member of the pair.
\end{remark}
%@+node:gcross.20110318151522.1400: *7* Single pair rearrangement.
\begin{lemma}
\label{single-pair-rearrangement}
Let $A:=(a,b)$ with $\{a,b\}\subseteq\genfun(\set Q)$ be a conjugal pair with respect to some set $\set S$, and $o$ be some Pauli operator such that $o\in\set G_{\set Q}(A)$.  Then there exists a pair $B:=(c,d)$ such that
\begin{enumerate}
\item $\{c,d\}\subseteq\genfun(\set Q)$;
\item $o\in \set G_{\set Q}(c)$;
\item $o\notin \set G_{\set Q}(d)$;
\item $(c,d)$ is a conjugal pair with respect to $\paren{\set S\backslash \{a,b\}}\cup\{c,d\}$; and
\item $(\genfun\circ\set U)(B) = (\genfun\circ\set U)(A)$.
\end{enumerate}
\end{lemma}

\begin{proof}
Let
$$
(c,d):=
\begin{cases}
\paren{a,b} & o\in \set G_{\set Q}(a), o\notin \set G_{\set Q}(b) \\
\paren{b,a} &  o\notin \set G_{\set Q}(a), o\in \set G_{\set Q}(b) \\
\paren{a,b\cdot a} & o\in \set G_{\set Q}(a), o\in \set G_{\set Q}(b) \\
\end{cases}
$$
Note that in any of the above cases, properties 1-3 are satisfied by construction, property 4 is satisfied because $c$ and $d$ are products of $a$ and $b$ which commute with every element in $\set S\backslash \{a,b\}$ and $\{c,d\}=0$, and finally property 5 is satisfied because $\{a,b\}\subseteq \genfun\paren{\{c,d\}}$ and $\{c,d\}\subseteq \genfun\paren{\{a,b\}}$.
\end{proof}
%@+node:gcross.20110318151522.1401: *7* Directed Gaussian elimination
\begin{remark}
This next Lemma contains the heart of this Proposition by introducing an analogue to a directed Gaussian elimination procedure.  Specifically, it shows that if we have a generator $a\in\set A$ that appears in one or more conjugal pairs, then we can take products of the conjugal pairs to eliminate it from appearing anywhere except in the first member of a single pair.
\end{remark}

\begin{lemma}
\label{directed-gaussian-elimination-of-logicals}
In the context of Proposition \ref{bound-on-recombinations}, suppose we are given an element $a\in \set A$ with the property that there exists a pair $Y''\in\set C$ such that $a\in\set G(Y'')$. Then there exists a conjugal pair $Y$ and set of conjugal pairs $\set D$, all with respect to $\set U\paren{\{Y\}\cup \set D} \cup \set S$, such that
\begin{enumerate}
\item $|\set D| = |\set C|-1$
\item $(\genfun\circ\set U)(\{Y\}\cup \set D)=(\genfun\circ\set U)(\set C)$;
\item $a\in (\set G_{\set Q}\circ p_1)(Y)$ but $a\notin (\set G_{\set Q}\circ p_2)(Y)$;
\item $a\notin \bigcup_{D\in \set D} \set G_{\set Q}(D)$; and
\item for every conjugal pair $O\in\set D$, we have that $\set G_{\set Q}(O) \cap \set A\backslash \{a\} \ne \emptyset$.
\end{enumerate}
\end{lemma}

\begin{proof}
Proof by induction on the size of $\set C$.  If $\set C=\{Y''\}$, then apply Lemma \ref{single-pair-rearrangement} letting $o:=a$, $A:=Y''$, and $Y:=B$, and we see that we have a pair $Y$ which is conjugal with respect to $\{Y\}\cup\set S$ and also such that $(\genfun\circ\set U)(\{Y\})=(\genfun\circ\set U)(\{Y''\})$.  Let $\set D:=\emptyset$, and we see that the remaining properties hold trivially, so we are done.

Now let us assume that this lemma has been proven for the case where $|\set C|=n-1$, and we are given a set $\set C$ with $n$ elements.  Take any $X''\in\set C\backslash\{Y''\}$, and apply the lemma to $\set C\backslash \{X''\}$, $\set A$, $a$ and $Y''$ to obtain the objects $Y'$ and $\set D'$ described in this Lemma without the primes.  If $a\notin\set G(X'')$, then by the assumptions of the Lemma we know that $\set G(X'') \cap \set A\backslash \{a\} \ne \emptyset$, so let $Y:=Y'$ and $\set D:=\set D'\cup\{X''\}$, and we are done.

Otherwise, apply Lemma \ref{single-pair-rearrangement}, setting $A:=X''$, $o:=a$, and $X':=B$, and let $X:=\paren{p_1(X')\cdot p_1(Y'),p_2(X')}$ and $Y:=\paren{p_1(Y'),p_2(Y')\cdot p_2(X')}$.  Note $X'$ and $Y'$ are conjugal pairs with respect to $\set U\paren{\{X',Y'\}\cup\set D}\cup\set S$ and $\{X',Y'\}\cap \paren{\set D\cup\set S}=\emptyset$, and so by Lemma \ref{combining-pairs} we conclude that $X$ and $Y$ are conjugal pairs with respect to $\set U\paren{\{X,Y\}\cup\set D}\cup\set S$, and also that $(\genfun\circ\set U)(\{X,Y\})=(\genfun\circ\set U)(\{X',Y'\})$;  since $X'$ was obtained from applying Lemma \ref{single-pair-rearrangement} to $X''$ and $a$, we furthermore conclude that $(\genfun\circ\set U)(\{X,Y\})=(\genfun\circ\set U)(\{X'',Y'\})$.  Since $X'$ was obtained as a result of Lemma \ref{single-pair-rearrangement}, we know that $a\in (\set G \circ p_1)(X')$ but $a\notin (\set G_{\set Q} \circ p_2)(X')$, and we also know from the earlier recursive application of this Lemma that $a\in (\set G \circ p_1)(Y')$ but $a\notin (\set G_{\set Q} \circ p_2)(Y')$.  Thus, we observe that by construction, $a\in (\set G_{\set Q} \circ p_1)(Y)$, and $a\notin \paren{(\set G_{\set Q} \circ p_2)(Y) \cup \set G_{\set Q}(X)}$.

Let $\set D:=\{X\}\cup\set D'$, and observe that $|\set D|=|\set D'|+1=|\set C\backslash \{X''\}|-1+1=|\set C|-1$, and also that $(\genfun\circ\set U)(\{Y\}\cup\set D)=(\genfun\circ\set U)(\{X,Y\}\cup\set D')=(\genfun\circ\set U)(\{X''\}\cup(\{Y'\}\cup\set D'))=(\genfun\circ\set U)(\{X''\}\cup(\set C'\backslash\{X''\}))=(\genfun\circ\set U)(\set C)$.  Furthermore, by the earlier recursive application of this Lemma we know that $a\notin\set G_{\set Q}(O)$ for every $O\in \set D'$, so since we have also established that $a\notin \set G_{\set Q}(X)$, we conclude that $a\notin\set G_{\set Q}(O)$ for every $O\in \set D$;  since also know that every such $O$ must also satisfy $\set G_{\set Q}(O)\cap \set A \ne \emptyset$, we conclude that every such $O$ satisfies $\set G_{\set Q}(O) \cap A\backslash\{a\}\ne\emptyset$.
\end{proof}
%@+node:gcross.20110318151522.1402: *7* Undirected Gaussian elimination
\begin{remark}
This next Lemma provides the small but important result that we can always find a generator $a$ that appears somewhere in the conjugal pairs;  this has the consequence that we can now perform \emph{undirected} Gaussian elimination (in contrast to the \emph{directed} Gaussian elimination procedure described in the previous Lemma) by picking an arbitrary generator to eliminate rather than specifying a particular generator up-front.
\end{remark}

\begin{lemma}
\label{undirected-gaussian-elimination-of-logicals}
In the context of Proposition \ref{bound-on-recombinations}, there exists a Pauli operator $a$ satisfying the assumption of Lemma \ref{directed-gaussian-elimination-of-logicals}.
\end{lemma}

\begin{proof}
Take any pair $Y'\in\set C$.  By the assumptions of Proposition \ref{bound-on-recombinations}, we know that $\set G(Y')\cap \set A \ne \emptyset$, which implies that there exists an element $a\in A$ such that either $a\in (\set G_{\set Q}\circ p_1)(Y')$ or $a\in (\set G_{\set Q}\circ p_2)(Y')$.  The existence of $Y$ and $\set D$ then follow immediately from the application of Lemma \ref{directed-gaussian-elimination-of-logicals}.
\end{proof}
%@+node:gcross.20110318151522.1403: *7* Elimination to create subset
\begin{remark}
This final Lemma (inside the proof of Proposition \ref{bound-on-recombinations}) shows using Gaussian elimination that there must be a number of generators from $\set A$ present in the pairs in $\set C$ that is equal to the size of $\set C$, since otherwise we could recombine the pairs in $\set C$ to obtain a pair that includes no generator from $\set A$, contradicting the assumptions of Proposition \ref{bound-on-recombinations}.
\end{remark}
\begin{lemma}
\label{elimination-to-create-subset}
In the context of Proposition \ref{bound-on-recombinations}, there exists a set of conjugal pairs, $\set X$, with respect to $\set X\cup\set S$, and a subset of operators, $\set O\subseteq \set A$, such that
\begin{enumerate}
\item $|\set X|=|\set O|=|\set C|$;
\item $(\genfun\circ\set U)(\set X)=(\genfun\circ\set U)(\set C)$; and
\item for every $o\in\set O$, there is a conjugal pair $Y\in\set X$ such that $o\in\set G_{\set Q}(Y)$;
\end{enumerate}
\end{lemma}

\begin{proof}
Proof by induction.  If $\set C$ is empty, then the empty sets trivially satisfy this Lemma.

Now suppose that we have proven this Lemma for $|\set C|=N-1$, and assume we have been given sets $\set C$ and $\set A$ such that $|\set C|=N$.  Applying Lemma \ref{undirected-gaussian-elimination-of-logicals} to $\set C$ and $\set A$ we obtain the conjugal pair $Y$,  the set of conjugal pairs $\set D$, and the element $a$ described in the conclusions of that Lemma.  Apply this Lemma recursively to the respective sets $\set D$ and $\set A\backslash\{a\}$, we obtain the sets $\set X'$ and $\set O'$ described (without the primes) in this Lemma; let $\set X := \set X'\cup\{Y\}$ and $\set O:=\set O'\cup\{a\}$.  Note that $\set X$ is a set of conjugal pairs with respect to $\set X\cup\set S$ since $\set X'$ is a set of conjugal pairs with respect to $\set X'\cup\set S$, and we know that the operators in $Y$ commute with every operator in every pair in $\set X'$ since they commute with every operator in $(\genfun\circ\set U)(\set D)=(\genfun\circ\set U)(\set X')$.

First, observe that $|\set O|=|\set O'|+1$ since $a\notin \set O'$.  Furthermore, $a\notin \bigcup_{x\in \set U(\set X')} \set G(x)$ since $a\notin \bigcup_{x\in \set U(\set D)} \set G_{\set Q}(x)$ by Lemma \ref{undirected-gaussian-elimination-of-logicals} and $(\genfun\circ\set U)(\set D)=(\genfun\circ\set U)(\set X')$ by recursive application of this Corollary.  Thus, $|\set X|=|\set X'|+1$ since $Y\notin X'$ as $a\in\set G(Y)$, and $|\set X|=|\set O|=|\set D|+1=|\set C|-1+1=|\set C|$.

Second, observe that since $(\genfun\circ\set U)(\set X')=(\genfun\circ\set U)(\set D)=(\genfun\circ\set U)(\set C)$ by recursive application of this Lemma and $(\genfun\circ\set U)(\{Y\}\cup\set D)=(\genfun\circ\set U)(\set C)$ by Lemma \ref{undirected-gaussian-elimination-of-logicals}, we conclude that $(\genfun\circ\set U)(\set X) = (\genfun\circ\set U)(\{Y\}\cup\set X') = (\genfun\circ\set U)(\{Y\}\cup\set D) = (\genfun\circ\set U)(\set C)$.

Finally, observe that for every $o\in\set O$ we either have that $o=a$, in which case $Y\in\set X$ and $o\in\set G_{\set Q}(Y)$, or $o\in \set A\backslash\{a\}$, in which case by recursive application of this Lemma we know that there is an operator $Z\in \set X'\subseteq \set X$ such that $o\in\set G_{\set Q}(Z)$.
\end{proof}
%@+node:gcross.20110318151522.1404: *7* Proof
\begin{remark}
With the preceding Lemmas having performed the heavy lifting, the proof of Proposition \ref{bound-on-recombinations} is quite simple.
\end{remark}

\begin{proof}[Proof of Proposition \ref{bound-on-recombinations}]
Proof by contradiction.  By Lemma \ref{elimination-to-create-subset}, there would have to exist a subset $\set O\subseteq\set A$ such that $|\set C|=|\set O|>|\set A|$, which is impossible.
\end{proof}
%@+node:gcross.20110318151522.1405: *6* Proof of Theorem
\begin{remark}
With the preceding Lemmas and Propositions, we now have all of the tools that we need to prove Theorem \ref{theorem:optimality-condition}.  Again, the idea behind this proof is that because the first members of pairs in the choice are contained in an unimprovable set, one cannot take products of the qubits in the choice in order to improve them;  thus, the only way one could construct a better choice would by forming $n+k$ independent qubits from products of $n$ qubits (where $k>0$), which is disallowed by the result of Proposition \ref{bound-on-recombinations}.  Hence, there can be no better choice.
\end{remark}

\begin{proof}[Proof of Theorem \ref{theorem:optimality-condition}]
Proof by contradiction.  Let $\lst P'$ be some choice of qubits stabilized by $\set S$ such that $(\genfun\circ\set U)(\lst P)=(\genfun\circ\set U)(\lst P')$ (which automatically implies that $|\lst P|=|\lst P'|$) and there exists some integer $k$ such that $M_{\set S}(\lst P')_k > M_{\set S}(\lst P)_k$;  in particular, let $k$ be the smallest such integer, and let $\set C:=\{\lst P'_i : i \ge k\}$.  Let $l$ be the smallest integer such that $\lst M_{\set S}(\lst P)_l\ge \lst M_{\set S}(\lst P')_k$ or $|\lst P|+1$ if there is no such integer, and let $\set A := \{p_1(\lst P_i) : i \ge l\}$; note that since $\lst M_{\set S}(\lst P')_k > \lst M_{\set S}(\lst P)_k$ we must have $l>k$, and hence $|\set C| > |\set A|$.

Take any conjugal pair $O:=(a,b)$ such that $\{a,b\}\in\genfun(\set C)$.  Since $a$ and $b$ anti-commute, it must be the case that $\{p_1(\lst P_i)\}_i\cap \set G_{\set U(\lst P)}(O)\ne\emptyset$, because if every operator in $\set G_{\set U(\lst P)}(O)$ were the second member of a pair in $\lst P$ then $a$ and $b$ would commute.  Let $c$ be a choice of $a$ or $b$ such that $\{p_1(\lst P_i)\}_i\cap \set G_{\set U(\lst P)}(c)\ne\emptyset$.  By Lemma \ref{combinations-can't-make-things-worse} we know that $\lst M_{\set S}(\lst P')_k\le\om_{\set S}(c)$ since $c\in\genfun(\set C)$.  By the assumption of this Theorem that $\{p_1(\lst P_i)\}_i$ is an unimprovable set that extends to $\set U(\lst P)$, we know that $\lst M_{\set S}(\lst P')_k \le \om_{\set S}(c)\le\min \{\om_{\set S}(x):x\in\{p_1(\lst P_i)\}_i\cap \set G_{\set U(\lst P)}(c)\}$.  From these bounds we conclude that $\{p_1(\lst P_i)\}_i\cap \set G_{\set U(\lst P)}(c)\subseteq \set A$, and since $\{p_1(\lst P_i)\}_i\cap \set G_{\set U(\lst P)}(c)\ne\emptyset$ we see therefore that $\set G_{\set U(\lst P)}(c)\cap\set A\ne\emptyset$ and so $\set G_{\set U(\lst P)}(O)\cap\set A\ne\emptyset$.

We have now demonstrated that for every pair $O:=(a,b)$ such that $\{a,b\}\in\set \genfun(\set C)$, we must have $\set G(O)\cap\set A \ne\emptyset$.  Observe that this means that sets $\set C$ and $\set A$ match the descriptions in Proposition \ref{bound-on-recombinations} (letting set $\set Q:=\{\set P_i\}_i$), and thus we see that it is impossible for $|\set C|>|\set A|$, and so we have a contradiction.  We thus conclude that no such choice $\lst P'$ can exist.
\end{proof}
%@-others
%@+node:gcross.20110318151522.1406: *5* Correctness of the algorithm
\subsection{Correctness of the algorithm}

%@+others
%@+node:gcross.20110318151522.1407: *6* Introduce Theorem
We now prove that this algorithm is correct --- that is, that it terminates and outputs an optimal choice of logical qubits.  We do so by proving the following theorem, which is the main result of this section.

\begin{theorem}
\label{theorem:algorithm is correct}
Given a set of commuting Pauli operators $\set S$ and a set of pairs $\set L\subseteq\centralizer_\pauligroup(\set S)$ conjugal in relation to $\set U(\set L)\cup\set S$, the sequence $\optimizer(\set S,\set L)$ is finite and if $(\set Q,\lst P,\lst s)$ is the last element then $\set Q=\emptyset$ and $\lst P$ is an optimal choice of logical qubits stabilized by $\set S$ such that $\genfun(\lst P)=\genfun(\set L)$.
\end{theorem}
%@+node:gcross.20110318151522.1408: *6* Lemma: An element can be moved from the extension to the unimprovable set
\begin{remark}
Before proving this Theorem, we shall first prove several related Lemmas and Propositions.

Our ultimate goal is to expand the unimprovable set so that it includes at least the first member of every conjugal pair in the set of logical qubit operators, since this means that we have satisfied the optimality condition.  Thus, we want to be able to add operators to this set while preserving the property of being an unimprovable set.

The following Lemma shows that if we have an operator $o$ in a set $\set X$ to which some unimprovable set extends, then if the smallest weight undetectable error acting on $o$ acts on no other operator in $\set X$ then we may move $o$ to the unimprovable set to obtain a new unimprovable set that extends to $\set X\slash \{o\}$.  The intuition here is that because said error acts only on $o$, it cannot be canceled by multiplying $o$ by other operators, and so it is an ``unimprovable'' operator that can be included in our unimprovable set.
\end{remark}

\begin{lemma}
\label{lemma:move-it-over}
If $\set O$ is an unimprovable set with respect to $\set S$ that extends to $\set X:=\{o\}\cup\set X'$, and there exists an undetectable error, $h$, of weight $\om_{\set S}(o)$ that acts on $o$ but not on any operator in $\set X'$, then $\set O':=\set O\cup\{o\}$ is an unimprovable set with respect to $\set S$ that extends to $\set X'$.
\end{lemma}

\begin{proof}
Take any subset $\set R\subseteq \set O'\cup\set X'$ such that $\set R\cap\set O'\ne\emptyset$.  We need to show that $\om_{\set S}\paren{r} \le \min_{a\in\set R\cap\set O'}\om_{\set S}(a)$ where $r:=\Pi(\set R)$.

First consider the case where $o\notin \set R$;  in this case we have that $\set R\subseteq \set O\cup\set X$ such that $\set R\cap\set O=\set R\cap\set O'\ne\emptyset$, and so since $\set O$ extends to $\set X$ we conclude that $\om_{\set S}\paren{r} \le \min_{a\in\set R\cap\set O}\om_{\set S}(a)= \min_{a\in\set R\cap\set O'}\om_{\set S}(a)$.

Now consider the case where $\set R\cap\set O'=\{o\}$.  In this case, by the assumptions of this Lemma, we know $h$ acts on $o$ but not on any operator in $\set X'$ which implies that $h$ acts on $r$ and so $\om_{\set S}(r) \le w(h)=\om_{\set S}(o)=\min_{a\in\set R\cap\set O'}\om_{\set S}(a)$.

Finally we consider the remaining case where $\{o\}\subset\set R\cap\set O'$. Let $\set Z := \set R\backslash\{o\}\ne\emptyset$.  Since $\set O$ extends to $\set X$ and $\set R\cap\set O=\set Z\cap\set O\ne\emptyset$, we know that $\om_{\set S}(x)\le \min_{a\in\set Z\cap\set O}\om_{\set S}(y)=:d$.  If $d \le \om_{\set S}(o)$, then $\om_{\set S}(r)\le d = \min_{a\in\set R\cap\set O'}\om_{\set S}(a)$, and we are done.  Otherwise, since $d> \om_{\set S}(o)$ we know that $h$ acts on $r$ since there can be no other operator in $\set Z$ that anti-commutes with $h$, and so $\om_{\set S}(r) \le w(h)\le\om_{\set S}(o)=\min_{a\in\set R\cap\set O'}\om_{\set S}(a)$
\end{proof}
%@+node:gcross.20110318151522.1409: *6* Lemma: Replacing element with product preserves unimprovability
\begin{remark}
In Case 1 of the algorithm we take an element that is a member of an unimprovable set and replace it with the product of this element times some elements in the set to which the unimprovable set extends.  We want to show that this preserves the unimprovability of the set, and this is done in the following Lemma.
\end{remark}

\begin{lemma}
\label{lemma:replacing element with product preserves unimprovability}
Suppose we are given an unimprovable set $\set O$ with respect to $\set S$ that extends to $\set X$.  Let $o$ be any element in $\set O$, and $\set Z\subseteq \set O\cup\set X$ such that $o\in \set Z$.  Let $o':=\Pi(\set Z)$ and $\set O' := \paren{\set O\backslash\{o\}}\cup\{o'\}$.  If $\om_{\set S}(o')=\om_{\set S}(o)$, then $\set O'$ is also an unimprovable set with respect to $\set S$ that extends to $\set X$.
\end{lemma}

\begin{proof}
Take any subset of elements $\set R\subseteq\set O'\cup\set X$ such that $\set R\cap\set O'\ne\emptyset$, and let $x := \Pi(\set R)$.  We need to show that $\om_{\set S}(x) \le \min_{a\in\set R\cap\set O}\om_{\set S}(a)$.  If $o'\notin\set R$, then this follows immediately from the fact that $\set R\subseteq\set O\cup \set X$ and $\set R\cap\set O\ne\emptyset$ and $\set O$ is an unimprovable set that extends to $\set X$, so assume that that $o'\in\set R$.  Since $o'=\Pi(\set Z)$ and $\set Z\subseteq\set O\cup\set X$, we conclude that the set $\set T\subseteq\set O\cup\set X$ which is the symmetric difference of $\set Z$ and $\set R$ satisfies the property that $x=\Pi(\set T)$.  Note that $o\in\set T$ since $o\in \set Z$ and $o\notin \set O'$ and so $o\notin \set R$.  Thus, $\set T \cap\set O\ne\emptyset$, and so $\om_{\set S}(x)\le \min_{a\in\set T\cap\set O}\om_{\set S}(a)$ since $\set O$ is an unimprovable set that extends to $\set X$.  Thus, for us to show that $\om_{\set S}(x) \le \min_{a\in\set R\cap\set O'}\om_{\set S}(a)$, it suffices for us to show that $\min_{a\in\set R\cap\set O'}\om_{\set S}(a)=\min_{a\in\set T\cap\set O}\om_{\set S}(a)$.

First note that since $\om_{\set S}(o')=\om_{\set S}(o)$, there is no element $z\in\set Z\cap\set O$ such that $\om_{\set S}(z)<\om_{\set S}(o)$.  Thus, any operator $t\in\set T\cap\set O$ such that $\om_{\set S}(t)<\om_{\set S}(o)$ must also appear in $\set R\cap\set O'$, and vice versa;  put another way, any operator that is less robust to errors than $o$ must be present in both $\set T\cap\set O$ and $\set R\cap\set O'$ together or neither.  Thus, if at least one such operator exists, then we conclude that $\min_{a\in\set R\cap\set O'}\om_{\set S}(a)=\min_{a\in\set T\cap\set O}\om_{\set S}(a)$ since in this case any minimizer of $\om_{\set S}$ must be shared between the two sets.  If no such operator exists, then since $o\in\set T\cap\set O$ and $o'\in\set R\cap\set O'$ and there is no other operator present in either set with a smaller minimum weight undetectable error, we conclude that $o$ is the minimizer of $\om_{\set S}$ over $\set T\cap\set O$ and $o'$ is the minimizer over $\set R\cap\set O'$ and since $\om_{\set S}(o')=\om_{\set S}(o)$ we have that $\min_{a\in\set R\cap\set O'}\om_{\set S}(a)=\om_{\set S}(o)=\min_{a\in\set T\cap\set O}\om_{\set S}(a)$.

Thus we have shown that $\min_{a\in\set R\cap\set O'}\om_{\set S}(a)=\min_{a\in\set T\cap\set O}\om_{\set S}(a)$, and since our choice of $\set R$ was arbitrary we conclude that $\set O'$ is an unimprovable set that extends to $\set X$.
\end{proof}
%@+node:gcross.20110318151522.1410: *6* Lemma: Recombining extension elements preserves extension
\begin{remark}
In both cases of the algorithm we replace a set to which an unimprovable set extends with a new set that a product of elements in the old set.  We want to show that the new set is also an extension of the unimprovable set, and this is proved by the following Lemma.
\end{remark}

\begin{lemma}
\label{lemma:recombining extension elements preserves extension}
If $\set O$ is an unimprovable set with respect to $\set S$ that extends to $\set X$, and $\set X'$ is a set such that $\set X'\subseteq\genfun(\set X)$, then $\set O$ also extends to $\set X'$.
\end{lemma}

\begin{proof}
Take any subset $\set R' \subseteq \set O\cup\set X'$ such that $\set A := \set R'\cap \set O \ne \emptyset$.  We need to show that $\om_{\set S}(x)\le\min_{y\in\set A}\om_{\set S}(y)$, where $x := \Pi(\set R')$.  Note that since $\set X'\subseteq\genfun(\set X)$, there exists a set $\set B\subseteq \set X$ such that $x = \Pi\paren{\set A\cup\set B}$, and so since $\set O$ extends to $\set X$ we conclude that $\om_{\set S}(x)\le\min_{y\in\set A}\om_{\set S}(y)$.  Since our choice of $\set R'$ was arbitrary, we conclude that that $\set O$ extends to $\set X'$.
\end{proof}
%@+node:gcross.20110318151522.1411: *6* Proposition: The set generated is an invariant
\begin{remark}
Most of the heavy lifting in this section is performed in the following Proposition, which uses induction to prove a number of properties about the output of the algorithm at every step.
\end{remark}

\begin{proposition}
\label{proposition:properties of the algorithm}
Given a set of Pauli operators $\set S$ and a set of pairs $\set L\subseteq\centralizer_\pauligroup(\set S)$ conjugal in relation to $\set U(\set L)\cup\set S$, for every $(\set Q,\lst P,\lst s)\in\optimizer(\set S,\set L)$ we have that
\begin{enumerate}
\item $\genfun\paren{\set U(\set Q)\cup\set U(\lst P)}=\genfun(\set L)$;
\item $\set U(\set Q)\cup\set U(\lst P)\subseteq\centralizer_\pauligroup(\set S)$;
\item $2(|\set Q|+|\lst P|)=|\set L|$;
\item $|\set U(\set Q)\cup\set U(\lst P)|=|\lst L|$;
\item $\set U(\set Q)\cap\set U(\lst P)=\emptyset$, and no operator appears in more than one pair in either $\set Q$ or $\lst P$;
\item $\set O$ is an unimprovable set of operators that extends to $\set X$;
\item $\max_{o\in\set O}\om_{\set S}(o)\le\min_{x\in\set X}(x)$;
\item $(\om_{\set S}\circ p_1)(q)=m_{\set S}(q)$ for all $q\in\lst P$;
\item $\lst M(\lst P)$ is ordered;
\item $\lst P$ is a choice of qubits stabilized by $\set S$;
\item $\lst P$ is an optimal choice of qubits;
\end{enumerate}
where $\set X := \unpack(\set Q)\cup\{p_2(\lst P_i):1 \le i \le |\lst P|, \lst s_i=1\}$ if $\lst P$ is non-empty and $\set X := \unpack(\set Q)$ otherwise, and $\set O:=\unpack(\lst P)\backslash\set X$.
\end{proposition}

\begin{proof}
Proof by induction.  It is easy to see that these properties hold for $\optimizer(\set S,\set L)_0=(\set L',\lst\emptyset,\lst\emptyset)$, so now assume that they hold for $(\set Q,\lst P,\lst s):=\optimizer(\set S,\set L)_i$, and let $(\set Q',\lst P',\lst s'):=\optimizer(\set S,\set L)_{i+1}$.  For convenience, define $\set X:=\unpack(\set Q)\cup\{p_2(\lst P_i):1 \le i \le |\lst P|, \lst s_i=1\}$ (or $\set X:=\unpack(\set Q)$ if $\lst P$ is empty), $\set X':=\unpack(\set Q')\cup\{p_2(\lst P_i'):1 \le i \le |\lst P'|, \lst s'_i=1\}$, $\set O:=\unpack(\lst P)\backslash\set X$ and $\set O':=\unpack(\lst P')\backslash\set X'$.

We now prove each of the conclusion above;  note that in each conclusion we may assume that the conclusions prior to it have already been established, so we do so implicitly.

Also, when we say that we are assuming we are in ``Case 1'' or ``Case 2'', we mean that we are assuming that $(\set Q',\lst P',\lst s')$ followed from respectively Case 1 or Case 2 in the definition of $\optimizer$.

\begin{enumerate}
%@+others
%@+node:gcross.20110318151522.1412: *7* The set generated is an invariant
\item

Examination of the definition reveals that $\set Q'$ and $\lst P'$ are constructed entirely from products of elements in $\set Q$ and $\lst P$ so that $\genfun\paren{\set U(\set Q')\cup\set U(\lst P')}\subseteq\genfun\paren{\set U(\set Q)\cup\set U(\lst P)}$.

If $\optimizer(\set S,\set L)_{i+1}$ was defined using Case 1 then let $(b,a):=\lst P_k$ and $(b',a'):=\lst P'_k$ where $k$ is the integer described in Case 1;  otherwise let $(a,b):=q$ be the pair selected from $\set Q$ in the definition and $(a',b')$ be the last element of $\lst P'$.  Note that in either case, $a=a'$.

In both cases, observe that for every operator $o\in\set U(\set Q)\cup\set U(\lst P)\backslash\{b\}$ we have that either $o$ or $o\cdot a$ is contained in $\set U(\set Q')\cup\set U(\lst P')$, and since $a$ is also contained in this set we see immediately that any operator in $\set U(\set Q)\cup\set U(\lst P)\backslash\{b\}$ can be obtained from products of elements in $\set U(\set Q')\cup\set U(\lst P')$ (i.e., from an operator in $\set U(\set Q')\cup\set U(\lst P')$ times possibly $a$).  Thus we conclude that $$\genfun\paren{\set U(\set Q)\cup\set U(\lst P)\backslash\{b\}}\subseteq\genfun\paren{\set U(\set Q')\cup\set U(\lst P')}.$$  Since $b'$ is the product of $b$ with elements in $$\genfun\paren{\set U(\set Q)\cup\set U(\lst P)\backslash\{b\}},$$ and $$\genfun\paren{\set U(\set Q)\cup\set U(\lst P)\backslash\{b\}}\subseteq\genfun\paren{\set U(\set Q')\cup\set U(\lst P')},$$ we conclude that $b\in\genfun\paren{\set U(\set Q')\cup\set(\lst P')}$ and so $$\genfun\paren{\set U(\set Q)\cup\set U(\lst P)}\subseteq\genfun\paren{\set U(\set Q')\cup\set U(\lst P')}.$$
Thus we have proven that $$\genfun\paren{\set U(\set Q')\cup\set U(\lst P')}=\genfun\paren{\set U(\set Q)\cup\set U(\lst P)}=\genfun(\set L),$$ and so we are done.
%@+node:gcross.20110318151522.1413: *7* The set generated stays in the centralizer
\item

This follows from the fact that $\set L\subseteq\centralizer_\pauligroup(\set S)\Rightarrow\genfun(\set L)\subseteq\centralizer_\pauligroup(\set S)$ and $\set U(\set Q)\cup\set U(\lst P)\subseteq\genfun\paren{\set U(\set Q)\cup\set U(\lst P)}=\genfun(\set L)$ (which we just proved).
%@+node:gcross.20110318151522.1414: *7* |L| = 2(|Q| + |P|)
\item

By construction, either $|\set Q'|=|\set Q|$ and $|\lst P'|=|\lst P$ (in Case 1) or $|\set Q'|=|\set Q|-1$ and $|\lst P'|=|\lst P|+1$ (in Case 2).  In either case we have that $2(|\set Q'|+|\lst P'|)=2(|\set Q|+|\lst P|)=|\set L|$.
%@+node:gcross.20110318151522.1415: *7* |L| = |U(Q)+U(P)|
\item

Since the elements in $\set L$ are members of conjugal pairs, they are therefore independent, and so we see that we need at least $|\set L|$ operators to generate $\genfun(\set L)$.  Thus we need $|\set L|\le|\set U(\set Q')\cap\set U(\lst P')|\le 2(|\set Q'|+|\lst P'|)=|\lst L|$,  where the second inequality comes from the fact that a pair can unpack to at most two operators, and the last equality comes from the previous conclusion.  We thus conclude that $|\set U(\set Q')\cap\set U(\lst P')|=|\lst L|$.
%@+node:gcross.20110318151522.1416: *7* No operator is repeated
\item

By combining the previous two conclusions we see that $|\set U(\set Q')\cup\set U(\lst P')|=2(|\set Q'|+|\lst P'|)$;  if this conclusion were false (i.e., an operator were repeated somewhere) then we would have that $|\set U(\set Q')\cup\set U(\lst P')|<2(|\set Q'|+|\lst P'|)$, which is contradicts our earlier results.
%@+node:gcross.20110318151522.1417: *7* Unimprovability is preserved
\item

First assume that we are in Case 1.  Let $k$ be the integer described in this case, $(a,b):=\lst P_k$, and $(a',b'):=\lst P'_k$.  Note that $a\in\set O$, and by construction $a':=\Pi(\set A')$ where $\set A'\subseteq \set O\cup\set X$ and $\set A'\cap\set O=\{a\}$, and so since by the inductive hypothesis we know that $\set O$ is a unimprovable set that extends to $\set X$ we know that $\om_{\set S}(a')\le\om_{\set S}(a)$.  Since by the inductive hypothesis we also know that $\max_{o\in\set O}\om_{\set S}(o)\le\min_{x\in\set X}(x)$, by Lemma \ref{combinations-can't-make-things-worse} we conclude that $\om_{\set S}(a')=\om_{\set S}(a)$.  Lemma \ref{lemma:replacing element with product preserves unimprovability} thus applies to our situation and allows us to conclude that $\set O'':=\paren{\set O\backslash\{a\}}\cup\{a'\}$ is an unimprovable set that extends to $\set X$.  Furthermore, since by construction $\set X'\subseteq\genfun(\set X)$ and $b\in\set X$, Lemma \ref{lemma:recombining extension elements preserves extension} allows us to conclude that $\set O''$ extends to $\{b\}\cup\set X'$.  By construction, there is an error of minimal weight that acts on $b$ but not on any other operator in $\set X'$, which means that by Lemma \ref{lemma:move-it-over} we conclude that $\set O''\cup\{b\}\equiv\set O'$ extends to $\set X'$.

Now assume that we are in Case 2.  Note that the only change from $\set O$ to $\set O'$ is the addition of a single element $o$ that has an error that acts only on it but not on any other operator in $\set X'$.  Note that since $\{o\}\cup\set X'\subseteq\genfun(\set X)$ we conclude from Lemma \ref{lemma:recombining extension elements preserves extension} that $\set O$ extends to $\{o\}\cup\set X'$, and from Lemma \ref{lemma:move-it-over} we conclude that $\set O'$ extends to $\set X'$.
%@+node:gcross.20110318151522.1418: *7* max w(O) <= min w(X)
\item

First observe that since $\set X'\subseteq\genfun(\set X)$, we conclude from Lemma \ref{combinations-can't-make-things-worse} that $\min_{x\in\set X}\om_{\set S}(x)\le\min_{x'\in\set X'}\om_{\set S}(x')$.

The difference between $\set O$ and $\set O'$ is the addition of a minimizer of $\om_{\set S}$ over $\set X$, $o$, and possible also the replacement of a single element.  Since $\max_{a\in\set O}\om_{\set S}(a)\le\min_{a\in\set X}\om_{\set S}(a)\le\min_{a\in\set X'}\om_{\set S}(a)$, we conclude that since $\om_{\set S}(o)=\min_{a\in\set X}\om_{\set S}(a)$ that therefore $\max_{a\in\set O\cup\{o\}}\om_{\set S}(a)\le\min_{a\in\set X'}\om_{\set S}(a)$.  If $\set O'=\set O\cup\{o\}$ then we are done.  Otherwise, we are in Case 1 which means that we have also replaced an element in $\set O$;  however, the operator we have replaced it with is the product of an operator from $\set O$ and operators from $\set X$, and since $\set O$ is an unimprovable set that extends to $\set X$ we conclude that the replacement can be no better than the operator it is replacing.  Thus, $\max_{a\in\set O'}\om_{\set S}(a)\le\min_{a\in\set X'}\om_{\set S}(a)$.
%@+node:gcross.20110318151522.1419: *7* om . p_1 = m
\item

Since $\max_{a\in\set O'}\om_{\set S}(a)\le\min_{a\in\set X'}\om_{\set S}(a)$, we immediately conclude that $(\om_{\set S}\circ p_1)(\lst P'_i)=m_{\set S}(\lst P'_i)$ when $\lst a'_i=1$.  By the inductive hypothesis, we know that $(\om_{\set S}\circ p_1)(\lst P'_i)=m_{\set S}(\lst P'_i)$ where $\lst P'_i=\lst P_i$;  furthermore, in both cases the pairs at the locations where $\lst a_i=0$ are unchanged from $\lst P$ to $\lst P'$, and in each case this turns out to leave just a single location that we still need to examine.

In Case 1, this location is the index $k$ described in that case, where $\lst a_k=1$ and $\lst a'_k=0$.  Since $p_1(\lst P'_k)$ is the product of a single element of $\set O$ and elements from $\set X$, we conclude from the fact that $\set O$ is an unimprovable set that extends to $\set X$ that $(\om_{\set S}\circ p_1)(\lst P'_k)\le(\om_{\set S}\circ p_1)(\lst P_k)$.  By the inductive hypothesis we know that $\max_{a\in\set O}\om_{\set S}(a)\le\min_{a\in\set X}\om_{\set S}(a)$.  Because $p_2(\lst P'_k)$ is a product of elements from $\set X$ we conclude from Lemma \ref{combinations-can't-make-things-worse} that $\min_{a\in\set X}\om_{\set S}(a)\le (\om_{\set S}\circ p_2)(\lst P'_k)$.  Since $p_1(\lst P)\in\set O$, we conclude that $(\om_{\set S}\circ p_1)(\lst P_k)\le\min_{a\in\set X}\om_{\set S}(a)$.  Combining all of these inequalities we reach the conclude that $(\om_{\set S}\circ p_1)(\lst P'_k)\le(\om_{\set S}\circ p_2)(\lst P'_k)$ and hence $(\om_{\set S}\circ p_1)(\lst P'_k)=m_{\set S}(\lst P'_k)$.

In Case 2, this location is the end of the sequence $\lst P'$, but since the addition to the sequences is a pair of operators from $\set X$ such that the first member is a minimizer of $\om_{\set S}$ over $\set X$ we conclude that $(\om_{\set S}\circ p_1)(\lst P'_{|\lst P'|})=m_{\set S}(\lst P'_{|\lst P'|})$.
%@+node:gcross.20110318151522.1420: *7* M(P) is ordered
\item

By the inductive hypothesis we have that $\lst M_{\set S}(\lst P)_i=(\om_{\set S}\circ \om_{\set S})(\lst P_i)$, and we have just shown that $\lst M_{\set S}(\lst P')_i=(\om_{\set S}\circ \om_{\set S})(\lst P'_i)$.  By the inductive hypothesis we know that $\lst M_{\set S}(\lst P)$ is ordered, and so to prove that $\lst M_{\set S}(\lst P')$ is ordered we need only check the places in the sequence where $p_1(\lst P_i)\ne p_1(\lst P'_i)$.  In both cases there is exactly one location where the first member of a pair is modified from $\lst P$ to $\lst P'$.

In Case 1, this is the index $k$ defined in that case, at which the first member was replaced with a product of that first member with elements in $\set X$.  Since this member is in $\set O$, and since $\max_{a\in\set O}\om_{\set S}(a)\le\min_{a\in\set X}\om_{\set S}(a)$ (by the inductive hypothesis), we conclude from the fact that $\set O$ is an unimprovable set that extends to $\set X$ that $(\om_{\set S}\circ p_1)(\lst P'_k)=(\om_{\set S}\circ p_1)(\lst P_k)$, and so we conclude that $\lst M_{\set S}(\lst P')$ is ordered.

In Case 2, this is the end of the sequence $\lst P'$ where a pair was appended to $\lst P$.  Since the pair contains elements from $\set X$, and $\max_{a\in\set O}\om_{\set S}(a)\le\min_{a\in\set X}\om_{\set S}(a)$, we conclude that $\lst M_{\set S}(\lst P')_{|\lst P'|}\ge\lst M_{\set S}(\lst P)_i$ for $i<|\lst P'|$, and so we conclude that $\lst M_{\set S}(\lst P')$ is ordered.
%@+node:gcross.20110318151522.1421: *7* P is a choice of qubits
\item

The fact that $\lst P$ is a choice of logical qubits stabilized by $\set S$ follows from directly from the previous conclusions.
%@+node:gcross.20110318151522.1422: *7* P is optimal
\item

From the definition of an unimprovable set it is easy to see that since $\set O'$ is an unimprovable set that extends to $\set X'$, it also extends to $\set X'\cup\set O'=\set U(\set Q')\cup\set U(\lst P')$.  Since $\{p_1(q):q\in\lst P'\}\subseteq \set O'$ and $\set U(\lst P')\subseteq \set X'\cup\set O'$, it is also easy to see from the definition that $\{p_1(q):q\in\lst P'\}$ is an unimprovable set that extends to $\set U(\lst P')$ --- that is, taking subsets does not affect the property of unimprovability.  Thus, we conclude from Theorem \ref{theorem:optimality-condition} that $\lst P'$ is therefore an optimal choice of qubits.
%@-others
\end{enumerate}
\end{proof}
%@+node:gcross.20110318151522.1423: *6* Proof of Theorem
\begin{remark}
Now that the heavy lifting has been done by the preceding Proposition, the proof of Theorem \ref{theorem:algorithm is correct} is relatively simple.
\end{remark}

\begin{proof}[Proof of Theorem \ref{theorem:algorithm is correct}]
At every step in the algorithm, we either change an entry in $\lst s$ from 1 to 0 or remove an element from $\set Q$.  Since $\lst s$ is of finite length, as long as $\set Q$ is non-empty there will be a step at which another element is removed from it.  Thus, there is an index $k$ such that if $\lst O(\set S,\set L)_k=(\set Q,\lst P,\lst s)$ then $\set Q=\emptyset$, and by definition this is the last element of the sequence.  By Proposition \ref{proposition:properties of the algorithm} we know that $\lst P$ is optimal and also that $\genfun(\lst P)=\genfun(\lst L)$ (since $\set Q$ is empty), and so we are done.
\end{proof}
%@-others
%@+node:gcross.20110318151522.1424: *5* Running time of the algorithm
\subsection{Running time of the algorithm}

\label{subsubsection:running time analysis}

%@+others
%@+node:gcross.20110318151522.1425: *6* Introduce Theorem
In this section we analyze the running time of the optimization algorithm;  the result is presented in the following Theorem.

\begin{theorem}
\label{theorem:bound on running time}
Suppose we are given
\begin{itemize}
\item a set of commuting Pauli operators, $\set S$, acting on $N$ physical qubits;
\item a set of pairs, $\set L\subseteq\centralizer_\pauligroup(\set S)$, conjugal with respect to $\set U(\set Q)\cup\set S$;
\item and a set of Pauli operators $\set C$ such that $\genfun(\set C)=\centralizer_\pauligroup(\set S)$;
\end{itemize}
then the time needed to compute the sequence $\optimizer(\set S,\set L)$ is in the set $$O\paren{|\set C|^2+|\set L|(|\set L|+d)3^d\choose{N}{d}},$$ where $d:=\lst M(\lst P)_{|\lst P|}$ and $(\set Q,\lst P,\lst S)$ is the last element in the sequence (i.e., $\lst P$ is the desired optimal choice of qubits).
\end{theorem}
%@+node:gcross.20110318151522.1426: *6* Definitions
%@+node:gcross.20110318151522.1427: *7* pseudo-generator
\begin{remark}
Before proving this Theorem, we shall first prove a number of related Lemmas and Propositions.

The most complicated part of analyzing the running time of the optimization algorithm is analyzing the time needed to find the minimum weight undetectable error.  In fact, the procedure for doing this was not even described explicitly in the algorithm, so we shall now explain how we do it.

The algorithm we employ is based on the Brouwer-Zimmermann search algorithm, which searches for the minimum weight binary string satisfying some property given a set of binary string generators endowed with a multiplication operation defined to be the exclusive-or operation.  The Brouwer-Zimmermann algorithm works by using a Gaussian elimination analogue to express the generators in reduced row echelon form;  it then performs its search by examining all products of $r$ generators for increasing $r$.  When all of the products of $r$ generators have been enumerated, one knows that the set of strings that has yet to be enumerated has weight $r+1$ or greater, since the row-echelon form means that every string has a column for which it is the only string with a 1 in that column, and so a product of $k$ strings must have a weight of at least $k$.  Thus, as the search proceeds, there is a growing lower-bound on the weight of the binary string, and the search halts when a string has been found that matches this bound.

This algorithm cannot be immediately applied to the current problem because we are not working with binary strings, and in particular Pauli operators have a more complicated multiplication operation that binary strings.  Fortunately, in~\cite{White:2006fj} White and Grassl showed that the Brouwer-Zimmermann enumeration can be generalized.

The key difference between binary strings and Pauli operators is that binary strings only have two possible values in a given column, whereas Pauli operators have four.  Thus, whereas we only need one element to generate all of the possible values in a given column for a binary string, we need \emph{two} elements to generate all of the possible values in a given column for a Pauli operator.  Thus, rather than working with generators, we instead work with a generalization that White and Grassl call \emph{pseudo-generators}, which we shall define here as follows.
\end{remark}

\begin{definition}
A \emph{pseudo-generator} is a set of either 1 or 2 Pauli operators.  In an abuse of notation, we extend the functions $\set U$ and $\genfun$ to be respectively $\set G\mapsto \bigcup_{\set g\in\set G} \set g$ and $\set G\mapsto (\genfun\circ\set U)(\set G)$ when applied to a set of pseudo-generators.
\end{definition}
%@+node:gcross.20110318151522.1428: *7* pseudo-product (plus bounding Lemma)
\begin{remark}
(The preceding definition does not follow that of White and Grassl exactly; it has been specialized to our situation for the sake of simplicity.)

Unlike `normal' generators --- i.e., Pauli operators --- a product of generators is not a Pauli operator but rather a set of Pauli operators, which we define as follows.
\end{remark}

\begin{definition}
\label{definition:pseudo-product}
Suppose we are given a set of $r$ pseudo-generators $\set G$.  Let $\set X:=\{\genfun(\set g)\backslash\{I\}: \lst g\in\set G\}$, $\set Y$ be the $r-$ary Cartesian product of the $r$ sets contained in $\set X$, and $\set Z$ be the set consisting of the normal quantum operator product of the $r$ operators in each $r-$tuple in $\set Y$.  Then $\set Z$ is defined to be the \emph{pseudo-product} of the pseudo-generators in $\set G$.  For convenience, we define a function $\pseudoproduct$ such that $\pseudoproduct(\set G)$ is the pseudo-product of the pseudo-generators in $\set G$.
\end{definition}

\begin{remark}
The following Lemma places a bound on the size of the pseudo-product.
\end{remark}

\begin{lemma}
\label{lemma:bound-on-pseudo-product}
The pseudo-product of $r$ pseudo-generators contains at most $3^r$ operators.
\end{lemma}

\begin{proof}
Every set in $\set X$ described in Definition \ref{definition:pseudo-product} has a cardinality of either 1 or 3, and the size of $\set Y$ is equal to the product of the sizes of all the sets in $\set X$;  since $|\set X|=r$, we therefore conclude that the cardinality of $\set Y$ and hence the number of operators in the pseudo-product is at most $3^r$.
\end{proof}

\begin{corollary}[to Lemma \ref{lemma:bound-on-pseudo-product}]
\label{corolary:bound-on-pseudo-product}
Given a set of $r$ pseudo-generators, $\set G$, then the set $\set O:=\{f(o):o\in\genfun(\set G)\}$ can be computed in time $O((T+r) 3^r)$, where the time needed to compute $f$ is in $O(T)$.
\end{corollary}

\begin{proof}
From Lemma \ref{lemma:bound-on-pseudo-product} we know that there are at most $3^r$ operators in the pseudo-product, so $|\set O|\le 3^r$.  Furthermore, for every element in the set we first need to compute the corresponding operator in the pseudo-product, which requires $r$ time since it is the product of $r$ operators, and then we need to compute $f$, which by assumption requires a time in $O(T)$.
\end{proof}
%@+node:gcross.20110318151522.1429: *7* disjoint
\begin{remark}
In order to be able to place a lower bound on binary strings that have yet to be examined in the Brouwer-Zimmermann enumeration, we need the generators of the binary strings over which we are searching to have the property that each generator has a column such that it is the only generator with a 1 in that column, so that products of $r$ generators must have at least weight $r$.  Because we want to similarly place a bound on unexamined products of pseudo-generators, we generalize this property with the following definition.
\end{remark}

\begin{definition}
\label{definition:disjoint-pseudo-generators}
A set of pseudo-generators $\set G$ is said to be \emph{disjoint} if for every $\set g\in\set G$ there exists some physical qubit $k$ such that either $X_k$ or $Z_k$ (or both) anti-commutes with every operator in $\genfun(\set g)\backslash\{I\}$, but both $X_k$ and $Z_k$ commute with every operator in $\bigcup_{\set g'\in\set G\backslash\{\set g\}}\genfun(\set g')$.
\end{definition}

\begin{remark}
With the following Lemma, we show that the property of disjointness is exactly what we need to obtain the bounds that we want.
\end{remark}

\begin{lemma}
\label{lemma:disjoint-pseudo-generators-bound}
All of the operators in the pseudo-product of any $r$ (distinct) pseudo-generators chosen from a disjoint set of pseudo-generators have weight of at least $r$.
\end{lemma}

\begin{proof}
Every operator in the pseudo-product is the product of $r$ factors, each of which is associated with some distinct physical qubit $k$ such that it anti-commutes with either $X_k$ or $Z_k$ (or both) but every other factor commutes with both $X_k$ and $Z_k$;  thus, the product must anti-commute with at least $r$ single-qubit operators acting on distinct physical qubits, and so it must have a weight of at least $r$.
\end{proof}
%@+node:gcross.20110318151522.1430: *6* Lemma: Minimal weight search
\begin{remark}
Now that we have the concept of a disjoint set of pseudo-generators and a result showing that an operator in a pseudo-product of $r$ of them must have a weight of at least $r$, we present in the following Lemma an algorithm for searching through the space spanned by the pseudo-generators for an operator satisfying a given property.
\end{remark}

\begin{lemma}
\label{lemma:minimal-weight-search}
Given a set of pseudo-generators $\set G$ acting on $N$ physical qubits and a test function $f:\pauligroup\to\{0,1\}$ such that $f^{-1}(1)\cap\genfun(\set G)\ne\emptyset$, then a solution $o$ such that $f(o)=1$ and $$\om_{\set S}(o)=\min\{w(o'):o'\in \genfun(\set G), f(o')=1\}$$ can be computed in time $O\paren{(T+d)3^d\choose{|\set G|}{r}}$ where $d:=\min\paren{w(o),|\set G|}$ and $T$ is the time needed to compute $f$.
\end{lemma}

\begin{remark}
This Lemma follows directly from the results in~\cite{White:2006fj}, though the proof is included here both for completeness and also to show specifically how the results specialize to our case.  A pseudo-code representation of this algorithm can be found in Table \ref{table:find-weight-minimizer}.
\end{remark}

\begin{proof}
Define $\set C_r$ to be the set of all operators such that if $o\in\set C_r$ then there is some subset of exactly $r$ pseudo-generators from $\set G$ such that $o$ is contained in their pseudo-product.  Note that $\cup_r \set C_r = \genfun(\set G)$, so for every operator $o$ in the search space there is an integer $r$ such that $o\in\set C_r$.  Corollary \ref{corolary:bound-on-pseudo-product} shows that we can evaluate $f$ on every element of the pseudo-product of $r$ pseudo-generators in time $O((T+r)3^r)$, so since there are $\choose{|\set G|}{r}$ ways to choose $r$ pseudo-generators from $\set G$ we conclude that we can search $\set C_r$ for a solution to $f(o)=1$ in time $O\paren{(T+r) 3^r \choose{|\set G|}{r}}$.

From Lemma \ref{lemma:disjoint-pseudo-generators-bound} we conclude that $w(o)\ge r$ for every $o\in\set C_r$.  By extension this means that $w(o)\ge r$ for every $o\in\bigcup_{r'=r}^{|\set G|}\set C_{r'}$, and therefore that if $o\notin\bigcup_{r'=0}^{r-1}\set C_{r'}$ and $o\in\genfun(\set G)$ then $w(o)\ge r$.  Thus, if there exists an $r$ such that $r=\min\{w(o): o\in\bigcup_{r'=0}^{r-1}\set C_{r'},f(o)=1\}$ then we know that $r=\min\{w(o):o\in\genfun(\set O),f(o)=1\}$ --- that is, $r$ is \emph{exactly} the weight of the minimum weight solution to $f(o)=1$, since any operator in the search space that \emph{isn't} contained in $\bigcup_{r'=0}^{r-1}\set C_{r'}$ must have a weight of at least $r$.  Put another way, after having enumerated all of the elements in $\bigcup_{r'=0}^{r-1}\set C_{r'}$ we can check to see whether the smallest solution to $f$ we have seen so far (if any) has weight less than or equal to $r$, and if so we are done since we have found the minimal weight solution.

Now consider the procedure of searching through each $\set C_r$ starting with $r=0$.  We know that we will eventually find at least one solution to $f(o)=1$, since in this Proposition we have assumed that such an operator exists in the search space (by the assumption that $f^{-1}(1)\cap\genfun(\set G)\ne\emptyset$).  Furthermore, employing this procedure we will find the minimal solution $o$ no \emph{later} than after we have searched through $\set C_r$ for $r=0\dots w(o)$, since at that point all of the unexamined operators have a weight greater than $w(o)$.  Thus, we conclude that we shall find the minimal weight solution after having searched at most all of the elements in $\bigcup_{r=0}^{\min(w(o),|\set G|)} C_r$, which we can do in time $$O\paren{\sum_{r=0}^{\min(w(o),|\set G|)}(T+r) 3^r \choose{|\set G|}{r}}\subseteq O\paren{(T+d) 3^d \choose{N}{d}},$$ where $d:=\min(w(o),|\set G|)$.
\end{proof}
%@+node:gcross.20110318151522.1431: *6* Pseudocode: Compute-Minimal-Weight-Operator
\begin{table}
\begin{codebox}
\Procname{$\proc{Find-Weight-Minimizer}(f,\lst G)$}
\li $r \gets 1$
\li $m \gets \infty$
\li \While $m > r$ and $r \le |\lst G|$
\li \Do
\li     \For each $\lst H\subseteq \lst G$ such that $|\lst H|=r$,
\li     and each $o$ in the pseudo-product of $\lst H$
\li     \Do
\li         \If $\func{weight}(o) < m$
\li         \Then
\li             $(q,u) \gets f(o)$
\li             \If $q$ is $\textsc{true}$
\li             \Then
\li                 $m \gets \func{weight}(o)$
\li                 $\alpha \gets (o,u)$
\li                 \If $m = r$
\li                 \Then
\li                     \Goto \ref{li:found-the-minimum}
                    \End
                \End
            \End
        \End
\li     $r \gets r + 1$
    \End
\li \Return $\alpha$ \label{li:found-the-minimum}
\end{codebox}
\caption[Algorithm \proc{Find-Weight-Minimizer}]{Algorithm which finds the minimal weight operator in a given generating set that satisfies a given predicate.  For the sake of convenience, we also allow the query function to return auxiliary information that is returned to the caller along with the minimal weight operator.} \label{table:find-weight-minimizer}
\end{table}
%@+node:gcross.20110318151522.1432: *6* Lemma: Bounds
\begin{remark}
The proceeding Lemma is rather general, so we shall show how it specializes to our case.  First, however, we use the following three Lemmas to prove that our search space is generated by exactly $N$ pseudo-generators.
\end{remark}

\begin{lemma}
\label{lemma:lower bound on number of disjoint pseudo-generators}
Given a set of disjoint pseudo-generators, $\set G$, the largest subset $\set X\subseteq\set U(\set G)$ such that $\set X$ commutes has size $|\set X|\le|\set G|$.
\end{lemma}

\begin{proof}
If $\set X$ contained more that $\set G$ operators then by the pigeon hole principle there would have to be at least two operators from the same pseudo-generator, and thus which did not commute
\end{proof}

\begin{lemma}
\label{lemma:upper bound on number of disjoint pseudo-generators}
A set of disjoint pseudo-generators $\set G$ acting on $N$ qubits satisfies $|\set G|\le N$.
\end{lemma}

\begin{proof}
This follows directly from the definition and the pigeon hole principle.
\end{proof}

\begin{lemma}
\label{lemma:exact bound on number of disjoint pseudo-generators}
For any set of commuting operators $\set S$ acting on $N$ physical qubits, if $\set G$ is a set of disjoint pseudo-generators satisfying $\genfun(\set G)=\centralizer_\pauligroup(\set S)$ then $|\set G|= N$.
\end{lemma}

\begin{proof}
Since $\centralizer_\pauligroup(\set S)$ contains a subset of $N$ independent commuting operators, so must $\genfun(\set G)$ and therefore $\set U(\set G)$;  thus, Lemma \ref{lemma:lower bound on number of disjoint pseudo-generators} implies that $|\set G|\ge N$, and combining this bound with that given by Lemma \ref{lemma:upper bound on number of disjoint pseudo-generators} we see that $|\set G|=N$.
\end{proof}
%@+node:gcross.20110318151522.1433: *6* Lemma: Minimal weight search (specialized)
\begin{remark}
We now prove a Lemma which shows how the search specializes to the case of our qubit optimization algorithm.
\end{remark}

\begin{lemma}
\label{lemma:search for minimal weight undetectable error}
Given
\begin{itemize}
\item a set of Pauli operators $\set S$ acting on $N$ physical qubits,
\item a set of disjoint pseudo-generator $\set G$ such that $\genfun(\set G)=\centralizer(\set S)$, and
\item a non-empty set of Pauli operators $\set Q$ such that $\set Q\cap\set S=\emptyset$ and every operator in $\set Q$ is a member of a conjugal pair in relation to $\set Q \cup \set S$,
\end{itemize}
then a minimal weight undetectable error acting on any operator in $\set Q$ can be found in time $$O\paren{(|\set Q|+d)3^d\choose{N}{d}}$$ where $d:=\min\paren{w(o),N}$.
\end{lemma}

\begin{proof}
First observe that by Lemma \ref{lemma:exact bound on number of disjoint pseudo-generators} we know that $|\set G|=N$.

Define the function $f:\genfun(\set G)\to\{0,1\}$ by
$$f(o):=
\begin{cases}
1 & \exists\,\, q\in\set Q \,\,\text{such that}\,\, \{o,q\}=0\\
0 & \text{otherwise},
\end{cases}
$$
Note that solutions to $f$ are undetectable errors acting on $\set Q$, and also that this function can be computed in time $O(|\set Q|)$ by checking the commutator for each element in $\set Q$.  Furthermore note that for every operator in $\set Q$ there is another operator in $\set Q$ which anti-commutes with it, and also that $\set Q\subset \centralizer_\pauligroup(\set S)$.  Thus, since $\set Q$ is non-empty, there is at least one operator $o\in\genfun(\set G)$ such that $f(o)=1$.  Thus, by Lemma \ref{lemma:minimal-weight-search}, we know that we can compute a minimal weight solution to $f$ in time $$O\paren{(|\set Q|+d)3^d\choose{N}{d}}$$ where $d:=\min\paren{w(o),N}$.
\end{proof}
%@+node:gcross.20110318151522.1434: *6* Lemma: Computing disjoint pseudo-generators
\begin{remark}
In order to make use of the preceding result, we need to have a set of disjoint pseudo-generators whose pseudo-product covers our search space.  However, we usually start instead with a set of ordinary Pauli operators that generate this space.  Thus, we shall now show that the former can be computed from the latter --- i.e., that given a set of Pauli operators, we can compute a set of disjoint pseudo-generators that spans the same space.  First we present a Lemma that provides a criteria sufficient to show that a set of pseudo-generators is distinct.
\end{remark}

\begin{lemma}
\label{lemma:disjointness-equvalence}
Given a set of pseudo-generators, $\set G$, if there exists a map $p:\set U(\set G)\to \paren{\{X_k\}_k \cup \{Z_k\}_k}$ such that
\begin{enumerate}
\item for every $o\in\set U(\set G)$, $o$ is the unique operator in $\genfun(\set G)$ that anti-commutes with $p(o)$ and
\item for every $\set g\in\set G$, the operators in $\set g$ are both mapped by $p$ to single-qubit operators acting on the same physical qubit $k$, and they are the only such operators in $\genfun(\set G)$ that are mapped by $p$ to operators acting on $k$,
\end{enumerate}
then $\set G$ is disjoint.
\end{lemma}

\begin{proof}
For every $\set g\in\set G$, we conclude from property 2 of $p$ that there is some physical qubit $k$ such that every operator in $\set g$ is mapped by $p$ to either $X_k$ or $Z_k$, and hence by property 1 this means that every operator in $\genfun(g)$ anti-commutes with either $X_k$ or $Z_k$.  Since by property 1 we know that the choice of $X_k$ or $Z_k$ is different for each operator in $\genfun(\set g)$, we conclude that if there is more than one operator in $\set g$ then the product anti-commutes with \emph{both} $X_k$ or $Z_k$.  Finally, by property 2 we know that every operator in $\set g'$ for $\set g'\in\set \set G\backslash\{g\}$ commutes with $X_k$ and $Z_k$.
\end{proof}

\begin{remark}
We now show that any set of operators that we are using to generate a search space can be expressed equivalently as a set of disjoint pseudo-generators.
\end{remark}

\begin{lemma}
\label{lemma:computing-disjoint-pseudo-generators}
Given any a set of Pauli operators, $\set O$, there exists a set $\set G$ of pseudo-generators such that
\begin{enumerate}
\item $|\set G|\le|\set O|$,
\item $\genfun(\set O)=\genfun(\set G)$,
\item the map $p$ described in Lemma \ref{lemma:disjointness-equvalence} exists for $\set G$,
\end{enumerate}
and $\set G$ can be computed in time $O(|\set O|^2)$.
\end{lemma}

\begin{remark}
The structure of this proof bears some similarities to Proposition \ref{make-independent-using-elimination}.  In contrast with Proposition \ref{make-independent-using-elimination}, however, in the setting of this Lemma we are working with operators that in general will not commute.
\end{remark}

\begin{proof}
Proof by induction.  For the base case, we observe that if $\set O$ is empty, then the trivial set $\set G:=\emptyset$ and the trivial function $p$ whose domain is the empty set satisfy the requirements.

Now assume that this Lemma has been proven for sets of cardinality $n-1$, and suppose we are given a (non-empty) set $\set O$ of cardinality $n$.  Take any operator $o\in\set O$.  By recursive application of this Lemma, we know that we can construct the set $\set G':=\set G$ and the function $p':=p$ described in this Lemma given $\set O:=\set O\backslash\{o\}$ in time $O\paren{(n-1)^2}=O(n^2)$.

Let $$o':=o\cdot \prod_{x\in\set U(\set G'),\,\,\{o,p(x)\}=0} x.$$  Note that for every $x\in\set U(\set G')$, it must be that $o'$ commutes with $p'(x)$, since $o'$ is formed from a product that has either two factors that anti-commute with $p'(x)$ (namely, $o$ and $p'(x)$) or no operators that anti-commute with $p'(x)$.  If $o'$ is the identity operator, then let $\set G:=\set G'$ and $p:=p'$ and we are done.  Otherwise, there must be some operator $z\in\paren{\{X_k\}_k \cup \{Z_k\}_k}\slash\{p'(x):x\in\set U(\set G')\}$ that anti-commutes with $o$.  Define the function $f$ by
$$f(x) :=
\begin{cases}
x \cdot o' & \{x,z\}=0, \\
x          & \text{otherwise},
\end{cases}
$$
and let $\set G'':=\left\{\{f(x):x\in\set g\}:\set g\in\set G'\right\}$ and $p'':=p'\circ f^{-1}$.  Note that $o'$ must be independent of the operators in $\set U(\set G')$, because $o'$ is not the identity and the product of $o'$ with any subset of operators $\set A\subset\set U(\set G')$ cannot be the identity since it must anti-commute with $p'(a)$ for every $a\in\set A$.  Thus, $f$ is a bijective map from $\set U(\set G')$ to $\set U(\set G'')$ and hence is invertible, and so we conclude that $p''$ is well-defined.  Since, as previously discussed, $o'$ commutes with $p(y)$ for every $y\in\set U(\set G')$, we conclude that multiplication by $o'$ does not change whether any operator $x\in\set U(\set G')$ commutes or anti-commutes with $p(y)$ for any $y\in\set U(\set G')$, and so we conclude that the properties listed in Lemma \ref{lemma:disjointness-equvalence} that $p'$ has in relation to $\set G'$ (from the inductive hypothesis) are preserved in the transformation by $f$ so that $p''$ also has the same properties in relation to $\set G''$.  Furthermore, since every operator $x\in\set U(\lst G')$ was multiplied by a factor of $o'$ if and only if it anti-commutes with $z$, we conclude that $f(z)$ must commute with $z$, and thus every operator in $\set U(\lst G'')$ must commute with $z$.

There are two cases to consider:  either there is no operator $x\in\set U(\set G'')$ such that $p''(x)$ acts on the same qubit as $z$, or there is exactly one, since if there were more than two then it would violate the properties of $p''$, and if there were exactly two then by construction $o'$ would commute with $z$ leading to a contradiction.  In the first case, let $\set G:=\set G''\cup\left\{\{o'\}\right\}$.  In the second case, let $\set G:=\paren{\set G''\backslash\left\{\{x\}\right\}}\cup\left\{\{x,o'\}\right\}$, where $x$ is the single operator in $\set U(\set G'')$ such that $p(x)$ acts on the same qubit as $z$.   In either case, define
$$
p(x) :=
\begin{cases}
p''(x) & x\in \set U(\set G''), \\
z & x=o',
\end{cases}
$$
observing that it is well-defined since $\set U(\set G)=\set U(\set G'')\cup\{o'\}$.

To prove conclusion 1, we note that $\set G$ has at most one more element than $\set G'$ and $\set O$ always has one more element than $\set O\backslash\{o\}$, so conclusion 1 follows from this fact combined with the inductive hypothesis.

To prove conclusion 2, we note that since $o'$ (and hence $o$) is independent with respect to $\set U(\set G')$, then because of how $\set G$ was constructed and the inductive hypothesis we have that $\genfun(\set G)=\genfun\paren{\set U(\set G'')\cup\{o'\}}=\genfun\paren{\set U(\set G')\cup\{o\}}=\genfun\paren{(\set O\backslash\{o\})\cup\{o\}}=\genfun(\set O)$.

To prove conclusion 3, we need to show that $p$ satisfies the properties listed in Lemma \ref{lemma:disjointness-equvalence}.  To prove the first property, we note that for every $x\in\set U(\set G)$ we have that either $x\in\set U(\set G'')$, in which case we have already shown that it is the unique operator that commutes with $p(x)$ as this is true for the operators in $\set U(\set G'')$ as well as for $o'$ (by construction), or $x=o'$, in which case this is still true since by construction $o'$ is the only operator in $\set U(\set G)$ that anti-commutes with $z$.  To prove the second property, we note that due to the inductive hypothesis we need only consider the single change from $\set G''$ to $\set G$, which consisted of either adding or replacing a pseudo-generator;  in the first case (adding a generator), observe that we showed earlier that no operator $y\in\set G''$ is such that $p''(y)$ acts on the same qubit as $z$, and in the second case (replacing a generator), note that we added $o'$ to the only generator in $\set G''$ containing an operator $y$ such that $p''(y)$ acts on the same qubit as $z$;  in either case, we see that the second property holds for $\set G$.

Finally, we consider the running time.  In addition to the $O(n^2)$ time required to construct $\set G'$, we required an additional $O(n)$ multiplication operations to construct $o'$ and $\set G''$;  hence the total running-time is $O(n^2)$.
\end{proof}

\begin{corollary}
\label{corolary:computing-disjoint-pseudo-generators}
Given any set of Pauli operators, $\set O$, there exists a disjoint set of pseudo-generators $\set G$ such that $|\set G|\le|\set O|$, $\genfun(\set O)=\genfun(\set G)$ and $\set G$ can be computed in time $O(|\set O|^2)$.
\end{corollary}

\begin{proof}
This follows immediately from Lemmas \ref{lemma:disjointness-equvalence} and \ref{lemma:computing-disjoint-pseudo-generators}.
\end{proof}
%@+node:gcross.20110318151522.1435: *6* Pseudocode: Compute-Pseudo-Generators
\begin{table}
{\tiny
\begin{codebox}
\Procname{$\proc{Compute-Pseudogenerators}(\lst O)$}
\li $\lst p \gets []$
\li $i \gets 0$
\li \While $i < |\lst O|$ \label{li:next-operator}
\li \Do
\li     $o \gets \lst O[i]$
\li     \For $j \gets 0$ \To $i-1$
\li     \Do
\li         $(n,z) \gets \lst p[j]$
\li         \If $z = 0$
\li         \Then
\li             \If $\func{anti}(o,X_n)$
\li             \Then $o \gets o \cdot \lst O[j]$
                \End
\li         \Else
\li             \If $\func{anti}(o,Z_n)$
\li             \Then $o \gets o \cdot \lst O[j]$
                \End
            \End
        \End
\li     \If $o$ is identity
\li     \Then
\li         delete $\lst O[i]$
\li         \Goto \ref{li:next-operator}
        \End
\li     \For $n \gets 0$ \kw{to} number of physical qubits
\li     \Do
\li         \If $\func{anti}(o,X_n)$
\li         \Then
\li             $z \gets 0$
\li             \Goto \ref{li:found-anti-commuting-pauli}
\li         \ElseIf $\func{anti}(o,Z_k)$
\li         \Then
\li             $z \gets 1$
\li             \Goto \ref{li:found-anti-commuting-pauli}
            \End
        \End
\li     \If $z = 0$ \label{li:found-anti-commuting-pauli}
\li     \Then
\li         \For $j \gets 0$ \kw{to} $i-1$
\li         \Do
\li             \If $\func{anti}(\lst O[k],X_n)$
\li             \Then $\lst O[j] \gets \lst O[j] \cdot o$
                \End
            \End
\li     \Else
\li         \For $j \gets 0$ \kw{to} $i-1$
\li         \Do
\li             \If $\func{anti}(\lst O[j],Z_n)$
\li             \Then $\lst O[j] \gets \lst O[j] \cdot o$
                \End
            \End
        \End
\li     append $(n,z)$ to $\lst p$
\li     $\lst O[i] \gets o$
\li     $i \gets i + 1$
    \End
\li $\lst G \gets []$
\li \For $n\gets 0$ \To number of physical qubits
\li \Do
\li     $\lst g \gets []$
\li     \For $i \gets 0$ \To $|\lst O|-1$
\li     \Do
\li         $(n',\_)\gets \lst p[i]$
\li         \If $n=n'$
\li         \Then append $\lst O[i]$ to $\lst g$
            \End
        \End
\li     \If $\lst g \ne []$
\li     \Then append $g$ to $\lst G$
        \End
    \End
\li \Return $\lst G$
\end{codebox}
}
\caption[Algorithm \proc{Compute-Pseudogenerators}]{Algorithm which computes a set of disjoint pseudo-generators that generates the input set of operators.} \label{table:compute-pseudo-generators}
\end{table}
%@+node:gcross.20110318151522.1436: *6* Proof of Theorem
\begin{remark}
We now have the tools that we need to analyze the running time of the algorithm.
\end{remark}

\begin{proof}[Proof of Theorem \ref{theorem:bound on running time}]
First observe that from Lemma \ref{lemma:disjointness-equvalence} we conclude that we can compute a set of disjoint pseudo-generators $\set G$ such that $\genfun(\set G)=\genfun(\set C)=\centralizer_\pauligroup(\set S)$ in time $O(|\set C|^2)$.  We will assume that this set of pseudo-generators is implicitly available to us throughout the algorithm so that we do not need to compute it more than once.

At each step of the algorithm, we first need to find an operator, $o$, that has an undetectable error of minimal weight inside a set which we know from Proposition \ref {proposition:properties of the algorithm} has at most $|\set L|$ elements.  By Lemma \ref{lemma:search for minimal weight undetectable error}, we conclude that this operator can be found in time $$O\paren{(|\set L|+d)3^d\choose{N}{d}}$$ where $d:=\min\paren{w(o),N}=w(o)$ (since the weight of any operator cannot be greater than $N$).  After this has been found, examination of the algorithm reveals that the computation performed afterward takes a running time in $O(|\set Q|+|\lst P|)=O(|\set L|)$, where the equality comes from Proposition \ref {proposition:properties of the algorithm}.  Thus, the total time needed for each step is in $$O\paren{(|\set L|+d)3^d\choose{N}{d}+|\set L|}=O\paren{(|\set L|+d)3^d\choose{N}{d}}.$$

Let $(\set Q',\lst P',\lst s')$ be the second to last element of $\optimizer(\set S,\set L)$ and $(\set Q,\lst P,\lst s)$ the last element.  In the final step of the algorithm, we move the last remaining pair in $\set Q'=\{q\}$ over to $\lst P'$, which means that the operator $o$ with the minimal weight is a member of $q$.  From Proposition \ref {proposition:properties of the algorithm}, we know that that $\om_{\set S}(o)\ge m_{\set S}(\lst P'_i)$ for any $i$.  Thus, at each step of the algorithm before this one we know that we spent a time in $$O\paren{(|\set L|+d)3^d\choose{N}{d}}$$ where $d:=w(o)$ --- i.e., a time no greater than the time spent on the last step.  Since the algorithm requires at most $|\set L|$ steps we conclude that the total running time, including that needed to compute the set of pseudo-generators, is in $$O\paren{|\set G|^2+|\set L|(|\set L|+d)3^d\choose{N}{d}}.$$  Since $\om_{\set S}(o)=\lst M(\lst P)_{|\lst P|}$, we conclude that $d\equiv\lst M(\lst P)_{|\lst P|}$ (since there can be at most $N$ qubits in the choice), and so we are done.
\end{proof}
%@-others
%@-others
%@+node:gcross.20110318151522.1451: *3* Practice
\chapter{Practice} \label{sec:lattice}
%@+node:gcross.20110318151522.1452: *4* Methodology
\section{Methodology} \label{sec:methodology}

In the previous section we presented an algorithm that computes the optimal subsystem code that can be implemented using a given set of measurements.  The procedure for optimizing the code requires an exponential amount of time, but fortunately the power of the exponential is a function of the distance of the best qubit in the code.   Because of this property, this algorithm can be effectively applied to search over a set of choices of measurement operators to see if there is any good choice for implementing a code, since it can (relatively) quickly skip over the bad choices of measurements.

In this section, we shall present an example of applying this algorithm to search for codes on quantum systems with the structure of a graph.  That is, we assume that we have a system of qubits, 2-body Pauli measurement operators and a graph such that there is a bijection between the qubits and vertices and between the edges and measurement operators, and also such that each measurement only acts on the two qubits corresponding to the vertices adjacent to its associated edge.  Specifying a particular graph constrains the number of qubits and the types of measurement operators, but it still allows a great deal of freedom in the choice of the measurement operator at each edge.  In Figure \ref{figure:2labelings} we illustrate an example of a graph with two possible such choices of measurement operator labelings;  note that for the sake of generality we do not impose the constraint that the two operators in the 2-body measurement be identical.

\begin{figure}
\begin{center}
\includegraphics[width=2in]{images/2labelings-1}
\includegraphics[width=2in]{images/2labelings-2}
\end{center}
\caption{
\label{figure:2labelings}
A figure illustrating two possible labelings of a graph, which correspond to two possible choices of 2-body measurements.  In both graphs we see that there are four vertices and five edges, which indicates that our system is constrained to have four qubits and five 2-body measurement operators.  The edges (without the labels) indicate the pairs of qubits on which the 2-body measurement operators are constrained to act within our system.  Within these constraints, we see in this figure two possible choices of measurement operators as specified by the two labelings of the edges of the graph.
}
\end{figure}

For reasons that will become clear, it turns out to be useful to specify choices of measurement operators in terms of ray labelings rather than edge labelings since the former is associated with vertices.  Define a \emph{ray} of a graph to be a pair consisting of a vertex and an edge adjacent to the vertex;  note that every ray can be uniquely associated with an edge, and every edge can be associated uniquely with a ray for each of its two incident vertices.  Thus, we can define a particular choice of measurement operators by labeling each ray in the graph with a single-qubit Pauli operator acting on the qubit of the incident vertex, and then letting the measurement operator associated with each edge be equal to the product of the single-qubit operators in the edge's two rays.

There is a natural symmetry of quantum codes that can be factored out to reduce the search space:  the relevant properties of the code are invariant under single-qubit rotations.  That is, transformations such as swapping the $X$ and $Z$ operators at the location of a single physical qubit in every stabilizer, gauge qubit, and logical qubit operator does not affect the code.  Thus, when labeling the rays of a vertex, exactly which ray is labeled $X$, $Y$, and $Z$ is not important;  what matters is which rays commute and which rays anti-commute.  We see therefore that we need only search over the possible ways to divide the rays into three indistinguishable groups, so that a vertex with $n$ rays only has $1+\frac{3^{n-1}-1}{2}$ relevant labelings that need to be examined.

The specific graphs we shall examine in this section are lattices generated by nine of the eleven convex vertex-uniform (also known as the ``Archimedean'') tilings of the plane --- that is, those tilings with the property that every face is convex and every vertex has the same sequence of faces~\cite{Gruenbaum1987}\footnote{In particular, see Theorem 2.1.3 on page 59 of Ref. \cite{Gruenbaum1987} for the proof that there are exactly eleven tilings with this property.}.  Since these tilings have many translational symmetries, we intentionally narrow our search to the set of labelings that share the translational symmetries of the lattice\footnote{This is not to claim that there are no interesting codes that break these translational symmetries;  however, the investigation of such codes is outside the scope of this particular study.}.  Since the ray labelings must be preserved under these symmetries, we can partition the rays of the graph into equivalence classes such that two rays are equivalent if and only if they are related by a translation symmetry;  thus we see that our narrowed search space is equivalent to the space of possible labelings of each \emph{class} of rays in the lattice examined.  Since there is a symmetry that can be factored out at each vertex (as discussed previously), we note that we can likewise partition the vertices into equivalence classes of vertices related by translation symmetries.  If there are $m$ vertex equivalence classes, and every vertex has $n$ rays, then our search space consists of $\paren{1+\frac{3^{n-1}-1}{2}}^m$ total possible labelings.  In Table \ref{table:combinatorics}, we list the eleven convex vertex-uniform tilings with the number of vertex equivalence classes, the number of rays at each vertex, and the total number of labelings.  (Two of the eleven tilings, ``truncated hexadeltille'' and ``snub hextille'', had such a large number of possible labelings that we decided to exclude them from our search.)

\begin{table}
\begin{tabular}{lrrr} \toprule
Archimedean Tiling & \# Classes & \# Rays & \# Labelings \\ \midrule
quadrille & 1 & 4 & 14\\
truncated quadrille & 4 & 3 & 625\\
snub quadrille & 4 & 5 & 2,825,761\\
isosnub quadrille & 2 & 5 & 1681\\
hextille & 2 & 3 & 25\\
truncated hextille & 6 & 3 & 15,625\\
snub hextille & 6 & 5 & 4,750,104,241\\
deltille & 1 & 6 & 122\\
hexadeltille & 3 & 4 & 2744\\
truncated hexadeltille & 12 & 3 & 244,140,625\\
rhombihexadeltille & 6 & 4 & 7,529,536\\ \bottomrule
\end{tabular}
\caption[Combinatorics of the tilings]{
\label{table:combinatorics}
A table listing the number of vertex equivalence classes, the number of rays at each vertex, and the total number of labelings for each of the 11 convex vertex-uniform tilings.  By our scheme the number of labelings is equal to $\paren{1+\frac{3^{n-1}-1}{2}}^m$, where $m$ is the number of vertex equivalence classes and $n$ is the number of rays at each vertex.}
\end{table}

Note that we could furthermore refine our search to consist of those codes which also share the \emph{rotational} symmetries of the lattice.  We explicit avoid making this refinement because the existence of such codes as the quantum compass model code~\cite{Bacon:06a} indicates that there are good codes on lattices that require breaking the rotational symmetry of the lattice.  However, we can use the rotational symmetries in a different way to reduce the search space as follows.  Partition the labelings into equivalence classes such that two labelings are in the same class if and only if there is a rotational symmetry that relates them, and observe that all of the labelings in each class will give rise quantum codes with identical properties.  Thus, we can reduce our search space to ignore redundant labelings by only examining one labeling in each equivalence class.

Our search algorithm thus works in the following manner.  We start by putting a total ordering on all of the lattice labelings (after having factored out the symmetry at each vertex.)  We enumerate these labelings in order.  For each labeling, we generate new labelings by applying each rotational symmetry to the current labeling.  If any of these new labelings is less than the current labeling under our ordering, then we skip the current labeling because we know that we have already previously examined an equivalent labeling.  Although this algorithm proceeds serially through the search space, it can be parallelized by making use of $n$ walkers, each of which starts at a different labeling (from $0$ to $n-1$) and which proceed by examining the current labeling and then skipping directly to the $n^{\text{th}}$ labeling after the current one.  In Table \ref{table:count-of-labels-scanned} we list the number of non-redundant labelings for each tiling.

\begin{table}
\begin{tabular}{lrrr} \toprule
Archimedean Tiling & \# Non-redundant & \# Total \\ \midrule
quadrille & 10 & 14\\
truncated quadrille & 155 & 625\\
snub quadrille & 706,881 & 2,825,761\\
isosnub quadrille & 743 & 1681\\
hextille & 11 & 25\\
truncated hextille & 2392 & 15,625\\
deltille & 58 & 122\\
hexadeltille & 594 & 2744\\
rhombihexadeltille & 904,741 & 7,529,536\\ \bottomrule
\end{tabular}
\caption[Number of non-redundant labelings in each tiling]{
\label{table:count-of-labels-scanned}
A table listing the number of labelings for each tiling that were not redundant under rotational symmetry transformations.  This number was obtained by placing an ordering on the labelings and counting the number of labelings such that no symmetry transformation obtained a labeling less than the current labeling.  Also listed for the sake of comparison are the total number of labelings from Table \ref{table:combinatorics}.}
\end{table}

In order to preserve the rotational symmetries of the tiling, it is important that the lattice be constructed such that the center of the lattice is at a point of rotational symmetry.  There is not a single unique center point that preserves all of the rotational symmetries of a given tiling, and furthermore for many tilings there are multiple rotational symmetry groups (known as ``wallpaper'' symmetry groups), each of which has a different set of center points.  We thus chose the center of our lattice by picking the largest of the wallpaper groups present in the tiling and choosing the center to give rise to the rotational symmetries in that group\footnote{Note that this approach does not mean that we have eliminated redundant labelings resulting from \emph{all} of the symmetries in the lattice.  For example, we have not eliminated labelings which are equivalent under rotations around a different point, nor which are equivalent under a glide-reflection symmetry.  It is certainly possible to eliminate these labelings, but we choose not to do so in this project for the sake of simplicity.  An implication of this is that for many codes we expect to see multiple labelings giving rise to them that are equivalent under symmetry transformations but not eliminated by our approach.}.  In table \ref{table:symmetries} we list the wallpaper symmetries for each of the 11 convex vertex-uniform tilings along with (where applicable) the particular symmetry group that we chose to utilize.

\begin{table}
\begin{tabular}{llr} \toprule
Archimedean Tiling & Symmetries & Chosen\\ \midrule
quadrille & p4m & p4m \\
truncated quadrille & p4m & p4m \\
snub quadrille & p4g, p4, and pg & p4 \\
isosnub quadrille & cmm & cmm \\
hextille & p6m & p6m \\
truncated hextille & p6m and p3m1 & p6m \\
snub hextille & p6 & N/A \\
deltille & p6m and p3m1 & p6m \\
hexadeltille & p6m and p3m1 & p6m \\
truncated hexadeltille & p6m & N/A \\
rhombihexadeltille & p6m & p6m \\
\bottomrule
\end{tabular}
\caption[Symmetry groups of the tilings]{
\label{table:symmetries}
A table listing the wallpaper symmetry groups (using crystallographic notation) for each of the 11 convex vertex-uniform tilings, along with the particular group that we chose for our search.  For two of the tilings no symmetry group was chosen because we decided not to search the tiling.
}
\end{table}

As is usually the case in physical systems, it is important to pay careful attention to the boundary conditions of the lattice.  In order to minimize boundary effects, we decided to put periodic boundary conditions on our lattices;  care had to be taken to impose the periodic boundary conditions in such a way as to preserve the rotational symmetry group.  For example, a boundary that only wraps from left to right and from top to bottom breaks some of the rotational symmetries for hexagonal tilings.  In Figures \ref{figure:boundaries1}  and \ref{figure:boundaries2} we illustrate how we placed the centers and the boundaries of the tilings.

\begin{figure*}
\centering
\subfloat[quadrille]{{\includegraphics[width=2in]{images/quadrille}}}
\subfloat[truncated quadrille]{{\includegraphics[width=2in]{images/truncated-quadrille}}}\\
\subfloat[snub quadrille]{{\includegraphics[width=2in]{images/snub-quadrille}}}
\subfloat[isosnub quadrille]{{\includegraphics[width=2in]{images/isosnub-quadrille}}}
\caption{
\label{figure:boundaries1}
A figure illustrating how we placed the centers and boundaries for each of the quadrille tilings that we scanned.  The boundaries are periodic, so that edges that pass through one side of the boundary wrap around to the opposite side.  Edges and vertices \emph{on} a boundary are merged with the corresponding edges and vertices on the opposite edge.
Note that under this scheme there can be no vertices on a corner.  For most tilings this will never happen, but it turns out that in the case of the deltille tiling there are vertices on the corners when the radius (smallest distance from the center to the boundary) is three times the radius of the unit cell;  we thus ignore deltille tilings of these sizes.
}
\end{figure*}

\begin{figure*}
\centering
\subfloat[hextille]{{\includegraphics[width=2in]{images/hextille}}}
\subfloat[truncated hextille]{{\includegraphics[width=2in]{images/truncated-hextille}}}\\
\subfloat[deltille]{{\includegraphics[width=2in]{images/deltille}}}
\subfloat[hexadeltille]{{\includegraphics[width=2in]{images/hexadeltille}}}\\
\subfloat[rhombihexadeltille]{{\includegraphics[width=2in]{images/rhombihexadeltille}}}
\caption{
\label{figure:boundaries2}
A figure illustrating how we placed the centers and boundaries for each of the detille and hextille tilings that we scanned.  See the caption of Figure \ref{figure:boundaries2} for more details.
}
\end{figure*}

Due to limits on our computational resources, we were limited in the size of the lattices that we could search with the algorithm.  We describe the size of the lattices using a quantity we call the `radius', which is an integral quantity equal to the length of the lattice divided by the length of the smallest lattice defined for that tiling;  the unit radius lattices are those illustrated in Figure {figure:boundaries}.  In Table \ref{table:maximum-radius-scanned} we show the maximum radius lattice that was completely scanned (i.e., such that every possible labeling was examined by the algorithm) for each tiling.

\begin{table}
\begin{tabular}{lcr} \toprule
Tiling & Maximum Radius & \# Qubits\\ \midrule
quadrille & 4 & 64\\
truncated quadrille & 6 & 576\\
snub quadrille & 5 & 200\\
isosnub quadrille & 8 & 768\\
hextille & 10 & 600\\
truncated hextille & 5 & 600\\
deltille & 8 & 256\\
hexadeltille & 3 & 108\\
rhombihexadeltille & 3 & 162\\
\bottomrule
\end{tabular}
\caption[Lattice sizes scanned for each tiling]{
\label{table:maximum-radius-scanned}
A table in which we show the maximum radius lattice that was completely scanned (i.e., such that every possible labeling was examined by the algorithm) for each tiling.  To give a sense of the size of the lattices involved, we also list the number of physical qubits (corresponding to vertices) for the lattice with the maximum radius.
}
\end{table}

Since each labeling of every lattice results in a quantum code, we had to provide some criteria for our search algorithm to decide whether a code was interesting enough to log.  We set our criteria relatively low:  a code was deemed to be interesting if there was at least one logical qubit with distance three, that is if there was at least one logical qubit such that a single arbitrary error on that qubit can be corrected.  This was done under the reasoning that as long as some of the logical qubits in a code are sufficiently useful to us to make implementing the code worthwhile, then we should not be troubled by the fact that there might be other logical qubits that are not useful because we can always ignore them (or, equivalently, classify them as gauge qubits).
%@+node:gcross.20110318151522.1453: *4* Results
\section{Results} \label{sec:results}

In the previous subsection we described the search space to which we applied the algorithm in order to computationally find possible codes that can be implemented using systems with 2-body interactions and a lattice structure following nine of the eleven convex vertex-uniform tilings.  In this subsection we present the results of this search.  The codes that we found are shown in the plots appearing in Figures \ref{figure:results-quadrille1}, \ref{figure:results-quadrille2}, \ref{figure:results-hextille1} and \ref{figure:results-hextille2}.  No plot appears for the deltille tiling because no codes were found for that tiling.  It is worth emphasizing that these codes indicated in these figures are \emph{all} of the (useful) codes that exist for the scanned lattices of that tiling given our constraints, since we scanned every possible labeling that was not redundant under a rotational symmetry transformation about the center.

%@+<< Figures >>
%@+node:gcross.20110318151522.1454: *5* << Figures >>
\begin{figure*}
\centering
\subfloat[]{{\includegraphics[width=2.5in]{images/results-quadrille}}}\\
\subfloat[]{{\includegraphics[width=3.85in]{images/results-isosnub-quadrille}}}
\caption{
\label{figure:results-quadrille1}
A figure containing plots of the results from scanning the (a) quadrille and (b) isosnub quadrille.  Every polygon in the plot corresponds to a code that was found with a distance equal to the number of sides of the polygon, so that triangles indicate distance three codes, squares indicate distance four codes, etc.  The position along the $x$-axis indicates the radius of the lattice where the code was found, where the radius is an integer defined to be the length of the lattice divided by the length of the smallest possible periodic lattice for the tiling;  it also indicates the number of physical qubits in the lattice where the code was found, which appears on the $x$-axis just under the value of the radius.  The position along the $y$-axis indicates the number of logical qubits with that distance in the code.
Note that in cases where multiple codes were found for the same radius and with the same number of logical qubits, multiple polygons are drawn, so that for example in plot (c) we see several cases in which a distance 3 code (triangle) and a distance 4 code (square) were found that were in lattices with the same radius and also had the same number of logical qubits.
}
\end{figure*}

\begin{figure*}
\centering
\subfloat[]{{\includegraphics[width=3.17in]{images/results-truncated-quadrille}}}\\
\subfloat[]{{\includegraphics[width=2.83in]{images/results-snub-quadrille}}}
\caption{
\label{figure:results-quadrille2}
A figure containing plots of the results from scanning the (a) truncated quadrille and (b) snub quadrille tilings.  See the caption of Figure \ref{figure:results-quadrille1} for an explanation of how to interpret these plots.
}
\end{figure*}

\begin{figure*}
\centering
\subfloat[]{{\includegraphics[width=4.5in]{images/results-hextille}}} \\
\subfloat[]{{\includegraphics[width=2.17in]{images/results-rhombihexadeltille}}}
\caption{
\label{figure:results-hextille1}
A figure containing plots of the results from scanning the (a) hextille and (b) rhombihexadeltille.  See the caption of Figure \ref{figure:results-quadrille1} for an explanation of how to interpret these plots.
}
\end{figure*}

\begin{figure*}
\centering
\subfloat[]{{\includegraphics[width=2.83in]{images/results-truncated-hextille}}} \\
\subfloat[]{{\includegraphics[width=2.17in]{images/results-hexadeltille}}}
\caption{
\label{figure:results-hextille2}
A figure containing plots of the results from scanning the (a) truncated hextille and (b) hexadeltille tilings.  See the caption of Figure \ref{figure:results-quadrille1} for an explanation of how to interpret these plots.
}
\end{figure*}
%@-<< Figures >>

Observe that two kinds of trends appear frequently in the results:  codes that grow in distance but remain constant in the number of logical qubits as the radius increase, and codes that remain constant in distance but grow in the number of logical qubits as the radius increases.  The former trend appears in the quadrille, snub quadrille, isosnub quadrille, hexadeltille, and rhombihexadeltille tilings\footnote{Note that where the former trend was present, the maximum radius that we scanned was often quite limited;  this is due to the exponential explosion in the cost of finding the optimal code as a function of the distance of the code.}.  The latter trend appears in the truncated quadrille, snub quadrille, hextille, truncated hextille, hexadeltille, and rhombihexadeltille tilings.  In many of the tilings there are also codes that were found that do not seem to belong to an obvious trend.

In the follow subsections we will focus on some specifics of the results for each of the tilings.

%@+others
%@+node:gcross.20110318151522.1455: *5* quadrille
\subsection{quadrille}

For the quadrille lattice, we only saw one labeling, illustrated in Figure \ref{figure:quadrille-code-labeling}, that resulted in an interesting code.  This labeling corresponds to the compass model code, and the algorithm correctly found that the distance of the code grows linearly with the radius of the lattice and is exactly equal to the square root of the number of qubits in the lattice.  This result is not terribly surprising, but it is good to see that our search technique employing the algorithm can correctly duplicate known results.

\begin{figure}
\includegraphics[width=3in]{images/quadrille-code-labeling}
\caption{
\label{figure:quadrille-code-labeling}
A figure illustrating the single labeling of the quadrille tiling (on a radius 2 lattice) that results in a useful quantum code.
}
\end{figure}
%@+node:gcross.20110318151522.1456: *5* truncated quadrille
\subsection{truncated quadrille}

There are three kinds of codes that appear in this tiling where the number of qubits increases with the radius:  two where the distance is fixed at 4, and one where the distance is fixed at 3.  For the best two of these three kinds of codes, the number of logical qubits ($l$) is related to the radius ($r$) by $l=(2r-1)^2$.    Since the number of physical qubits ($n$) is given by $n=(4r)^2$, the number of logical qubits per physical qubit is thus given by $\frac{l}{n}=\paren{\half-\frac{1}{r}}^2$, a quantity which converges to $\frac{1}{4}$ as $r\to\infty$.  There were four labelings with this property that we saw in our search:  two with distance 3 qubits (illustrated in Figure \ref{figure:truncated-quadrille-code-3-labelings}), and two with distance 4 qubits (illustrated in Figure \ref{figure:truncated-quadrille-code-4-labelings}).

\begin{figure*}
\subfloat[distance 3 code labelings]{
\label{figure:truncated-quadrille-code-3-labelings}
\includegraphics[width=2.5in]{images/truncated-quadrille-code-3-labeling-1}
\includegraphics[width=2.5in]{images/truncated-quadrille-code-3-labeling-2}
}\\
\subfloat[distance 4 code labelings]{
\label{figure:truncated-quadrille-code-4-labelings}
\includegraphics[width=2.5in]{images/truncated-quadrille-code-4-labeling-1} 
\includegraphics[width=2.5in]{images/truncated-quadrille-code-4-labeling-2}}\\
\caption{
\label{figure:truncated-quadrille-code-labelings}
A figure illustrating four labelings of the truncated quadrille tiling (on a radius 1 lattice) that result in a quantum code with distance 3/4 (in respectively \ref{figure:truncated-quadrille-code-3-labelings}/\ref{figure:truncated-quadrille-code-4-labelings}), and a number of distance logical qubits proportional to the square of the radius of the labeling.
}
\end{figure*}
%@+node:gcross.20110318151522.1457: *5* snub quadrille
\subsection{snub quadrille}

%@+at
% (select labeling from code_best_distances where tiling = 'snub quadrille' and radius = 3 and distance = 3 and number_of_qubits=1) intersect
% (select labeling from code_best_distances where tiling = 'snub quadrille' and radius = 4 and distance = 4 and number_of_qubits=1) intersect
% (select labeling from code_best_distances where tiling = 'snub quadrille' and radius = 5 and distance = 5 and number_of_qubits=1)
%  order by labeling;
%@@c

There are two kinds of interesting codes found in this filing.  First, we saw exactly one labeling that has the property that the distance is four and the number of logical qubits ($l$) is given by $l=2r(r-1)$, where $r$ is the radius of the lattice.  Since the number of physical qubits ($n$) is given by $n=8r^2$, this means that the number of logical qubits per physical qubit is given by $\frac{l}{n}=\frac{2r(r-1)}{8r^2}=\frac{1}{4}\paren{1-\frac{1}{r}}\to\frac{1}{4}$ as $r\to\infty$.  This labeling is illustrated in Figure \ref{figure:snub-quadrille-code-4-labeling}.

\begin{figure}
\includegraphics[width=3.5in]{images/snub-quadrille-code-4-labeling} % labeling #1836744
\caption{
\label{figure:snub-quadrille-code-4-labeling}
A figure illustrating a labeling of the snub quadrille tiling (on a radius 2 lattice) that results with distance 4 and a number of distance logical qubits proportional to the square of the radius of the labeling.
}
\end{figure}

Second, more usefully, we saw twelve labelings which result in a code that has one qubit whose distance grows with the size of the lattice.  Two of these labelings are illustrated in Figure \ref{figure:snub-quadrille-code-r-labeling}.

\begin{figure*}
\includegraphics[width=3.5in]{images/snub-quadrille-code-r-labeling-1} % labeling #180508
\includegraphics[width=3.5in]{images/snub-quadrille-code-r-labeling-2} % labeling #287828
\caption{
\label{figure:snub-quadrille-code-r-labeling}
A figure illustrating two of the labelings of the snub quadrille tiling (on a radius 2 lattice) that result in a quantum code with a single qubit whose distance grows with the radius of the lattice.
}
\end{figure*}
%@+node:gcross.20110318151522.1458: *5* isosnub quadrille
\subsection{isosnub quadrille}

We only saw two labelings of the isosnub lattice that result in useful codes, both of which only have a single qubit that seems (assuming that the trend seen in Figure \ref{figure:results-hextille} can be extrapolated) to have a distance that grows with the radius of the lattice.  These two labelings are illustrated in Figure \ref{figure:isosnub-quadrille-code-r-labeling}.

\begin{figure*}
\includegraphics[width=3in]{images/isosnub-quadrille-code-labeling-1} % labeling #346
\includegraphics[width=3in]{images/isosnub-quadrille-code-labeling-2} % labeling #764
\caption{
\label{figure:isosnub-quadrille-code-r-labeling}
A figure illustrating two of the labelings of the isosnub quadrille tiling (on a radius 1 lattice) that result in a quantum code with a single qubit whose distance grows with the radius of the lattice.
}
\end{figure*}
%@+node:gcross.20110318151522.1459: *5* deltille
\subsection{deltille}

We scanned this tiling up to a radius of eight;  no interesting codes were found in any of the 122 labelings.
%@+node:gcross.20110318151522.1460: *5* hextille
\subsection{hextille}

In this tiling we saw four labelings which resulted in two kinds of interesting codes:  two of the labelings (illustrated in Figure \ref{figure:hextille-code-3-labelings}) resulted in codes of distance 3 that were present for every value of the radius, and two of the labelings (illustrated in Figure \ref{figure:hextille-code-4-labelings}) resulted in codes of distance 4 that were only present for \emph{even} values of the radius.  The former resulted in codes which had a number of logical qubits ($l_3$) given by $l_3=\frac{(r-1)(r-2)}{2}$, where $r$ is the radius, and the latter resulted in codes which had a number of logical qubits ($l_4$) given by $l_4=r(r+3)$.  Since the number of qubits ($n$) is given by $n=6r^2$, we have that the number of logical qubits per physical qubit for the distance and distance 4 codes were given respectively by $d_3=\frac{l_3}{n}=\frac{1}{12}\paren{1-\frac{1}{r}}\paren{1-\frac{2}{r}}$ and $d_4=\frac{l_4}{n}=\frac{1}{6}\paren{1+\frac{3}{r}}$;  as $r\to\infty$, we have that $d_3\to\frac{1}{12}$ and $d_4\to\frac{1}{6}$.

It is interesting to observe that there is no distance/qubit count trade-off in this tiling.  As long as the radius is even, the distance 4 code is superior in \emph{both} distance \emph{and} logical qubit count over the distance 3 code.

\begin{figure*}
\subfloat[distance 3 code labelings] {
\label{figure:hextille-code-3-labelings}
\includegraphics[width=2.5in]{images/hextille-code-3-labeling-1} % labeling #9
\includegraphics[width=2.5in]{images/hextille-code-3-labeling-2} % labeling #19
}\\
\subfloat[distance 4 code labelings] {
\label{figure:hextille-code-4-labelings}
\includegraphics[width=2.5in]{images/hextille-code-4-labeling-1} % labeling #7
\includegraphics[width=2.5in]{images/hextille-code-4-labeling-2} % labeling #8
}
\caption{
\label{figure:hextille-code-labelings}
A figure illustrating four labelings of the hextille tiling (on a radius 2 lattice) that result in a quantum code with distance 3/4 (in respectively \ref{figure:hextille-code-3-labelings}/\ref{figure:hextille-code-4-labelings}), and a number of distance logical qubits proportional to the square of the radius of the labeling.
}
\end{figure*}
%@+node:gcross.20110318151522.1461: *5* truncated hextille
\subsection{truncated hextille}

In the truncated hextille there are four kinds of codes where the number of qubit increases with the radius:  three with the distance fixed at 3, and one with the distance fixed at 4.

The best of the distance 3 codes has the number of logical qubits ($l_3$) given by $l_3=2r^2-1$, where $r$ is the radius of the code.  Since the number of physical qubits ($n$) is given by $n=24r^2$, the number of logical qubits per physical qubit is thus given by $\frac{l_3}{n}=\frac{1}{12}\paren{1-\frac{1}{24r}}^2$, a quantity which converges to $\frac{1}{12}$ as $r\to\infty$.  The two labelings we saw which give rise to this code are illustrated in Figure \ref{figure:truncated-hextille-code-3-labelings}.

The best of the distance 4 codes has the number of logical qubits ($l_4$) given by $l_4=2r(r-1)$, and the number of logical qubits per physical qubit is thus given by $\frac{l_4}{n}=\frac{1}{12}\paren{1-\frac{1}{r}}$, a quantity which converges to $\frac{1}{12}$ as $r\to\infty$.  We see from this analysis that although the best distance 4 code contains fewer logical qubits than the best distance 3 code, they both converge to the same number of logical qubits per physical qubit in the large radius limit.  One of the nine labelings we saw which give rise to this distance 4 code are illustrated in Figure \ref{figure:truncated-hextille-code-4-labelings}.

\begin{figure*}
\includegraphics[width=3.5in]{images/truncated-quadrille-code-3-labeling-1} % labeling #5859
\includegraphics[width=3.5in]{images/truncated-quadrille-code-3-labeling-2} % labeling #12369
\caption{
\label{figure:truncated-hextille-code-3-labelings}
A figure illustrating two labelings of the truncated hextille tiling (on a radius 1 lattice) that result in a quantum code with distance 3 and a number of distance logical qubits proportional to the square of the radius of the labeling.
}
\end{figure*}

\begin{figure}
\includegraphics[width=3.5in]{images/truncated-quadrille-code-4-labeling-1} % labeling #4792
\caption{
\label{figure:truncated-hextille-code-4-labelings}
A figure illustrating two labelings of the truncated hextille tiling (on a radius 1 lattice) that result in a quantum code with distance 4 and a number of distance logical qubits proportional to the square of the radius of the labeling.
}
\end{figure}
%@+node:gcross.20110318151522.1462: *5* hexadeltille
\subsection{hexadeltille}

There are many codes that appear in the hexadeltille tiling, but it is difficult to draw conclusions about trends due to the limit on the size of the lattices that were scanned.  The good news, though, is that the reason why scanning larger radii was difficult is because there is a code in this tiling with a qubit whose distance grows with the radius of the lattice.  One of the nine labelings that we saw with this property is illustrated in Figure \ref{figure:hexadeltille-code-labeling}. 

\begin{figure}
\includegraphics[width=3in]{images/hexadeltille-code-labeling}
\caption{
\label{figure:hexadeltille-code-labeling}
A figure illustrating one of the labelings of the hexadeltille tiling (on a radius 1 lattice) that results in quantum code whose distance grows with the size of the tiling.
}
\end{figure}
%@+node:gcross.20110318151522.1463: *5* rhombihexadeltille
\subsection{rhombihexadeltille}

This tiling is interesting because it had many more labelings that resulted in codes  than all of the other tilings combined;  specifically, for the rhombihexadeltille tiling we saw 48,807 labelings that resulted in useful codes, whereas for all of the other tilings combined we saw only 421 labelings that resulted in useful codes.  This is even more remarkable considering that the largest lattice we were able to scan for the rhombihexadeltille tiling was smaller than that the for most of the other tilings.

As can be seen in Figure \ref{figure:results-hextille}, this tiling is also interesting because it features so many different kinds of codes, including both codes that seem to grow in the number of logical qubits with radius and codes that grow in distance with size.  It is the only tiling that features a lattice that contains a labeling resulting in a code for every distance up to 6.

The rhombihexadeltille tiling is the only tiling we have seen which has code both with a distance greater than 4 and multiple qubits;  we saw six labelings which resulted in codes with distance 6 and two qubits, and four labelings which resulted in codes with distance 5 and four qubits.  In Figure \ref{figure:rhombihexadeltille-code-56-labelings} we show an example of each of these labelings.

\begin{figure*}
\subfloat[distance 5 code labeling]{
\label{figure:rhombihexadeltille-code-5-labeling}
\includegraphics[width=4.15in]{images/rhombihexadeltille-code-5-labeling} % labeling #819916
}\\
\subfloat[distance 6 code labeling]{
\label{figure:rhombihexadeltille-code-6-labeling}
\includegraphics[width=4.15in]{images/rhombihexadeltille-code-6-labeling} % labeling #2316780
}
\caption{
\label{figure:rhombihexadeltille-code-56-labelings}
A figure illustrating labelings of the rhombihexadeltille tiling (on a radius 2 lattice) that result in a quantum code with distance 5 and four qubits (\ref{figure:rhombihexadeltille-code-5-labeling}) and a quantum code with distance 6 and two qubits (\ref{figure:rhombihexadeltille-code-6-labeling}) when applied to a radius 3 lattice.
}
\end{figure*}

Two of labelings resulted in codes with the highest number of qubits -- 16 logical qubits at distance 4 for a lattice of radius three.  These two labelings are illustrated in Figure \ref{figure:rhombihexadeltille-code-4-labelings}.



\begin{figure*}
\includegraphics[width=4.15in]{images/rhombihexadeltille-code-4-labeling-1} % labeling #699975
\\
\includegraphics[width=4.15in]{images/rhombihexadeltille-code-4-labeling-2} % labeling #1740330
\caption{
\label{figure:rhombihexadeltille-code-4-labelings}
A figure illustrating two labelings of the rhombihexadeltille tiling (on a radius 2 lattice) that result in a quantum code with distance 4 and sixteen when applied to a radius 3 lattice.
}
\end{figure*}
%@-others
%@+node:gcross.20110318151522.1464: *4* Discussion
\section{Discussion} \label{sec:discussion}

There are few surprises in our results.  For example, the best code that we found that maximized the logical qubit distance per physical qubit was the compass model code in the quadrille tiling, which is already well-known.  Furthermore, all of the codes obeyed the upper bounds $kd\in O(n)$ and $d^2\in O(n)$ --- where $k$ is the number of logical qubits in the code, $d$ is the distance of the code, and $n$ is the number of physical qubits implementing the code --- that were derived in \cite{Bravyi:10a} for codes having spatially local generators.

Some of the observed differences between the tilings are an artifact of the search space.  For example, every code found on the hextille tiling could also be implemented on the deltille tiling, but although we found two kinds of codes for the hextille tiling we found no codes for the deltille tilings.  This is because our search space included no way for the deltille tilings to ``knock out'' the middle qubits in each hexagonal tiling, and furthermore the hextille tiling search space included two classes of vertices which could have independent labelings whereas the deltille tiling search space had only one class of vertices.

Although it is not clear how many of the codes we found will have practical applications, the success of this search demonstrates the feasibility of using brute-force computation to find useful codes within a constrained search space.
%@+node:gcross.20110318151522.1466: *3* Conclusion
\section{Conclusion}
\label{sec:conclusion}

In this part we have presented an algorithm for computing the optimal quantum subsystem code that can be implemented using a given set of measurements.  We have shown that although this algorithm requires exponential time in the worst case, this exponential is a function of the code distance, and so the algorithm terminates (relatively) quickly when the optimal code has low distance.  Because of this, the algorithm can be used to perform a brute-force search through a space of possible measurements in order to see which give rise to ``useful'' (high-distance) codes.  We demonstrated the feasibility of this approach by applying the algorithm to search for codes implemented on systems with lattice structures corresponding to nine of the eleven convex vertex-uniform tilings, and on all but one of these nine tilings we found useful codes.

This algorithm should prove helpful in two kinds of ways in particular.  First, it can be applied in an exploratory setting to do the tedious work of computing the code resulting from a set of measurements so that the researcher can experiment with new ideas for choices of measurement to see how well they work.  Second, it can be applied to hone a `rough' idea for how a code might be implemented (such as a particular lattice configuration) into a concrete idea by scanning through the possible choices of the degrees of freedom to see if any result in useful codes; of course, cleverness can often come up with an answer more quickly than a computationally intensive search, but it is good to have the alternative of brute-force computation to fall back on when brute-force cleverness fails.
%@+node:gcross.20110318151522.1469: ** Matrix Product States
\part{Matrix Product States}
\label{part:Matrix Product States}
%@+node:gcross.20110318151522.1486: *3* Motivation
\chapter{Introduction}

Straightforward representations of quantum systems tend to grow exponentially with increasing system size.  Thus, if one wants to have a hope of simulating a large quantum system, one needs to pick a clever method for representing it.  Such a representation needs to have three properties:  it must have a scaling law that makes large systems tractable, it needs to faithfully duplicate the properties of the original system, and it needs to allow one to compute expectation values without returning to the expensive representation.

Matrix product states \cite{Rommer:1997gf} \cite{Perotti:2005bh} \cite{cond-mat/0404706} \cite{cond-mat/0505140} \cite{Schollwock:2005ul} have been popular in the last couple of decades because they exhibit all three properties:  they grow linearly with the size of the system, they tend to produce good approximations for many interesting systems \cite{cond-mat/0505140}, and they allow for $O(N)$ calculations of expectations for tensor product operators (where $N$ is the size of the system).  Most operators are not tensor products, but one can always write them as a sum of tensor product operators.  A general operator would require an exponential amount of terms to do this, but fortunately most operators of interest only require $O(N)$ terms.  Thus, the cost of computing an expectation for a matrix product is $O(N^2)$.

This result can be improved.  One can also get $O(N)$ behavior for calculating the expectations of matrix product operators.  Thus, if one can factor an operator into a matrix product, then one can reduce the $O(N^2)$ calculation into a $O(N)$ calculation.

In practice, one does even better than this.  A typical use of a matrix product state is as an ansatz for the variational method\footnote{This idea was originally proposed by Ostlund and Rommer\cite{Ostlund:1995uq}.  It was inspired, however, by the DMRG algorithm (originally proposed by White \cite{White:1992ys}) which has proven to be a very effective means of finding quantum ground states.}.  This technique involves sweeping through the matrices and locally optimizing each site.  Naively, this would require $O(N^2)$ computation time at each site ($N$ terms in the operator, $O(N)$ for each term), but if one performs the ``sweep'' by moving from adjacent site to adjacent site, then one can cache the old results of computations in such a way as to achieve $O(1)$ computation time per site.

In the past, this $O(1)$ behavior has typically been achieved by writing a special caching algorithm for each Hamiltonian \cite{cond-mat/0404706}.  However, by writing a caching code that works with matrix product operators, one can achieve this in a general way for all Hamiltonian;  one need only supply as input a factorization of an operator.

Thus we see that it is incredibly useful to be able to write down a matrix factorization for operators of interest.  This has already been done for many operators \cite{cond-mat/0701428}, but to date no systematic method has been presented to do this for any operator.

In this part, we shall present such a method.  We shall begin with some background material that presents a pedagogical introduction to matrix product states.  Then we will introduce the key to our method:  a new type of diagram which allows one to visualize matrix product states in a way that makes transparent the type of state that they generate.  Although the application of these diagrams to matrix product states is novel, the diagrams themselves are not:  rather, they will be shown to be merely variants of complex weighted finite state automata.  Once this connection has been made, it will be shown how one can obtain a matrix product factorization of a state or operator by starting with an automaton which generates the ``pattern'' of the operator, and then translating this automaton into a set of matrix factors.  It will then be shown how this process generalizes to multiple dimensions.
%@+node:gcross.20110318151522.1484: *3* Theory
\chapter{Theory}
\label{sec:MPSTheory}
%@+node:gcross.20110318151522.1471: *4* One-Dimensional Chains
\section{One-Dimensional Chains}
%@+node:gcross.20110318151522.1472: *5* Background
\subsection{Background}

Consider a quantum system with $N$ independent observables, such as the $Z$ spin components of a linear one-dimensional chain of spin-$\half$ particles.  In general, the representation of this system must be expressed as a tensor with $N$ indices, $A_{i_1,i_2,\dots,i_N}$.  Each element of this tensor represents the amplitude of the a particular system configuration; for example $A_{\downarrow\uparrow\uparrow\downarrow}$ gives the amplitude of a particular system of four particles being in the $\downarrow\uparrow\uparrow\downarrow$ state.  Part of the difficulty in simulating quantum mechanical systems lies from the fact that when one adds another particle to a system, one must add another index to the representing tensor.  Thus, the information needed to represent a quantum state in general grows exponentially with the number of particles.

Fortunately, it turns out that not all quantum states require the full content of an $N$-index tensor.  Some states are special in that they are \textit{separable}, which means that their $N$-index tensor can be factored into the outer-product of $N$ 1-index tensors,

\eqn[separable]{\Psi_{\alpha\beta\gamma\dots} = A_\alpha B_\beta C_\gamma \dots}

This representation is very nice because it only grow linearly with the number of observables;  since it is so nice, in fact, it is not surprising that it comes with a price:  it cannot be used to model systems with any entanglement.

It would be nice to be able to add some entanglement into the above representation in such a way that we do not cause it to revert back to the full $N$-index tensor.  For example, suppose that the observables corresponding to indices ``$\alpha$'' and ``$\beta$'' in \eqref{separable} were maximally entangled -- i.e., their state is given by

$$\ket{\uparrow\uparrow}+\ket{\downarrow\downarrow} \equiv
\bmat{1}{0}{0}{1},
$$

\noindent where the matrix elements correspond to configuration amplitudes as shown in the following table:

\begin{center}
\begin{tabular}{c||c|c}
               & $\alpha=\downarrow$ & $\alpha=\uparrow$ \\
\hline
\hline
$\beta=\downarrow$ &       1        &       0      \\
\hline
$\beta=\uparrow$   &       0        &       1
\end{tabular}
\end{center}

There is no way to obtain the above matrix by taking the outer product of two vectors -- $A_\alpha$ and $B_\beta$ in \eqref{separable} -- but one could obtain it by taking the inner product of two matrices, such as

$$A_{\alpha\,i} = \bmat{0}{1}{1}{0} = B_{i\,\beta}, \quad \sum_i A_{\alpha i} B_{i \beta} = \bmat{1}{0}{0}{1} $$

Thus, we see that we could represent our state in the form,

$$\Psi_{\alpha\beta\gamma\dots} = \sum_i A_{\alpha i} B_{i\beta} C_\gamma \dots$$

This had the desired result -- we were able to add a small amount of entanglement to our separable state without greatly enlarging it.  The inner index $i$ can be thought of as a ``bond'' between two of the particles that allows them to communicate to each other.

If one wished, one could put a bond between all the particles in the (linear one-dimensional) chain,

\eqn[mps-oldnotation]{\Psi_{\alpha\beta\gamma\delta\dots} = \sum_{i,j,k,l} A_{\alpha i} B_{i \beta j} C_{j \gamma k} D_{k \delta} \dots,}

\noindent at which point one would obtain what is called a ``matrix product state''.

This method is not limited to representations of states; it is also possible to likewise factor operators into so-called ``matrix product operators'',

$$\Psi_{(\alpha\beta\gamma\delta\dots)(\alpha'\beta'\gamma'\delta'\dots)} = \sum_{i,j,k,l} A_{\alpha\alpha' i} B_{i\beta\beta'j} C_{j\gamma\gamma'k} D_{k \delta\delta'} \dots.$$

Matrix product states have gained much interest in the last decade because they turn out to have entanglement properties that are sufficient to represent many systems of interest.  Furthermore, they are very flexible:  one can add additional inner indices whenever one wants to introduce entanglement between two particles, forming tensor networks that can represent systems in any number of dimensions and with any lattice structure.

\begin{comment}
In practice, these states are typically used as an ansatz for the variation method -- that is, one assumes a certain tensor-network form, then uses the variational method to find the elements of each tensor.  It is often useful, however, for one to be able to construct a matrix product state (or operator) by hand.  In this paper, we will first introduce a method of visualizing matrix product states as walks through a graph;  it will be shown that this interpretation is equivalent to viewing a matrix product state as a special case of a ``weighted finite automata''.  It will then be shown how to construct a matrix product state from a weighted finite automata, which allows intuitive ``by-hand'' construction of states and operators.  Extensions to multiple dimensions will then be discussed.
\end{comment}

In this paper, it will prove useful to distinguish between two types of indices:  the indices being summed over, which correspond to entanglement introduced between observables, and those not, which correspond to the observables themselves.  Thus, the former will be denoted by subscripts and the latter will be denoted by superscripts; for example, \eqref{mps-oldnotation} should appear in the form,

$$\Psi^{\alpha\beta\gamma\delta\dots} = \sum_{i,j,k,l} A_i^\alpha B_{ij}^\beta C_{jk}^\gamma D_{k}^\delta \dots,$$

Note that we are not normalizing our states;  this will not be a concern for the purpose of our discussion.
%@+node:gcross.20110318151522.1473: *5* Matrix Product Diagram
\subsection{Matrix Product Diagram}

Consider the 4-particle ``W'' state,

$$\ket{\Psi} = \ket{\downarrow\uparrow\uparrow\uparrow} + \ket{\uparrow\downarrow\uparrow\uparrow} + \ket{\uparrow\uparrow\downarrow\uparrow} + \ket{\uparrow\uparrow\uparrow\downarrow},$$

\noindent which is a sum over all possible states in which one and only one particle has spin-down.  A matrix product representation of this state is,

$$\Psi^{\alpha\beta\gamma\delta\dots} = \sum_{i,j,k} A_i^\alpha B_{ij}^\beta C_{jk}^\gamma D_{k}^\delta,$$

\noindent where $\alpha$ is the index of the spin-component of the first particle, $\beta$ is the index of the second particle, etc., and the tensors on the right-hand side are given by,

$$A^\uparrow = \begin{bmatrix} 1 & 0 \end{bmatrix}, \quad A^\downarrow = \begin{bmatrix} 0 & 1 \end{bmatrix},$$

$$B^\uparrow=C^\uparrow = \bmat{1}{0}{0}{1}, \quad B^\downarrow=C^\downarrow = \bmat{0}{1}{0}{0}.$$

$$D^\uparrow = \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \quad D^\downarrow = \begin{bmatrix} 1 \\ 0 \end{bmatrix},$$

Alternatively, one may use the following notation:  instead of writing a separate matrix for each value of the superscript indices, instead label each matrix element by a value of the observable.  Furthermore, adopt the convention that when taking the inner-product between matrices, one should multiply the matrix elements together by using the outer-product.  This allows us to express our state in the more compact (and hopefully transparent) form,

\eqn[W-matrix-factorization]{\Psi = \underbrace{\begin{bmatrix}\uparrow & \downarrow \end{bmatrix}}_{A}\cdot
         \underbrace{\bmat{\uparrow}{\downarrow}{0}{\uparrow}}_{B}\cdot
         \underbrace{\bmat{\uparrow}{\downarrow}{0}{\uparrow}}_{C}\cdot
         \underbrace{\begin{bmatrix}\downarrow \\ \uparrow \end{bmatrix}}_{D}}

To illustrate that this factorization of our state works, we step through the multiplication of the matrices, starting with the two on the right:

$$
\begin{aligned}
  \Psi &= \paren{\begin{bmatrix}\uparrow & \downarrow \end{bmatrix}\cdot
          \paren{\bmat{\uparrow}{\downarrow}{0}{\uparrow}\cdot
          \paren{\bmat{\uparrow}{\downarrow}{0}{\uparrow}\cdot
          \begin{bmatrix}\uparrow \\ \downarrow \end{bmatrix}}}}\\
       &= \paren{\begin{bmatrix}\uparrow & \downarrow \end{bmatrix}\cdot
          \paren{\bmat{\uparrow}{\downarrow}{0}{\uparrow}\cdot
          \begin{bmatrix}\uparrow\downarrow+\downarrow\uparrow \\ \uparrow\uparrow \end{bmatrix}}}\\
       &= \paren{\begin{bmatrix}\uparrow & \downarrow \end{bmatrix}\cdot
          \begin{bmatrix}\uparrow\uparrow\downarrow+\uparrow\downarrow\uparrow+ \downarrow\uparrow\uparrow \\ \uparrow\uparrow\uparrow \end{bmatrix}}\\
       &= \uparrow\uparrow\uparrow\downarrow + \uparrow\uparrow\downarrow\uparrow + \uparrow\downarrow\uparrow\uparrow + \downarrow\uparrow\uparrow\uparrow
\end{aligned}
$$

This factorization may be equivalently expressed in the form of the diagram in Figure \ref{fig:Wdiagram}, which was obtained directly from the matrices in \eqref{W-matrix-factorization}.  The nodes correspond to indices, and the edges correspond to matrix elements.  The matrices were treated as a table of weights for edges connecting each set of nodes.  That is, for each $2\times 2$ matrix $M_{ij}$, the elements were mapped to edges as shown in Figure \ref{fig:edgemap}.  Where a matrix edge was zero, the edge was omitted.  The nodes shared between edges indicate common indices being summed over.

\begin{figure}%[htp]
\centering
\subfloat[
    Diagram representing the matrix product form of the ``W''-state.  Each possible ``walk'' from left to right generates a term in the state, as illustrated in \ref{fig:examplewalk}
    \label{fig:Wdiagram}
]{\framebox{\includegraphics[width=\columnwidth]{images/mypaper-diagram-2}}}\\
\subfloat[An example walk through the matrix product state illustrated in \ref{fig:Wdiagram}, which generates the term $\uparrow\uparrow\downarrow\uparrow$.
\label{fig:examplewalk}
]{\framebox{\includegraphics[width=\columnwidth]{images/mypaper-diagram-4}}}\\
\subfloat[
    Mapping of edges to matrix elements.
    \label{fig:edgemap}
]{\framebox{\includegraphics[width=3in]{images/mypaper-diagram-3}}}
\caption{Matrix product diagrams}
\end{figure}

The arrows place an ordering on the indices.  They are not strictly necessary to define the diagrams, but they are useful because they allow one to view the diagram in terms of paths.  Specifically, each choice of indices corresponds to a ``walk'' from the left side of the diagram to the right.  For example, the choice $i=0, j=0, k=1$ corresponds to the walk as shown in \ref{fig:examplewalk}.

Each possible walk from the left to the right generates a term in our sum, so that the walk shown in \ref{fig:examplewalk} generates the term $\uparrow\uparrow\downarrow\uparrow$.  As discussed earlier, edges which do not appear in the diagram correspond to vanishing matrix elements;  this may be thought of as disallowing a walk between certain nodes, as any term which tries to include nonexistant edges is multiplied by zero and thus does not contribute to the sum.  For example, in Figure \ref{fig:Wdiagram}, note that there is no path that returns to the top from the bottom (such as $i=0$, $j=1$, $k=0$).
%@+node:gcross.20110318151522.1474: *6* Extension to Operators
\subsubsection{Extension to Operators}

We are not restricted to labeling edges of matrix product diagrams with states;  the tensors at each site may be objects with any number of superscript indices.  This allows us to factor operators as well as states, resulting in a matrix product operator\footnote{Matrix product operators were originally introduced by Verstraete, Garcia-Ripoll and Cirac \cite{cond-mat/0406426}, but they were used as density operators -- i.e., as representations of states, rather than of Hamiltonians or other physical operators.  McCulloch, however, later showed how many classes of physical operators can be factored, and discussed why it can be useful to write them in this form\cite{cond-mat/0701428}.}, which recall has the form,

$$\Psi^{(\alpha\beta\gamma\delta\dots)(\alpha'\beta'\gamma'\delta'\dots)} = \sum_{i,j,k,l} A_i^{\alpha\alpha'} B_{ij}^{\beta\beta'} C_{jk}^{\gamma\gamma'} D_{k}^{\delta\delta'} \dots.$$

\begin{figure}
\framebox{\includegraphics[width=\columnwidth]{images/mypaper-diagram-10}}
\caption{Matrix product diagram for a magnetic field operator. \label{fig:magfield}}
\end{figure}

For example, if we were to take the matrix product representation for the W state, as given in \eqref{W-matrix-factorization}, and replace $\uparrow$ with the identity matrix and $\downarrow$ with the Pauli $Z$ spin matrix (which I will denote by \textbf{Z}) then we we would obtain the diagram in Figure \ref{fig:magfield}.

This operator represents the action of a magnetic field in the $z$ direction coupling to each of the particles.
%@+node:gcross.20110318151522.1475: *5* Weighted Finite Automata States
\subsection{Weighted Finite Automata States}

Up to this point we have considered our state to be an $N-$dimensional tensor, where $N$ is the number of observables.  Let us now use a different but equivalent description that is applicable when all of our observables are of the same kind (e.g., z-components of spin).  First, we define a set $\Sigma$ to be our ``alphabet'';  it contains all the possible values for our observable.  For example, for a spin-$\half$ chain, we shall choose $\Sigma:=\{0,1\}$ where $0$ labels the spin-up state and $1$ labels the spin-down state.  Then we may describe our state as a function which maps strings of length $N$ of the alphabet $\Sigma$ to complex numbers:  $f: \Sigma^N\to \mathbb{C}$.

We can generalize this function.  Suppose that the size of our system is itself a variable -- that is, we want to consider systems with 1 particle, 2 particles, etc., and to have the descriptions for all of these systems captured in a single function.  Then we can make our function a map not from $\Sigma^N$, but from $\Sigma^*$, the set of all finite-lengthed strings of $\Sigma$ symbols.

When phrased in this form, it can be shown that saying that our state has a matrix product representation is equivalent to saying that the function $f$ can be computed by a special kind of weighted finite automaton.  A complex-weighted finite automaton\footnote{Complex-weighted automata are a generalization of \textit{real}-weighted finite automata, which were originally introduced by Culik and Kari as a technique for compressing greyscale images \cite{II:1993fk} \cite{II:fk} \cite{Jiang:2003wd}.  It is worth noting that Latorre devised a very similar algorithm for image compression motivated by matrix product states, though without making the connection to finite state automata \cite{Latorre2007}.  This is interesting because it shows how the separate fields of Quantum Physics and Computer Science have independently converged to the same idea.} is defined by a 5-tuple, $(Q,\Sigma,W,\alpha,\Omega)$, where

\begin{enumerate}
\item $Q$ is a finite set of \textit{states}
\item $\Sigma$ is a finite \textit{alphabet}; in our case, we shall let $\Sigma=\{0,1\}$ for the two possible values of the $z$ component of spin
\item $W:Q\times\Sigma\times Q\to \mathbb{C}$ is the \textit{weight function};  we may equivalently represent this function as a set of complex $Q\times Q$ matrices, $W_a$ for each symbol $a$ in our alphabet $\Sigma$
\item $\alpha:1\times Q$ is the (complex-valued) \textit{initial distribution}
\item $\Omega:Q\times 1$ is the (complex-valued) \textit{final distribution}
\end{enumerate}

For a string $a_0a_1\dots a_N\in\Sigma^*$, the output of our automaton is defined to be

\eqn[wfa-compute-eqn]{f(a_0a_1\dots a_N) = \alpha \cdot W_{a_0} \cdot W_{a_1} \dots W_{a_N} \cdot \Omega}

A finite state automaton can be thought of as a machine which moves from one one state to another based on an input signal.  To see a simple example of this, we consider a simplification of a weighted finite automata called a \textit{deterministic finite automaton}, which outputs either 0 (``reject'') or 1 (``accept'').  The values of all matrices -- $W_a$, $\alpha$, and $\Omega$ -- are restricted to be either $1$ or $0$, and there may only be one non-zero matrix element of each row of each matrix.  For example, the following machine accepts (or ``recognizes'') all strings which end in either two 0's or two 1's and rejects all others:

\begin{enumerate}
\item $Q:=\{A,B,C,D,E\}$
\item $\Sigma:=\{0,1\}$
\item
$$W_0:=
\begin{bmatrix}
\,\,0\,\,&\,\,1\,\,&\,\,0\,\,&\,\,0\,\,&\,\,0\,\,\\
\,\,0\,\,&\,\,0\,\,&\,\,1\,\,&\,\,0\,\,&\,\,0\,\,\\
\,\,0\,\,&\,\,0\,\,&\,\,1\,\,&\,\,0\,\,&\,\,0\,\,\\
\,\,0\,\,&\,\,1\,\,&\,\,0\,\,&\,\,0\,\,&\,\,0\,\,\\
\,\,0\,\,&\,\,1\,\,&\,\,0\,\,&\,\,0\,\,&\,\,0\,\,\\
\end{bmatrix},\quad W_1:=
\begin{bmatrix}
\,\,0\,\,&\,\,0\,\,&\,\,0\,\,&\,\,1\,\,&\,\,0\,\,\\
\,\,0\,\,&\,\,0\,\,&\,\,0\,\,&\,\,0\,\,&\,\,1\,\,\\
\,\,0\,\,&\,\,0\,\,&\,\,0\,\,&\,\,0\,\,&\,\,1\,\,\\
\,\,0\,\,&\,\,0\,\,&\,\,0\,\,&\,\,1\,\,&\,\,0\,\,\\
\,\,0\,\,&\,\,0\,\,&\,\,0\,\,&\,\,1\,\,&\,\,0\,\,\\
\end{bmatrix}.$$
\item $\alpha = \begin{bmatrix}\,\,1\,\,&\,\,0\,\,&\,\,0\,\,&\,\,0\,\,&\,\,0\,\,\end{bmatrix}$
\item $\Omega = \begin{bmatrix}0\\0\\1\\0\\1\end{bmatrix}$
\end{enumerate}

\begin{figure}
\framebox{\includegraphics[width=\columnwidth]{images/mypaper-diagram-9}}
\caption{Example finite state automaton which recognizes any string that ends in either two 0's or two 11's. \label{fig:automaton1}}
\end{figure}

While this is the canonical form, it is not very transparent.  The diagram in Figure \ref{fig:automaton1} is an equivalent method of defining this automaton.  The unconnected arrow on the far left indicates that the system should start in state A.  C and D are shaded to indicate that they are the states which the machine ``accepts'' -- that is, the machine outputs one (as the value for $f$, not to be confused the with the symbol 1 in the alphabet) if and only if it is currently on such a state at the end of a string;  otherwise, it outputs zero.  The arrows indicate how the machine should transition between states in response to a symbol;  for example, the machine will move from state A to state B if the first symbol is a 0, and to state D if the first symbol is a 1.

This machine works by using states B through E as a sort of ``memory''.  States B and D are used for the machine to remember that the last symbol it saw (respectively a 0 or a 1) was different from the one before it;  states C and E are used for the machine to remember that it has already seen two symbols of a kind.

Note that for each state there is one and only one transition for each symbol, and this transition has weight 1;  this due to our restriction that our machine had to be a deterministic finite automaton.  Removal of this restriction allows us to have zero or multiple transitions for each symbol at each state, and also to give a weight to each transition.  Because of this, each input string can have multiple paths, or even no paths through our diagram;  for each possible path we associate a weight equal to the product of all the weights along the path;  furthermore, there may be more than one initial state, and each initial state and final state may itself have a weight.  The output of our automaton is the sum of all weights of all paths from all inital to all final states.  This procedure is not an extension of our definition of a weighted finite automaton, but rather a restatement of it, as it is implicit in \eqref{wfa-compute-eqn}.

Also, note that in this light a matrix product state can be seen as just a special case of a weighted finite automaton.  Each of the nodes on the diagrams drawn earlier is a state, with the edges between them labeling transitions.  In a matrix product state, however, there is a separate transition matrix for each position in the string; that is, the 3rd symbol always pases through the same region on the graph, and no other symbol passes through this same region, whereas in a finite state automaton all states are potentially accessible to all symbols.  (Equivalently, one could say that a matrix product state is a weighted finite automaton in which all the transition matrices are block diagonal.)

The ability to share states allows one to write down very compact
 representations of states in weighted finite automaton form.  For example, the W state can expressed as an automaton with only two states, as shown in Figure \ref{fig:Wautomaton}.

\begin{figure}
\subfloat[
    Finite state automaton recognizing the W state.
    \label{fig:Wautomaton}
]{\framebox[\columnwidth][c]{\includegraphics[width=\columnwidth]{images/mypaper-diagram-11}}}\\
\subfloat[
    Finite state automaton recognizing state with neighboring 1's.
    \label{fig:Neighbor-automaton}
]{\framebox[\columnwidth][c]{\includegraphics[width=\columnwidth]{images/mypaper-diagram-12}}}
\caption{Examples of finite state automata which recognize quantum states}
\end{figure}

Again, observe that our states act as a form of memory.  When the machine is in state A, it has not yet seen a 1.  When the machine is in state B, it has already seen a 1.  Upon seeing a 1, it either transitions from A to B, or dies if it is already in B (i.e., outputs 0 for the state).  With this manner of thinking, it is easy to see how to extend this machine to output the state

$$110000\dots + 011000\dots + 001100\dots + \dots,$$

\noindent that is, the set of states with two neighboring 1's;  we already have a state, B, which indicates that the machine has seen one 1, so all we have to do is add another state, C, which indicates that it has seen two 1's.  We also have to update the transitions so that the machine dies unless the two states are neighbors.  The result is shown in Figure \ref{fig:Neighbor-automaton}.

\begin{figure*}
\framebox{\includegraphics[width=\textwidth]{images/mypaper-diagram-13}}
\caption{Example of converting a finite state automaton (in this case, for the W-state) into a matrix product state diagram.  Note how some edges have been faded in order to indicate that they have been removed.}
\label{fig:automaton2mps}
\end{figure*}

Just as it is possible to view a matrix product state as a special case of a weighted finite state, it is always possible to construct a matrix product state from a weighted finite automaton.  To do this, one creates a copy of all the states of the automaton for each particle in the system, and remaps the transitions so that one is always moving from one set of states to another.  Finally, one removes the all but the initial transition in the leftmost set of states and all but the final transition in the rightmost set of states.  For example, for the state just described the matrix product representation would be as shown in Figure \ref{fig:automaton2mps}, where the faded state/edges indicate that they have have been removed.

At this point, note that we have obtained a factorization of the nearest neighbor coupling operator,

$$XXII + IXXI + IIXX.$$

To see this, we observe that this has the same pattern as the state

$$1100 + 0110 + 0011,$$

\noindent which is what we just factored above.  Using the same form for the diagram, we see that the matrix factorization is 

$$
\begin{bmatrix}
\textbf{I} & \textbf{X} & 0 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
\textbf{I} & \textbf{X} & 0 \\
0 & 0 & \textbf{X} \\
0 & 0 & \textbf{I} \\
\end{bmatrix}
\cdot
\begin{bmatrix}
\textbf{I} & \textbf{X} & 0 \\
0 & 0 & \textbf{X} \\
0 & 0 & \textbf{I} \\
\end{bmatrix}
\cdot
\begin{bmatrix}
0 \\
\textbf{X} \\
\textbf{I} \\
\end{bmatrix}
$$

Thus, we see that we now have a method for factoring operators:

\begin{enumerate}
\item Write down a weighted finite automata which generates the pattern of the operator.
\item Translate this into a matrix product operator diagram.
\item Write down matrices based on the diagram.
\end{enumerate}

This method is most efficient for operators which are translationally invariant.  If an operator has additional position-dependent structure, then one should incorporate this structure into the matrix product diagrams, rather than into the weighted finite automata.  To see what is meant by this, suppose our coupling operator took the peculiar form,

$$XXII + IX\underline{\textbf{Z}}I + IIXX.$$

\begin{figure}
\subfloat[
    \label{fig:nolongerinvariant-automaton}
]{\framebox{\includegraphics[width=\columnwidth]{images/mypaper-diagram-14}}}\\
\subfloat[
    \label{fig:nolongerinvariant-mps}
]{\framebox{\includegraphics[width=\columnwidth]{images/mypaper-diagram-15}}}
\caption{An example of what happens to an automaton (fig. \ref{fig:nolongerinvariant-automaton}) and a matrix product state (fig. \ref{fig:nolongerinvariant-mps}) when translational invariance is lost.}
\end{figure}

It is still possible to write down a weighted finite automata for this operator, as shown in Figure \ref{fig:nolongerinvariant-automaton}.  However, as you can see, capturing this position-dependent behavior requires the addition of several states, which means that our matrix factors would have to be much larger.  Thus, rather than proceeding in this way, it would be better to note that this operator looks almost like the previous operator except with a $Z$ in a special place, and then proceed by modifying the previous diagram in that single spot to obtain the diagram shown in \ref{fig:nolongerinvariant-mps}, which corresponds to the factorization,

$$
\begin{bmatrix}
\textbf{I} & \textbf{X} & 0 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
\textbf{I} & \textbf{X} & 0 \\
0 & 0 & \textbf{X} \\
0 & 0 & \textbf{I} \\
\end{bmatrix}
\cdot
\begin{bmatrix}
\textbf{I} & \textbf{X} & 0 \\
0 & 0 & \fbox{\textbf{Z}} \\
0 & 0 & \textbf{I} \\
\end{bmatrix}
\cdot
\begin{bmatrix}
0 \\
\textbf{X} \\
\textbf{I} \\
\end{bmatrix}.
$$

Thus we see that when encoding translationally invariant behavior in an operator it is better to work with weighted finite automata, and when encoding position dependent behaviour it is better to work with matrix product diagrams.
%@+node:gcross.20110318151522.1476: *5* Calculation of Expectations
\subsection{Calculation of Expectations}

\label{calcexp}

In the proceeding section we have demonstrated a systematic method for obtaining a factorization of an operator.  In this section, we shall show how a matrix factorization of an operator allows us to compute expectations of matrix product states efficiently.

\begin{figure}
\framebox{\includegraphics{images/mypaper-diagram-16}}
\caption{An example of using box-and-line notation to represent a network of tensors.}
\label{fig:box-line-notation-introduced}
\end{figure}

We shall use ``box-and-line'' notation to represent a network of tensors that is being partly or fully contracted, so for example the tensor network

$$\Psi^{\alpha\beta\gamma\delta} = \sum_{ijkl} A_{i} B_{ijk}^\alpha C_{kl}^\beta D_{jl}^{\gamma\delta},$$

\noindent is represented by the diagram shown in Figure \ref{fig:box-line-notation-introduced}, where the boxes represent tensors, edges connecting boxes represent summed (internal) indices, and edges with arrows indicate external indices.

\begin{figure}
\subfloat[
    Matrix product state by itself
    \label{fig:box-line-mps}
]{\framebox{\includegraphics{images/mypaper-diagram-17}}}\\
\subfloat[
    Expectation of an arbitrary operator
    \label{fig:box-line-expectation-arbitrary}
]{\framebox{\includegraphics{images/mypaper-diagram-18}}}\\
\subfloat[
    Expectation of a matrix product operator.
    \label{fig:box-line-expectation-mpo}
]{\framebox{\includegraphics{images/mypaper-diagram-19}}}
\caption{Box-and-line notation applied to matrix product states.}
\end{figure}

With this notation, we see that the matrix product state given by,

$$S^{\alpha\beta\gamma\delta} = \sum_{ijk} \paren{S_1}_{i}^\alpha\paren{S_2}_{ij}^\beta\paren{S_3}_{jk}^\gamma\paren{S_4}_{k}^\delta,$$

\noindent is represented by the diagram in Figure \ref{fig:box-line-mps},  and expectation value of this state with respect to some operator $\mathscr{O}^{(\alpha'\beta'\gamma'\delta'),(\alpha\beta\gamma\delta)}$ is given by the diagram in Figure \ref{fig:box-line-expectation-arbitrary}.  If this were the best we could do, then the matrix product state would not have helped us very much because we would still need to perform an exponential amount of calculations.  Fortunately, we can improve upon this if we can factor $\mathscr{O}$ into matrix product form,

$$\mathscr{O}^{(\alpha'\beta'\gamma'\delta'),(\alpha\beta\gamma\delta)}
  = \sum_{i'',j'',k''}\paren{O_1}^{\alpha'\alpha}_i\paren{O_2}^{\beta'\beta}_{ij}\paren{O_3}^{\gamma'\gamma}_{jk}\paren{O_4}^{\delta'\delta}_k,
$$

\noindent so that our tensor network now becomes that shown in Figure \ref{fig:box-line-expectation-mpo}.

\begin{figure}
\framebox{\includegraphics{images/mypaper-diagram-21}}
\caption{Building transfer matrices for the expectation of a matrix product operator.}
\label{fig:formation-of-transfer-matrices}
\end{figure}

This sum is now performed in two stages;  first, we sum the site and operator matrix at each index to form ``transfer matrices'',

$$\paren{E_1}_I \equiv \paren{E_1}_{(i,i',i'')}
                 = \sum_{\alpha,\alpha'} \paren{S^*_1}^{\alpha'}_{i'}\paren{O_1}^{\alpha'\alpha}_{i''}\paren{S_1}^{\alpha}_i, \dots$$

\noindent thus forming the new tensor network shown in Figure \ref{fig:formation-of-transfer-matrices}, and then we contract the new network.  This procedure takes $O(3N)$ matrix multiplications, and the largest matrices that we ever need to form are the $E_n$ matrices.  Thus we see that computing expectation values for a matrix product operator is an $O(N)$ procedure\footnote{Upon completion of this work, we learned of similar results by \underline{$\quad$} in \underline{$\quad$}}.
%@+node:gcross.20110318151522.1477: *5* Energy Minimization and Caching
\subsection{Energy Minimization and Caching}

\label{caching}

Up to now, we have discussed how to perform operations on matrix product states that are known.  In general, however, one will want to investigate systems for which the eigenstates are unknown.  In this case, matrix product states provide an ansatz for the variational method.  That is, one assumes that a ground state has a particular matrix product form, and then searches for the matrix elements which give the lowest energy state representable in that form;  put another way, one seeks the matrix product state $\mathscr{S}$ which minimizes the normalized expectation value of the Hamiltonian,

$$f(\mathscr{S}) = \frac{\coip{\mathscr{S}}{H}{\mathscr{S}}}{\cip{\mathscr{S}}{\mathscr{S}}}.$$

\noindent The hope is that the result of this process will be a reasonable approximation to the true ground state.

In general finding the global minimizer of $f$ is an NP-complete problem \cite{quant-ph/0609051}.  Fortunately, a local search heuristic suffices for many systems of interest:  at each step in the minimization process, all but one of the site matrices are held constant, and then the energy is minimized with respect to the single remaining matrix.

\begin{figure}
\framebox{\includegraphics[width=\columnwidth]{images/mypaper-diagram-28}}
\caption{Formation of a matrix which allows us to express the expectation of $\mathscr{O}$ in quadratic form with respect to site 3.}
\label{fig:Omatrix}
\end{figure}

Suppose we are varying over the third matrix.  Recall that the expectation of an operator can be represented by a diagram of the form shown in Figure \ref{fig:formation-of-transfer-matrices};  since we are holding all but the third matrix constant, we can form the matrix $\mathscr{O}^{3}$ which is the contraction of all the tensors in the network save $S^3$ and its conjugate, as shown in Figure \ref{fig:Omatrix}.  We see now that the energy as a function of $S^3$ is just the quadratic ratio form,

$$f(S_3) = \frac{S^*_3\cdot \mathscr{H}^3 \cdot S_3}{S^*_3 \cdot \mathscr{N}^3 \cdot S_3},$$

\noindent where $\mathscr{H}^3$ and $\mathscr{N}^3$ are the aforementioned tensor contraction for $\mathscr{O}=\mathscr{H}$ and $\mathscr{O}=\textbf{I}$ respectively.  It can be shown that minimizing the above form (a Rayleigh quotient) is equivalent to solving the generalized eigenvalue problem

$$\mathscr{H}^3 \cdot S_3 = \lambda \mathscr{N}^3\cdot S_3,$$

\noindent and then picking the eigenvector $S^3$ with the smallest value for $\lambda$.  The cost of solving this eigenvalue problem depends only on the size of the site matrix $S^3$, not on the number of sites, $N$;  however, it also relies on $\mathscr{H}^3$, which in general is very expensive to calculate.

\begin{figure}
\framebox{\includegraphics[width=3.75in]{images/mypaper-diagram-27}}
\caption{Tensor contractions used to compute $L_3$ and $R_3$}
\label{fig:formation-of-LR}
\end{figure}

Fortunately, if we can factor $\mathscr{H}$ into a matrix product operator, then computing $\mathscr{H}^3$ is cheap.  As in the previous section, we observe that we may form $E_i$ matrices by contracting $S_i^*$, $O_i$, and $S_i$ together at each site;  furthermore, we may also contract all of the $E_i$ matrices to the left of site 3 to form $L_3$ and all of the $E_i$ matrices to the right of site 3 to form $R_3$.  The result of this is the form shown in Figure \ref{fig:formation-of-LR}.

Computing $L_i$ and $R_i$ at some site $i$ would naively be an $O(N)$ operation; however, by using caching we can instead make it an amortized\footnote{By ``amortized'' here we mean that although it takes $O(N)$ time to initialize $L_1$ and $R_1$ on the first step, it takes $O(1)$ for all remaining steps, and there are typically at least $N$ steps, so on average the operation takes $O(1)$ time per step.} $O(1)$ operation.  To see why, note that $L_i$ and $R_i$ may be computed recursively:

$$
\begin{aligned}
&L_1 = I,\quad L_{i} = L_{i-1}\cdot E_{i-1},\\
&R_N = I,\quad R_i = R_{i+1}\cdot E_{i+1} \\
\end{aligned}
$$

So once we have $R_1$, we already have $R_2$ through $R_N$.  Thus, if we start by minimizing the energy with respect to site 1, and then sweep to the right (i.e., site 2, site 3, up to site N), then although it took us $O(N)$ time to compute $R_1$, we get the $R_i$ matrices for all of the rest of the sites up through $N$ for free.  Once we hit site N, we start moving left back through $N-1,N-2$, etc., and at each step it only takes us one additional matrix multiplication to compute $R_i$ from $R_{i+1}$.  Ergo, the time needed at each step to compute $R_i$ is amortized $O(1)$;  by a similar argument, we see the same for the $L_i$ matrices.  This process is illustrated in Figure \ref{fig:recursive-LR}.

\begin{figure}
\framebox{\includegraphics[width=\columnwidth]{images/mypaper-diagram-25}}
\caption{Use of recursion and caching to calculate $L$ and $R$ in amortized $O(1)$ time at each site. \label{fig:recursive-LR} }
\end{figure}

The notion of using caching to speed up these calculations is not a new one;  the same process has already been described by Verstraete, Porras, and Cirac \cite{cond-mat/0404706}.  However, whereas their process is limited to one- and two-body operators, this new procedure works for any form of operator which can be written in matrix product form;  furthermore, the process is the same for all such operators, rather than requiring a new process for each class of operator -- e.g., one-body, two-body, etc.

To summarize: if we can factor our Hamiltonian into a matrix product operator, then we can calculate our matrices $\mathscr{H}^i$ and $\mathscr{N}^i$ needed to optimize a site matrix from matrices which can be calculated using a recursion rule.  By caching the intermediate steps of the recursion, and moving through the sites to be optimized in order from left to right and back, we can calculate $\mathscr{H}^i$ and $\mathscr{N}^i$ in amortized $O(1)$ time.
%@+node:gcross.20110318151522.1478: *4* Arbitrary-Dimensional Graph
\section{Arbitrary-Dimensional Graph}
%@+node:gcross.20110318151522.1479: *5* Motivation
\subsection{Motivation}

\label{arbitrary-motivation}

\begin{figure}
\subfloat[
    Total ordering imposed on two-dimensional grid
    \label{fig:2d-total-ordering}
]{\framebox[2.5in]{\includegraphics[width=2.25in]{images/mypaper-diagram-31}}}
\subfloat[
    4-$X$ operator on grid.  The $X$'s represent the locations of the $X$ operators on the grid; at all other sites there are $I$ operators.
    \label{fig:2d-4X-operator}
]{\framebox[2.5in]{\includegraphics[width=2in]{images/mypaper-diagram-30}}}
\vspace{.5in}
\subfloat[
    Finite state automaton needed for operator in \ref{fig:2d-4X-operator} given the total ordering shown in \ref{fig:2d-total-ordering}
    \label{fig:2d-4X-FSA}
]{\framebox{\includegraphics[width=4in]{images/mypaper-diagram-32}}}
\caption{A total ordering is imposed on the 2D grid in \ref{fig:2d-total-ordering};  this allows us to write down a finite state automaton representation of the 4-$X$ operator shown in \ref{fig:2d-4X-operator}, but the resulting automaton in \ref{fig:2d-4X-FSA} is less than ideal.}
\end{figure}

Matrix product states were designed for studying one-dimensional systems, and that is where they excel.  Nothing technially stops one from using them to study higher-dimensional systems, however -- as long as one is willing to effectively reduce these systems into a one-dimensional system by imposing an ordering; for example, on a $6\times 6$ two-dimensional grid one could impose the ordering shown in Figure \ref{fig:2d-total-ordering}.

However, there is a price one pays for doing this.  Suppose that one wants to represent a Hamiltonian on a $6\times 6$ grid which consists of 4-site $X$ terms arranged in a square as shown in \ref{fig:2d-4X-operator}. The automata which encodes such a Hamiltonian takes the form shown in \ref{fig:2d-4X-FSA}.  The states in the middle (D-G) act as a memory which tell the automata how many sites it has walked past since the second $X$.  This needed so that the automata can put the last two $X$'s in the correct place on the following row.  The number of states required here grows with the number of columns in the grid.

We see that although we can write down such an automata, and so form a matrix product representation of this operator, it is less than ideal because the representation depends on the size of the grid.  This comes from the fact that information can only flow in one direction;  ideally, we would like the information that an $X$ has been placed on one row to somehow go directly down one row rather than having to sweep through the rest of the current row first.  We could, of course, adjust the ordering to sweep down columns instead of across rows, but then we lose the ability to cheaply send information across a row;  using matrix product states, there is no way we can make it easy to communicate in two directions simultaneously.
%@+node:gcross.20110318151522.1480: *5* Tensor Network Diagrams
\subsection{Tensor Network Diagrams}

\label{tensordiagram}

The previous section has described the limitations of matrix product states.  These limitations come from the fact that each tensor is only connected by indices to two other tensors.  (Or equivalently, we might say that the problem is that each site is only directly entangled to two other sites.)  We can get a more powerful representational form by using a more complicated index structure; for example, we could use the following structure,

\eqn[crazy-tensor-product]{\Psi^{\alpha\beta\gamma\delta\mu\nu} =
  \sum_{ijkl} A^\alpha B^{\beta}_{i} C^{\gamma}_{ijk} D_{l} E_{jl}^{\delta\mu} F_{kl}^{\nu}.}

In this example, we see that there are many different types of factors that are possible.  The first, $A^\alpha$ is a simple outer-product factor;  this indicates that there is no entanglement between $\alpha$ and any of the other observables.  The second two tensors, $B^\beta_i$ and $C^\gamma_{ij}$, are connected by an inner product -- i.e., a sum over the subscript index $i$ -- and so we see that $\beta$ and $\gamma$ have some entanglement between them.  $\gamma$ is also entangled with $\delta$ and $\mu$ through the index $j$ and $\nu$ through the index $k$;  this illustrates that entanglement may be shared between one observable and any number of others, and that those other observables need not be directly adjacent to a factor in the above.  Note that the factor $D$ does not have a superscript;  it does not give any direct information about an observable, but rather (in a manner of speaking) it coordinates communication between observables.  On the other hand, $E_{jl}^{\delta\mu}$ has two superscripts, so that it gives information about two observables at once.  Finally, note that the index $l$ is shared between three tensors;  putting the same index in multiple places allows several observables to be simultaneously entangled with each other.

\begin{figure}
\subfloat[
    A diagram representing the tensor product structure of eqn. \eqref{crazy-tensor-product}.
    \label{crazy-diagram}
]{\framebox{\includegraphics[width=4in]{images/mypaper-diagram-22}}}\\
\subfloat[
    An invalid walk through \ref{crazy-diagram}.
    \label{crazy-diagram-bad-walk}
]{\framebox{\includegraphics[width=4in]{images/mypaper-diagram-23}}}\\
\subfloat[
    An acceptable walk through \ref{crazy-diagram}.
    \label{crazy-diagram-good-walk}
]{\framebox{\includegraphics[width=4in]{images/mypaper-diagram-24}}}
\caption{Tensor product state diagrams}
\end{figure}

One possible diagram with the above tensor structure is that shown in \ref{crazy-diagram}.  Just like in the one-dimensional matrix product states, we may put arrowheads on the edges, and think of our states as being generated by ``walks'' through the diagram.  However, now there are points where our walk may split into many paths (in this case, the indices $j$ and $k$) which are taken simultaneously.  Whenever these two paths rejoin, they must rejoin at the same node or else the walk is rejected.  For example, Figure \ref{crazy-diagram-bad-walk} illustrates an invalid choice of path, whereas \ref{crazy-diagram-good-walk} illustrates a correct choice.

This rule for the rejoining of paths is just a restatement of the fact that only one value may be picked for each index for each term in the sum.

Note that although there is a partial ordering on the steps in our walk, there is not a total ordering.  That is, although our arrowheads tell us that a link from C must be chosen before a link from E or F, they do not tell us whether a link from E should be chosen before F or vice versa.  This contrasts with the one-dimensional case, where there is a total ordering.
%@+node:gcross.20110318151522.1481: *5* Weighted Finite Signaling Agents
\subsection{Weighted Finite Signaling Agents}

Recall that in the previous part, we showed that matrix product diagrams are equivalent to defining a weighted finite automata which encodes the state.  If we wanted, we could similarly relate our generalized tensor network states to weighted finite automata.  There is a catch, though:  automata require the system to be in a concete ``state'' at any moment in time, and they also require there to be a total ordering of the input.  Our tensor network state diagrams have neither of these properties.

To see why these properties are absent, we return to Figure \ref{crazy-diagram}.  For the B transition, the system picks one of the states in $i$ in response to the first input symbol, so both properties hold.  For the C transition, however, the system picks two new states for the system -- one from $j$ and one from $k$ -- in response to the second input symbol.  At this point, not only is the system in multiple states, but the symbol to which it will respond next is not defined, since the order of E and F has not been specified.

%{\small
Of course, we could force these properties to be present by combining indices $j$ and $k$ into a single index $m$ that unifies them.  (i.e., $m=1$ would be equivalent to $j=1, k=1$;  $m=2$ would be equivalent to $j=1,k=2$, etc.)  We would then have to replace our tensors $E$ and $F$ with a single tensor $G$.  It is possible that in this particular situation we would obtain something simpler, but in general this process will result in much larger and more complicated tensors than we started with.
%}

Thus, instead of thinking in terms of states, it proves more useful to think in terms of signals.  Each site corresponds to an agent that receives signals from channels, makes a (nondeterministic, weighted) decision based on these incoming signals and an input symbol, and then sends signals to output channels.  $i$, $j$, $k$, $l$, and the rest of the unlabeled nodes above correspond to our channels, and $B-F$ correspond to our agents.


\begin{figure}
\framebox{\includegraphics[width=3in]{images/mypaper-diagram-33}}
\caption{Flow of information for finite signaling agent on 2D grid}
\label{fig:2d-information-flow}
\end{figure}

To see how this works in practice, we consider how one might design an agent to generate the 4-$X$ Hamiltonian (fig. \ref{fig:2d-4X-operator}) discussed in section \ref{arbitrary-motivation}.  We allow our agents to receive signals from two directions: up and left, and to send signals in two directions:  down and right;  the flow of information is illustrated in Figure \ref{fig:2d-information-flow}.

The grey circles represent sites (or ``agents'') in our system, and the arrows represent links (or ``channels'').  At each site is an agent, which is a rank 5 tensor with an index corresponding to each channel and an index correspoding to the input symbol.  The (slightly grayed) arrows on the outside of the diagram which only connect to one node implement the boundary conditions;  they do this by starting the system with a particular set of ``initial'' signals, sent through the top and left boundary channels, and then accepting only those inputs which cause the signal received from the bottom and right boundary channels to be one of the valid ``final'' signals.

Each signal is an integer that corresponds to an index in a tensor;  it often proves convenient, though, to map these integers to names in order to make clear the working of the agent.  For example, table \ref{tbl:assign} gives the signal names that we use for the agent recognizing our 4-$X$ Hamiltonian.

\begin{table}
\subfloat[For convenience, labels are assigned to index numbers in order to give intuitive names to the signals.
    \label{tbl:assign}
]{
\begin{tabular}{l|l} \toprule
Name    & Index Number \\ \toprule
Exterior & 0 \\
Boundary w/ $X$ & 1 \\
Boundary & 2 \\
Interior w/ $X$ & 3 \\
Interior & 4 \\
\bottomrule
\end{tabular}}

\subfloat[The transition table defining the finite signaling agent.
    \label{tbl:transitions}
]{
\begin{tabular*}{\columnwidth}{cllclll} \toprule
& Input Signals & & Symbol & & Output Signals \\ \toprule
%\multicolumn{5}{c}{\begin{minipage}{5in}(The following matrix elements handle the case in which no $X$s have been place; the agent may (nondeterministically) choose to either start a square here or not.)\end{minipage}} \\
 & \parbox{1in}{\noindent
\begin{trivlist}
\item[$\uparrow$] Exterior
\item[$\leftarrow$] Exterior
\end{trivlist}}
 & $\longmapsto$ & $I$ & $\longmapsto$ &
\parbox{1in}{\noindent
\begin{trivlist}
\item[] Exterior $\rightarrow$
\item[] Exterior $\downarrow$
\end{trivlist}}
\\
\hline
\parbox{1cm}{\includegraphics{images/grey-50}} & \parbox{1in}{
\begin{trivlist}
\item[$\uparrow$] Exterior
\item[$\leftarrow$] Exterior
\end{trivlist}}
 & $\longmapsto$ & $X$ & $\longmapsto$ &
\parbox{1.35in}{\noindent
\begin{trivlist}
\item[] Boundary w/ $X$ $\rightarrow$
\item[] Boundary w/ $X$ $\downarrow$
\end{trivlist}}\\
\hline
%\multicolumn{5}{c}{\begin{minipage}{5in}(These next matrix elements handle the case where one of two $X$s has been placed.)\end{minipage}} \\
\parbox{1cm}{\includegraphics{images/grey-40}} & \parbox{1.25in}{
\begin{trivlist}
\item[$\uparrow$] Boundary w/ $X$
\item[$\leftarrow$] Exterior
\end{trivlist}}
 & $\longmapsto$ & $X$ & $\longmapsto$ &
\parbox{1.25in}{\noindent
\begin{trivlist}
\item[] Interior w/ $X$ $\rightarrow$
\item[] Boundary $\downarrow$
\end{trivlist}}\\
\hline
\parbox{1cm}{\includegraphics{images/grey-40}} &  \parbox{1.3in}{
\begin{trivlist}
\item[$\uparrow$] Exterior
\item[$\leftarrow$] Boundary w/ $X$
\end{trivlist}}
 & $\longmapsto$ & $X$ & $\longmapsto$ &
\parbox{1.25in}{\noindent
\begin{trivlist}
\item[] Boundary $\rightarrow$
\item[] Interior w/ $X$ $\downarrow$
\end{trivlist}}\\
\hline
\parbox{1cm}{\includegraphics{images/grey-30}} & \parbox{1in}{
\begin{trivlist}
\item[$\uparrow$] Exterior
\item[$\leftarrow$] Boundary
\end{trivlist}}
 & $\longmapsto$ & $I$ & $\longmapsto$ &
\parbox{1.25in}{\noindent
\begin{trivlist}
\item[] Boundary $\rightarrow$
\item[] Interior $\downarrow$
\end{trivlist}}\\
\hline
\parbox{1cm}{\includegraphics{images/grey-30}} & \parbox{1in}{
\begin{trivlist}
\item[$\uparrow$] Boundary
\item[$\leftarrow$] Exterior
\end{trivlist}}
 & $\longmapsto$ & $I$ & $\longmapsto$ &
\parbox{1.25in}{\noindent
\begin{trivlist}
\item[] Interior $\rightarrow$
\item[] Boundary $\downarrow$
\end{trivlist}}\\
\hline
\parbox{1cm}{\includegraphics{images/grey-25}} & \parbox{1.25in}{
\begin{trivlist}
\item[$\uparrow$] Interior w/ $X$
\item[$\leftarrow$] Interior w/ $X$
\end{trivlist}}
 & $\longmapsto$ & $X$ & $\longmapsto$ &
\parbox{1.25in}{\noindent
\begin{trivlist}
\item[] Interior $\rightarrow$
\item[] Interior $\downarrow$
\end{trivlist}}\\
\hline
\parbox{1cm}{\includegraphics{images/grey-10}} & \parbox{1in}{
\begin{trivlist}
\item[$\uparrow$] Interior
\item[$\leftarrow$] Interior
\end{trivlist}}
 & $\longmapsto$ & $I$ & $\longmapsto$ &
\parbox{1.25in}{\noindent
\begin{trivlist}
\item[] Interior $\rightarrow$
\item[] Interior $\downarrow$
\end{trivlist}} \\
\bottomrule
\end{tabular*}
}

\caption{These tables define a finite signaling agent which recognizes the 4-$X$ Hamiltonian.}

\end{table}

We define an agent by specfying how it reacts to incoming signals.  In this case, there are two incoming signals:  one from above, and one from the left.  Since the agent is nondeterministic, it can have several possible reactions to the incoming signals, each corresponding to a symbol it recognizes (or generates) and signals that it sends right and down.  For our 4-$X$ Hamiltonian, our agent takes the form defined in table \ref{tbl:transitions}.

\begin{figure}
\subfloat [
    A term accepted by the agent
    \label{fig:active-agent-good}
]{\framebox{\includegraphics[height=3in]{images/mypaper-diagram-34}}}\\
\subfloat [
    A term rejected by the agent due to intersecting boundaries
    \label{fig:active-agent-bad}
] {\framebox{\includegraphics[height=3in]{images/mypaper-diagram-35}}}
\caption{Transitions experienced by an agent for two possible terms}
\end{figure}

To see what is going on, consider Figure \ref{fig:active-agent-good}, which illustrates the agent accepting four $X$'s on the grid.  The background at each point is shaded to indicate which of the above transitions is taking place at that point.  Note that the grid is divided into three general regions:  the exterior (shaded white), the boundary (shaded dark grey), and the interior (shaded light grey).  Inside the boundary and the interior, $X$s are excluded because there is no transition which includes them.  The boundary has the role of forbidding additional squares $X$s from being accepted in the exterior, since we have chosen our transitions so that boundaries can only be continued in one direction (vertical or horizontal).  As Figure \ref{fig:active-agent-bad} illustrates, a second group of $X$s which is in the exterior of the first group results in an intersecting boundary which causes the pattern to be rejected.

Now that we have written down an agent which accepts squares with 4 $X$s, we see that we have immediately obtained a factorization of the Hamiltonian which contains a term for each possible placement of these operators.  This factorization is a tensor network with the tensors located at the grid points;  links between nodes indicate that the corresponding indices of the two tensors should be summed over.  The tensors at each node are of rank 5 -- four of the dimensions correspond to the links, and the fifth corresponds to the operator.  There are only eight non-zero elements of this tensor, corresponding to the eight entries in the table above: $(0,0,0,0,I), (0,0,1,1,X), (1,0,3,2,X),$ etc.

\begin{comment}
Formally, a \emph{weighted nondeterministic finite signalling agent} is defined by a 6-tuple, $(Q,\Sigma,W,A,\alpha,\Omega)$, where

\begin{enumerate}
\item $Q$ is a finite set of \emph{signals}
\item $\Sigma$ is a finite alphabet
\item $W:Q^l\times\Sigma\times Q^m\to \mathbb{C}$ is the \emph{weight function};  we may equivalently represent this function as a set of complex $Q^{l+m}$ matrices, $W_a$ for each symbol $a$ in our alphabet $\Sigma$
\item $A\subset Z^{l+m}$ is a set of \emph{agent locations}
\item $\alpha:Q^p$ is the (complex-valued) \emph{initial distribution}
\item $\Omega:Q^q$ is the (complex-valued) \emph{final distribution}
\end{enumerate}

This is very similar to the formal definition of a weighted nondeterministic finite automaton.  Note the addition of $A$, a set of so-called ``agent locations''.
\end{comment}
%@+node:gcross.20110318151522.1482: *5* Calculation of Expectations Using Recursion
\subsection{Calculation of Expectations using Recursion}

Assume that we have a tensor network state and a factorization of an operator which has the same network structure as the state.  As in section \ref{calcexp}, we see that the expectation of the operator may be reduced to the contraction of a network of ``transfer matrices'';  for example, for the peculiar state shown in section \ref{tensordiagram}, calculating the expectation of our operator is equivalent to contracting a tensor network of the form shown in Figure \ref{crazy-transfer-matrix-network}.

\begin{figure}
\centering
\framebox{\includegraphics[width=\columnwidth]{images/mypaper-diagram-37}}
\caption{\label{crazy-transfer-matrix-network} Tensor network giving the expectation of some operator with respect to the site shown in Figure \ref{crazy-diagram}.  Note that the index $l$ connects three tensors.}
\end{figure}

We may wish to minimize the energy with respect to some site $n$.  As discussed in section \ref{caching}, we can reduce this to an eigenvalue problem for the matrix consisting of the contraction of (essentially) all of the transfer matrices except the one at $n$.  This contraction can be expressed as a set of recursion rules;  for example, for a two-dimensional grid we have the following rules:

$$
\begin{aligned}
O_{i,j} &= L_{i,j} \cdot A_{i,j} \cdot B_{i,j} \cdot R_{i,j}, \\
L_{i,j} &= L_{i-1,j}\cdot C_{i-1,j}, \quad L_{1,j} = I \\
R_{i,j} &= R_{i+1,j}\cdot C_{i+1,j}, \quad R_{N,j} = I \\
C_{i,j} &= A_{i,j}\cdot E_{i,j} \cdot B_{i,j} \\
A_{i,j} &= A_{i,j-1}\cdot E_{i,j-1}, \quad A_{i,1} = I \\
B_{i,j} &= B_{i,j+1}\cdot E_{i,j+1}, \quad B_{i,N} = I \\
\end{aligned}
$$

\noindent (The $\cdot$ operation is implicitly over only the connected indices.)

The computation of $O_{33}$ is illustrated in Figure \ref{fig:2d-recursive-structure}.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{images/mypaper-diagram-38}
\caption{Use of recursion to calculate $O_{33}$. \label{fig:2d-recursive-structure}}
\end{figure}

As was the case in section \ref{caching}, as long as we move from each site to an adjacent site, it only takes us (amortized) $O(1)$ time to calculate $O_{ij}$.  In the Figure \ref{fig:2d-recursive-structure}, for example, we see that to compute $O_{43}$, we need only calculate $C_{33}$ and then $L_{43}$.

Unfortunately, it is intractible to contract arbitrarily large multi-dimensional tensor networks.  (Formally, Schuch et al. \cite{quant-ph/0611050} have shown that this is a \#P-complete problem.)  This is because whenever one contracts together tensors with more than two indices, one obtains a larger tensor.  For example, when taking the dot-product between two four-index tensors one obtains a six-index tensor,

$$\sum_f A_{abcd} B_{defg} = C_{abcefg}.$$

\noindent These extra indices result in ``double-bonds'' between tensors.  (We have already seen multi-bonds when computing the $E$ matrices, as shown in Figure \ref{fig:formation-of-LR}.)

Thus, as we contract each row, the size of our tensors increases by some factor, which means that the cost of contracting a tensor network in general grows exponentially with the size of the network!  Fortunately, there is a lossy compression technique which involves approximating a row resulting from a contraction with a new row with fewer bonds;  this has been used successfully to model hard-core bosons in a 2D optical lattice\cite{cond-mat/0611522}.
%@+node:gcross.20110326130640.1393: *4* Intermezzo: Infinite Systems
\section{Intermezzo: Infinite Systems}

In this section, we present an algorithm which computes a translationally invariant MPS representation of the ground state of an infinite system.  The intuition behind our approach is as follows.  Suppose one were given an infinitely large, translationally invariant 1D quantum chain held at zero temperature.  If one were to add an additional site to this chain and allow the chain to relax, then one would expect that all of the old sites would remain unchanged, and the new site would change to match the rest.  If one could emulate the environment experienced by a single site in this infinite chain, then by embedding a site into this environment and allowing the system to relax, one would obtain a site which ``looks like'' all of the sites in our infinite chain, giving us a compact representation for the chain.

We note that this algorithm bears some similarity to the product wave
function renormalization group (PWFRG) \cite{Nishino:1995kx} in that
both algorithms have the same goal---to compute representations of
ground states for infinite systems---and both use the same underlying
matrix product structure to represent states \cite{pwfrg-mps-2}.  The
difference is that while the PWFRG approach seeks the infinite limit
by starting with a small system and progressively enlarging it, we
start from the very beginning with an \emph{infinite} system,
represented in terms of effective environments which we progressively
refine.  Furthermore, we incorporate matrix product operators into our
approach which allow us to model systems with long-range interactions.

The remainder of this section shall be organized as follows.  First, we shall review the matrix product formalism, and show how it allows us to construct a representation of a site embedded in and entangled with an environment.  Second, we shall outline the operation of an algorithm which uses an iteration procedure to approximate the effective environment of a site in an infinite chain;  we shall then explain how one can obtain the expected values of operators from the output of this algorithm.  Third, we shall explain how to obtain matrix product factorizations of Hamiltonians with exponentially decaying interactions.  Finally, we shall pull all of these ideas together and show how they can be applied to abtain an accurate MPS representation of the ground state of the Haldane-Shastry model---an exactly solvable model with long range interactions.
%@+node:gcross.20110326130640.1394: *5* Algorithms
\subsection{Algorithms}
\label{algorithms}
%@+node:gcross.20110326130640.1395: *6* Finding a MPS representation of the ground state
\subsubsection{Finding a MPS representation of the ground state}
\label{main-algorithm}

We start by recalling the form of a matrix product state for a finite system \cite{Schollwock:2005ul}.  For a system with $n$ sites, a matrix product state with boundaries takes the form  \begin{eqnarray}
&&\mathcal{S}^{\alpha_1,\alpha_2,\dots,\alpha_n} = \nonumber \\
&&\quad\sum_{\{i_k\}} \paren{L^{\mathcal{S}}_0}_{i_1}\paren{S_1}^{\alpha_1}_{i_1 i_2} \paren{S_2}^{\alpha_2}_{i_2i_3}\dots \paren{S_n}^{\alpha_n}_{i_n i_{n+1}} \paren{R^{\mathcal{S}}_{n+1}}_{i_{n+1}}. \nonumber
\end{eqnarray}  The left-hand-side corresponds to the rank-n tensor describing our state;  each index $\alpha_k$ corresponds to a basis of a physical observable, such as the $z$-component of spin, at site $k$.  The rank-3 tensors $S_k$ are the site tensors, which have two kinds of indices:  the superscript index, which has dimension $d$ and corresponds to the actual physical observable, and the subscript indices, which have dimension $\chi$ and give information about entanglement between each site and its neighbors.  The vectors $L_0$ and $R_{n+1}$ give the boundary conditions.

The advantage of the matrix product representation is that it decomposes the quantum state into a collection of tensors, each of which is naturally associated with a site on the lattice.  Because of this, we shall see that we can ``zoom in'' on one site and only have to work with its corresponding tensor and a relatively small environment.  In this respect, it is almost as nice as a truly local description which would let us ignore all the other sites entirely--a fact which is remarkable given that it can still capture non-local properties resulting from entanglement.

Given this form, we now consider how to compute the expected values of operators.  As discussed in Ref. \cite{cond-mat/0701428} and Ref. \cite{caching}, operators of interest can typically be expressed as a matrix product operator, \begin{eqnarray}
&&\mathcal{O}^{(\alpha_1,\alpha_1'),(\alpha_2,\alpha_2'),\dots,(\alpha_n,\alpha_n')} = \nonumber \\
&&\quad\sum_{\{i_k\}} \paren{L_0^{\mathcal{O}}}_{i_1}\paren{O_1}^{\alpha_1,\alpha_1'}_{i_1 i_2}\dots \paren{O_n}^{\alpha_n,\alpha_n'}_{i_n i_{n+1}} \paren{R_{n+1}^{\mathcal{O}}}_{i_{n+1}} \label{matrix-product-operator}
\end{eqnarray}  Having decomposed both the state and the operator into a product of tensors associated with lattice sites, we likewise decompose the expected value into a product of tensors by combining the state and operator tensor at each site to define,  \begin{eqnarray}
\paren{E_k^{\exp{\mathcal{O}}}}_{i,j} \equiv \paren{E_k^{\exp{\mathcal{O}}}}_{(i',i'',i'''),(j',j'',j''')}:= \nonumber & \\
\sum_{\alpha_k,\alpha_k'} \paren{S_k^*}^{\alpha_k}_{i',j'}\paren{O_k}^{\alpha_k,\alpha_k'}_{i'',j''} \paren{S_k}^{\alpha_k'}_{i''',j'''}. \nonumber &
\end{eqnarray}  (Note that we have taken all of the left and right subscript indices and grouped them together.)  To complete our decomposition of the expected value, we shall also need to define the left and right boundaries, \begin{eqnarray}
&&\paren{L_0^{\exp{\mathcal{O}}}}_{i\equiv(i',i'',i''')}:= \paren{L_0^{\mathcal{S} *}}_{i'}\paren{L_0^{\mathcal{O}}}_{i''}\paren{L_0^{\mathcal{S}}}_{i'''}, \label{MPO-boundaries} \\
&&\paren{R_{n+1}^{\exp{\mathcal{O}}}}_{j\equiv(j',j'',j''')}:= \paren{R_{n+1}^{\mathcal{S}*}}_{j'}\paren{R_{n+1}^{\mathcal{O}}}_{j''}\paren{R_{n+1}^{\mathcal{S}}}_{j'''}. \nonumber
\end{eqnarray}  These boundary vectors can be thought of as defining an environment into which our system of $n$ sites
has been embedded.  With these tensors so defined, the expected value of the matrix product operator $\mathcal{O}$ with respect to the matrix product state $\mathcal{S}$ is given by $\coip{\mathcal{S}}{\mathcal{O}}{\mathcal{S}} = L_0^{\exp{\mathcal{O}}} \cdot E_1^{\exp{\mathcal{O}}} \cdot E_2^{\exp{\mathcal{O}}} \cdots E_n^{\exp{\mathcal{O}}} \cdot R_{n+1}^{\exp{\mathcal{O}}},$ where $\cdot$ indicates summation over the adjoining subscript indices.

Now we have almost arrived at the picture described in the introduction, except that we have an environment and multiple sites rather than a single site.  To compute the effective environment of a particular site in this chain, we absorb all of the sites surrounding it into the system environment by contracting the $E_k^{\mexp{O}}$ matrices into the left and right boundaries -- that is, we make the inductive definitions $L_k^{\exp{\mathcal{O}}}:= L_{k-1}^{\exp{\mathcal{O}}}\cdot E_k^{\exp{\mathcal{O}}}$ and $R_k:=E_k^{\exp{\mathcal{O}}}\cdot R_{k+1}^{\exp{\mathcal{O}}}$.  By doing this, we can write the expect value of the operator as a function of this site tensor,  \begin{eqnarray}
\label{fn-of-site-tensor}
&&\coip{\mathcal{S}}{\mathcal{O}}{\mathcal{S}}(S_k) \equiv S_k^* \circ M^{\exp{\mathcal{O}}_k} \circ S_k :=  \\
&&\quad= \sum_{\genfrac{}{}{0pt}{}{\alpha,\alpha',i',i''',}{j',j'''}}\paren{S_k^*}^\alpha_{i',j'} \paren{M^{\exp{\mathcal{O}}_k}}^{(\alpha),(\alpha')}_{(i',j'),(i''',j''')}\paren{S_k}^{\alpha'}_{i''',j'''} \nonumber
\end{eqnarray}  where $\circ$ denotes summation over the appropriate adjacent subscript and superscript indices, and  \begin{eqnarray}
\label{m-matrix-definition}
&&\paren{M^{\exp{\mathcal{O}}_k}}^{(\alpha),(\alpha')}_{(i',j'),(i''',j''')} := \nonumber\\
&&\quad\sum_{i'',j''}\paren{L_{k-1}^{\exp{\mathcal{O}}}}_{(i',i'',i''')} \paren{O_k}^{\alpha_k,\alpha_k'}_{i'',j''} \paren{R_{k+1}^{\exp{\mathcal{O}}}}_{(j',j'',j''')} \nonumber
\end{eqnarray}

We have now obtained an explicit means to compute the expected value
of an observable $\mathcal{O}$ as a function of the site tensor at position $k$
knowing only the environment of the site as given by
$L^{\mexp{O}}_{k-1}$ and $R^{\mexp{O}}_{k+1}$.  However, recall that
we want to be more than passive observers---we want to actively move
the system as close as possible to its ground state.  Thus, we now
want to vary the site tensor $S_k$ in order to minimize the energy of
the system.  If we let the Hamiltonian (matrix product) operator be
denoted by $\mathcal{H}$ and the identity operator by $\mathcal{I}$, then employing equation \eqref{fn-of-site-tensor}
we see that what we seek is the site tensor which minimizes the function
$$\mathcal{E}^k(S_k) := \frac{S_k^*
  \circ M^{\exp{\mathcal{H}}_k} \circ S_k}{S_k^* \circ
  M^{\exp{\mathcal{I}}_k} \circ S_k},$$
which gives us the (normalized) energy of the system as a function of only the site tensor at position $k$ (i.e., assuming the environment has been frozen in place).  Since this is a Rayleigh quotient, computing the minimizer is equivalent to solving a generalized eigenvalue problem.

There is a subtlety in this procedure, however, which is that one needs to take steps to make sure that the normalization matrix $M^{\mexp{I}_k}$ is well-conditioned.  This can be done by imposing the ``right-normalization'' condition $\sum_{\alpha,i}\paren{S_{l}^*}^{\alpha}_{ij}\paren{S_{l}}^{\alpha}_{ij'}=\delta_{jj'}$ on all the sites to the left of $k$, and the ``left-normalization'' condition $\sum_{\alpha,j}\paren{S_{l}^*}^{\alpha}_{ij}\paren{S_{l}}^{\alpha}_{i'j}=\delta_{ii'}$ on all the sites to the right of $k$.  This ensures that the subscript indices connected to $S_k$ are orthonormal, which makes $M^{\mexp{I}_k}$ well-conditioned.  We shall discuss how this is done in our algorithm shortly;  for more information on how the normalization condition is used for finite-length systems, see Ref. \cite{cond-mat/0404706}.

Now we have all of the ingredients that we need to build our algorithm:  a way of expressing a chain as a site tensor embedded in an environment, and a way to relax a system in this representation constrained so that only the site tensor (and not its environment) is changed.  However, up to this point we have been working with a finite system; in order to apply these ideas to infinite systems, we shall modify our notation slightly to replace position labels with \emph{iteration labels}.  That is, we shall let $L^{\mexp{O}}_k$ and $R^{\mexp{O}}_k$ denote the infinite environment
at iteration step $k$, and $S_k$ denote the inserted site at this iteration.  With this notation, the algorithm to find the ground state of a system is given in Table \ref{algorithm}. It is dominated by the costs of absorbing the site and operator tensors into the environment (in step 3b.iv, and in the Lanczos iteration in step 3a), which are respectively $O(cd\chi^3)$ and $O(c^2d^2\chi^2)$, with $c$ referring to the auxiliary dimension of the operator tensor.

\begin{table}
%\begin{ruledtabular}
\begin{tabular}{p{0.075in}p{0.15in}p{0.1in}p{2.8in}}
1. & \multicolumn{3}{p{3.3in}}{Set $\chi=1$, and $L^{\mathcal{S}}=R^{\mathcal{S}}=1$.} \\
2. & \multicolumn{3}{p{3.3in}}{Compute $L_1^{\mexp{O}}$ and $R_1^{\mexp{O}}$ for the Hamiltonian
(matrix product)
operator $\mathcal{H}$ and the identity operator $\mathcal{I}$.
(The latter, of course, has trivial boundaries.)
} \\
3. & \multicolumn{3}{p{3.3in}}{Until convergence has been reached:} \\
   & (a) & \multicolumn{2}{p{2.9in}}{Use an eigenvalue solver (such as ARPACK \cite{arpack}) to obtain the minimal eigenvalue and corresponding eigenvector of the generalized eigenvalue problem $M^{\mexp{H}_k}S_k = \lambda M^{\mexp{I}_k} S_k.$  To accelerate convergence, feed in $S_{k-1}$ as a starting estimate for the eigenvector.
} \\
   & (b) & \multicolumn{2}{p{2.9in}}{If this is an odd-numbered step, then right-normalize $S_k$ and contract into the left boundary.  Specifically:} \\
   &     & i.  & Merge the superscript and first subscript index of $S_k$ to form a matrix, and compute the singular value decomposition (SVD), $U\cdot \Sigma\cdot V^\dagger$. \\
   &     & ii. & Set $\tilde S_k:=U\cdot V^\dagger$, and ungroup indices to return $\tilde S_k$ to its original rank-3 shape. \\
   &     & iii.& Compute $E_k^{\mexp{O}}$ for $\mathcal{H}$ and $\mathcal{I}$ using the normalized $\tilde S_k$. \\
   &     & iv. & Contract the site into the left boundary by setting $L_{k+1}^{\mexp{O}}:=L_{k}^{\mexp{O}}\cdot E^{\mexp{O}}_k$ and $R^{\mexp{O}}_{k+1}:=R_k^{\mexp{O}}$.  (This step ``absorbs'' the site into the environment.) \\
   & (c) & \multicolumn{2}{p{2.9in}}{If this is an even-numbered step, perform an analogous process, but \emph{left}-normalize $S_k$ by merging the superscript and \emph{second} index together before the SVD, and then contract the normalized site into the \emph{right}-boundary.} \\
4. & \multicolumn{3}{p{3.3in}}{If a better approximation to the ground state is desired, then increase $\chi$ for the system and repeat step 3.  Increasing the dimension of a subscript can be done without altering the state by multiplying the adjoining tensors by a $\chi\times (\chi+\Delta\chi)$ matrix and its inverse;  this allows one to build on the work of previous iterations (and hence accelerate convergence), rather than having to start from scratch with the new $\chi$.
}
\end{tabular}
%\end{ruledtabular}
\caption{Algorithm to compute a (normalized) translationally invariant matrix product state representation of the ground state of an infinite chain.}
\label{algorithm}
\end{table}

We note here that this algorithm is unstable when applied directly to systems with anti-ferromagnetic interactions because ground states of such systems are not invariant under translations of one site.  Happily, since such systems are invariant under translations of \emph{two} sites, there is a simple fix:  work with blocks of two spins rather than one by setting $d=4$ and multiplying two of the operator tensors together to form a two-site operator tensor\footnote{Note that this approach does not scale well beyond two sites;  that is, given an interaction which is symmetric under translations of $n$ sites, blocking the sites together grows the size of the representation by a factor of $2^n$.  An alternative strategy is to add $n$ sites at a time to the center of the system, and then use a sweeping algorithm like that described in Ref. \cite{cond-mat/0404706} to optimize the site tensors;  the final representation is then given by $n$ site tensors rather than one.};  this only affects the inputs to the algorithm and does not require changing the algorithm itself.
%@+node:gcross.20110326130640.1396: *6* Computing expected values of operators
\subsubsection{Computing expected values of operators}
\label{exp-algorithm}

The output of the algorithm of the previous section is a normalized site tensor $\tilde S$ which gives a translationally invariant representation for the ground state of the system.  In order to make this useful, we need to have a way to obtain the expected value of operators from it.  Of course, for extensive observables the expected value will be infinite since we have an infinitely large system, so instead we seek the more useful quantity of the expected value per site.  We shall consider two cases of operators:  local operators and general matrix product operators.  The basic trick in both cases is to note that $\lim_{N\to\infty}\paren{E^{\mexp{O}}}^N \cdot \vec{v} = \Lambda^N \cdot \vec{v}$, where $\Lambda$ is a matrix in the maximal eigenspace of $E^{\mexp{O}}$ -- that is, in the infinite limit the action of the operator will converge to its action on the maximal eigenspace, since its action on all other eigenspaces will be negligible by comparison.  (Assuming, of course, that  $\Lambda \cdot \vec{v} \ne 0$.)

First we consider local operators, which can be expressed as sum of terms of the form $I^{\otimes\infty} \otimes O_1 \otimes O_2 \otimes \dots \otimes O_N \otimes I^{\otimes\infty}$, such as a magnetic field operator, $I^{\otimes\infty} \otimes Z\otimes I^{\otimes\infty}$, or a two-point correlator, $I^{\otimes\infty}\otimes Z\otimes I^{\otimes r}\otimes Z \otimes I^{\otimes\infty}$.  Since our system is translationally invariant, to compute the expected value per site of this operator we need only evaluate the expected value of one term in the sum.  Assuming that $E^{\mexp{I}}$ has a non-degenerate maximal eigenvalue, we have that for (almost) any vector $\vec{v}$, $\paren{E^{\mexp{I}}}^\infty\cdot \vec{v} \sim \vec{v}^R$ and $\vec{v}\cdot \paren{E^{\mexp{I}}}^\infty \sim \vec{v}^L$, where $\vec{v}^L$ and $\vec{v}^R$ are the respective left and right eigenvectors corresponding to the maximal eigenvalue.  Ergo, the expected value of one term of this operator (and thus the expected value per site) is given by  $$\frac{\vec{v}^L\cdot E^{O_1}\cdot E^{O_2}\dots E^{O_N}\cdot \vec{v}^R}{\vec{v}^L\cdot \paren{E^{\mexp{I}}}^N \cdot \vec{v}^R}.$$

Next we consider general matrix product operators.  We start by writing an expression for the expected value per site for a finite chain of length $N$, whose site tensors are copies of the (normalized) site tensor obtained from the variational algorithm in the previous subsection.  Since the infinite chain has no boundaries, to compute the expected value of the finite chain we need to explicitly supply left and right boundaries, respectively $L^{\mathcal{S}}$ and $R^{\mathcal{S}}$, and so the expected value per site of $\mathcal{O}$ is a function of the boundaries given by $$\mathcal{E}^N(L^{\mathcal{S}},R^{\mathcal{S}}):=\frac{1}{N}\frac{L^{\mexp{O}} \cdot \paren{E^{\mexp{O}}}^N \cdot R^{\mexp{O}}}{L^{\mexp{I}} \cdot \paren{E^{\mexp{I}}}^N \cdot R^{\mexp{I}}},$$  where $L^{\mexp{O}}$, $R^{\mexp{O}}$, $L^{\mexp{I}}$, and $R^{\mexp{I}}$ are implicitly functions of $L^{\mathcal{S}}$ and $R^{\mathcal{S}}$ given by equation \eqref{MPO-boundaries}.  Assuming $\mathcal{O}$ corresponds to an extensive observable, we expect that $\lim_{N\to\infty} \mathcal{E}^N(L_{\mathcal{S}},R_{\mathcal{S}}) = \mexp{O}$ -- that is, in the infinite limit the expected value per site converges to some number $\mexp{O}$ which is independent of the boundaries.  With this physically reasonably assumption, we can reason about the structure of $E^{\mexp{O}}$ and $E^{\mexp{I}}$ to obtain an algorithm for computing $\mexp{O}$.

We first observe that since the maximal eigenvalue of $E^{\mexp{I}}$ is 1 (due to the normalization of the site tensor, $\tilde S$), so must be the maximal eigenvalue of $E^{\mexp{O}}$, since otherwise $\mathcal{E}^N$ would be exponential in $N$.  Furthermore, since $\mathcal{E}^N$ is linear in $N$ for large $N$, the maximal eigenspace of $E^{\mexp{I}}$ must have a Jordan block structure--that is, there must be a matrix $U\equiv[\,\vec{u}_1\,\,\vec{u}_2\,]$  with orthonormal columns $\vec{u}_i$ that provide a basis for this eigenspace such that $A:=U \cdot E^{\mexp{O}} \cdot U^\dagger = \bmat{1}{\alpha}{0}{1}.$ The matrix element $\alpha$ can be thought of as giving us the unnormalized expected value of $\mathcal{O}$ per site.  In order to normalize it, we observe that as $N\to\infty$,
$$
\begin{aligned}
\mathcal{E}^N&\to
\frac{1}{N}\frac{L^{\mexp{O}} \cdot \paren{\vec{u}_1 {\vec{u}_1}^\dagger + \vec{u}_2 {\vec{u}_2}^\dagger + N\alpha \vec{u}_1{\vec{u}_2}^\dagger}\cdot R^{\mexp{O}}}{L^{\mexp{I}} \cdot \paren{E^{\mexp{I}}}^N \cdot R^{\mexp{I}}} \\
& \to
\alpha\frac{L^{\mexp{O}} \cdot (\vec{u}_1 {\vec{u}_2}^\dagger) \cdot R^{\mexp{O}}}{L^{\mexp{I}} \cdot \paren{E^{\mexp{I}}}^N \cdot R^{\mexp{I}}}.
\end{aligned}$$
Since we expect $\mathcal{E}^N$ to be independent of the boundaries for large $N$, it must be the case that $$
\begin{aligned}
L^{\mexp{I}} \cdot \paren{E^{\mexp{I}}}^N \cdot R^{\mexp{I}}
&\to \beta \left[ L^{\mexp{O}} \cdot (\vec{u}_1{\vec{u}_2}^\dagger) \cdot R^{\mexp{O}}\right]\\
&\equiv \beta \left[L^{\mexp{I}} \cdot (\vec{v}^L_1 \vec{v}^{R\dagger}_2) \cdot R^{\mexp{I}}\right], \\
\end{aligned}
$$ where $\vec{v}^L_{k} :=  \vec{u}_k \cdot L^{\mathcal{O}} \equiv \sum_{i''} u_{k,(i',i'',i''')} L^{\mathcal{O}}_{i''}$, and $\vec{v}^R_{k} :=  \vec{u}_k \cdot R^{\mathcal{O}} \equiv \sum_{i''} u_{k,(i',i'',i''')} R^{\mathcal{O}}_{i''}$ -- that is, $\vec{v}^L_k$ and $\vec{v}^R_k$ are the projection of the eigenspace of $E^{\mexp{O}}$ into a space applicable to $E^{\mexp{I}}$ by dotting out the respective left and right boundaries of the matrix product operator $\mathcal{O}$; put another way, if we let $V^L:=[\,\vec{v}^L_1\,\,\vec{v}^L_2\,]$ and $V^R:=[\,\vec{v}^R_1\,\,\vec{v}^R_2\,]$, then the limiting action of $E^{\mexp{I}}$ in the (projected) eigenspace is given by $B:=V^L \cdot E^{\mexp{I}} \cdot {V^R}^\dagger = \bmat{0}{\beta}{0}{0}$.  The matrix element $\beta$ gives us the normalization constant, so that $\mexp{O}:=\lim_{N\to\infty}\mathcal{E}^N=\alpha/\beta$.

It remains to extract $\alpha$ and $\beta$ from the respective matrices $A$ and $B$.  Our eigenvalue solver is not guaranteed to give us a basis of the eigenspace that makes $A$ and $B$ have the nice form above, but happily it is easy to see that $\alpha = \sqrt{\tr(A\cdot A^\dagger)-2}$ and $\beta =\sqrt{\tr(B\cdot B^\dagger)}$, and these formulas are invariant under similarity transforms of $A$ and $B$ so they work independent of the basis we are given.  The algorithm we have derived in the proceeding paragraphs is summarized in Table \ref{MPO-expectation-algorithm}.


\begin{table}
%\begin{ruledtabular}
\begin{tabular}{p{0.075in}p{0.15in}p{0.1in}p{2.8in}}
1. & \multicolumn{3}{p{3.3in}}{Compute the maximal two-dimensional eigenspace of $E^{\mexp{O}}$---for example, by using ARPACK \cite{arpack} and requesting a Schur basis rather than a (non-existant) eigenvector basis---and store the basis vectors in the columns of a matrix $U$.} \\
2. & \multicolumn{3}{p{3.3in}}{Express the action of $E^{\mexp{O}}$ in this space by computing the $2\times 2$ matrix $A:=U\cdot E^{\mexp{O}}\cdot U^\dagger.$} \\
3. & \multicolumn{3}{p{3.3in}}{Extract the unnormalized expected value from this matrix by computing $\alpha:=\sqrt{\tr(A^\dagger\cdot A)-2}.$} \\
4. & \multicolumn{3}{p{3.3in}}{Project out the operator-dependent boundary conditions from the vectors in this eigenspace to obtain $V^L:=L^{\mathcal{O}}\cdot U\equiv \sum L^{\mathcal{O}}_{i''} U_{(i',i'',i'''),k}$ and $V^R:=R^{\mathcal{O}}\cdot U\equiv \sum R^{\mathcal{O}}_{i''} U_{(i',i'',i'''),k}$.} \\
5. & \multicolumn{3}{p{3.3in}}{Express the action of $E^{\mexp{I}}$ in this projected space by computing the $2\times 2$ matrix $B:={V^L} \cdot E^{\mexp{I}} \cdot {V^R}^\dagger$.} \\
6. & \multicolumn{3}{p{3.3in}}{Extract the normalization constant by computing $\beta:=\sqrt{\tr(B^\dagger\cdot B)}.$} \\
7. & \multicolumn{3}{p{3.3in}}{Obtain the (normalized) expected value by computing $\mexp{O}:=\alpha/\beta$.}
\end{tabular}
%\end{ruledtabular}
\caption{Algorithm to compute $\mexp{O}$ for general matrix product operator $\mathcal{O}$.}
\label{MPO-expectation-algorithm}
\end{table}
%@+node:gcross.20110326130640.1397: *5* Matrix product factorization of exponentially decaying interactions
\subsection{Matrix product factorization of exponentially decaying interactions}
\label{automaton}

In the algorithms of section \ref{algorithms}, we assume that there is a matrix product representation of our Hamiltonian.  It has previously been shown that there are matrix product factorizations of Hamiltonians with short-range interactions.\cite{cond-mat/0701428,caching}  In this section, we extend these results to show that there are also matrix product factorizations of Hamiltonians with long-range exponentially decaying interactions as well.

So consider a Hamiltonian of an $N$-site system experiencing a sum of long-range exponentially decaying interactions,
$$\mathcal{H}:=\sum_{i+1+r+1+j=N} \paren{\sum_n \alpha_n \beta_n^r} \textbf{I}^{\otimes i}\otimes \textbf{X} \otimes \textbf{I}^{\otimes r}\otimes \textbf{X} \otimes \textbf{I}^{\otimes j},$$ where $\textbf{I}$ is the identity operator and $\textbf{X}$ is the Pauli operator $\sigma^X$.  We shall now employ the formalism in Ref. \cite{caching} to obtain a factorization of this Hamiltonian.  We start by changing our interpretation of the Hamiltonian:  rather than thinking of it as a sum of tensor product terms with complex coefficients, we shall instead think of it as a function which maps arbitrary strings of \texttt{X} and \texttt{I} \emph{symbols} to complex numbers.  In our sum each term takes the form $\paren{\sum_n \alpha_n \beta_n^r} \textbf{I}^{\otimes i}\otimes \textbf{X} \otimes \textbf{I}^{\otimes r}\otimes \textbf{X} \otimes \textbf{I}^{\otimes j}$ with $i,j,r \ge 0$, so our corresponding function maps strings of the form $\texttt{I}^i\, \texttt{X}\, \texttt{I}^r\, \texttt{X}\, \texttt{I}^j$ to $\sum_n \alpha_n \beta_n^r$, and all other strings to zero.

We shall now construct a \emph{finite state automaton} which computes this function.  A finite state automaton can be thought of as a machine which reads through an input string and changes its state in response to each input symbol according to a set of transition rules.  The transitions are non-deterministic and weighted -- that is, the automaton can take many transitions simultaneously, and at the end of the string it outputs a number corresponding to the sum of the product of the weights along each sequence of transitions that it took.  The automaton starts on a designated \emph{initial} state;  if it does not end on a designated \emph{accept} state or it encounters a symbol without an associated transition, then it outputs a zero (overriding other weights).

\begin{figure}
\begin{center}
\framebox{\includegraphics[width=4in]{images/diagram-1}}
\caption{Finite state automata representation of a sum of exponentially decaying $XX$ interactions.}
\label{finite-state-automaton}
\end{center}
\end{figure}

The automaton which computes the function representing our operator is illustrated in Fig. \ref{finite-state-automaton}.  States are indicated by circles, and transition rules are indicated by arrows labeled with a symbol and a weight  (1 if not otherwise specified).  An unconnected arrow designates the initial state (1), and shading designates the accept state (2).  A good way to think about what is going on is that terms are generated by each possible walk from state $1$ to state $2$.  So for example, by taking the path $1\!\to\!1\!\to\!3\!\to\!3\!\to\!3\!\to\!2$ we generate the term $(\alpha_1 \beta_1^2) \textbf{I} \otimes \textbf{X} \otimes \textbf{I}\otimes \textbf{I} \otimes \textbf{X}$;  summing over all walks for strings of the form $\texttt{I}\,\texttt{X}\,\texttt{I}^2\texttt{X}$ we obtain the desired coefficient $\sum_n \alpha_n \beta_n^2$.

From this automaton, we immediately obtain a matrix product operator representation.  The initial and accept states give us the values for the left and right boundaries:  $\paren{L^{\mathcal{O}}}_k=\delta_{1,k}$ and $\paren{R^{\mathcal{O}}}_k=\delta_{2,k}$.  The elements of the operator tensor, $O^{\textbf{A}}_{i,j}$, are given by the weight on the $i\to j$ transition with the symbol corresponding to operator $\textbf{A}$ (0 if no such transition exists);  for example, we have that $O^{\textbf{X}}_{1,3}=\alpha_1$.  To get a feeling for why this works, observe that a run of the automaton is equivalent to starting with a vector giving initial weights on the states, multiplying this vector some number of times by a transition matrix, and then dotting the result with a vector that filters out all but the weights on the accept states;  this procedure is exactly equivalent to the form of equation \eqref{matrix-product-operator}.

This process is easily extended to include terms with arbitrary spin coupling interactions, such as $\sigma^X\sigma^Y$, $\sigma^Y\sigma^Y$, $\sigma^Z\sigma^Z$, etc.  Furthermore, one can combine a sum of several such interactions into a single automaton by having them all share the same starting and ending states. 
%@+node:gcross.20110326130640.1398: *5* Case study: Haldane-Shastry Model
\subsection{Results:  Haldane-Shastry Model}

Now we pull all of the ideas from the previous sections together and apply them to tackle the Haldane-Shatry model.\cite{PhysRevLett.60.635,PhysRevLett.60.639}  In the infinite limit this model is given by the Hamiltonian $\mathcal{H} = \sum_i \sum_r \vec{\sigma}_i\cdot\vec{\sigma}_{i+r}/r^2$,  which features an anti-ferromagnetic dipole interaction which falls off with the square of the distance between sites.  (Note that since this model features anti-ferromagnetic interactions, we need to work with blocks of two sites, as discussed at the end of section \ref{main-algorithm}.)  Although $\mathcal{H}$ cannot be expressed exactly as a matrix product operator, we can approximate it arbitrarily well by a sum of exponentially decaying interactions, $\mathcal{H}\approx  \sum_i \sum_r \vec{\sigma}_i\cdot\vec{\sigma}_{i+r}\paren{\sum_n \alpha_n \beta_n^{r-1}}$ (with $\alpha_n\in\mathbb{R}$ and $|\beta_n| \le 1$), which can be factored exactly using the technique in section \ref{automaton}.  Since there are three spin-coupling interactions, $\sigma^X_i\sigma^X_{i+r}$, $\sigma^Y_i\sigma^Y_{i+r}$, and $\sigma^Z_i\sigma^Z_{i+r}$, which we combine into a single automaton as discussed at the end of section \ref{automaton}, we obtain an automaton with a number of states equal to three times the number of terms in the expansion, $N$, plus two more for the starting and ending states;  this quantity gives us the size of the auxiliary dimension for the corresponding matrix product operator, $c=3N+2$.

It remains to find the coefficients in this expansion.  One approach is to numerically solve for the coefficients which minimize the sum of the squares of the difference between the approximation and the exact potential for distances up to some cut-off -- that is, to find the minimizer of the function,
$$f(\alpha_1,\beta_1,\dots,\alpha_N,\beta_N) = \sum_{i=1}^N \sum_{r=1}^{r_{\text{cutoff}}} \paren{\alpha_i \beta_i^{r-1} - \frac{1}{r^2}}^2,$$
where $r_{\text{cutoff}}$ should be chosen to be just beyond the maximum effective range of the approximation, since larger values of $r_{\text{cutoff}}$ result in a longer running time for the minimization without resulting in a better fit.

For our application of the algorithm, we used a nonlinear least-squares minimization routine from \texttt{MINPACK} to find coefficients for expansions with 3, 6, and 9 terms;  the resulting approximate potentials are plotted along side the exact potential in Fig. \ref{expansion}.  The upper cutoff on $r$ was set to 10000 because, as can be seen in Fig. \ref{expansion}, this was just beyond the effective maximum range obtainable from a 9-term approximation.

Given this approximate matrix product factorization, we applied the algorithm in Table \ref{algorithm} to compute a translationally invariant matrix product state representation of the ground state for selected values of $\chi$, employing each of the 3-, 6-, and 9-term expansions.  The energy per site was computed using the algorithm in Table \ref{MPO-expectation-algorithm}, and compared to the exact value obtained from Ref. \cite{PhysRevLett.60.639}.  The difference between these values (i.e., the residual) is plotted for each expansion as a function of $\chi$ in Fig. \ref{residuals}.  Note that the residuals for all three expansions agree up to some point, and then diverge to different ``floors''.  This is because at first the small value of $\chi$ is the dominating factor which limits the fidelity of the ground state, and then later as $\chi$ becomes large the finite number of terms in the exponential approximation becomes the dominating factor.

\begin{figure}
\includegraphics[width=\columnwidth]{images/residuals}
\caption{Difference between the energy of the computed state and the energy of the exact ground state, plotted for each of the three exponential expansions that were employed.}
\label{residuals}
\end{figure}

\begin{figure}
\includegraphics[width=\columnwidth]{images/combined}
\caption{(top) Decaying 3-, 6-, and 9-term exponential approximations to $1/r^2$ potential.  (bottom) Two-point correlator for selected values of $\chi$ using the the 9-term exponential approximation;  a residual of the correlator for each value of $\chi$ is plotted simultaneously and labeled by $\varepsilon_\chi$.}
\label{correlator}
\label{expansion}
\end{figure}

For the 9-term expansion, we also computed the two-point correlator -- that is, $C(r):=\exp{\sigma^X \otimes I^{\otimes (r-1)}\otimes\sigma^X}$ -- using the algorithm given in section \ref{exp-algorithm} for computing the expected value of a local operator.  The result for several values of $\chi$ is plotted in Fig. \ref{correlator} against the exact value from Ref. \cite{PhysRevLett.60.639}.  Note that our approximation gets good agreement up to some length, after which it becomes a constant.  This is because the algorithm is attempting to approximate this correlator using a sum of decaying exponentials plus a constant term, analagously to how we used a sum of decaying exponentials to approximate the $1/r^2$ interactions;  by increasing $\chi$, we are increasing the number of terms available to track the correlator, which results in systematic improvement.
%@+node:gcross.20110318151522.1483: *4* Conclusion
\section{Conclusion}

In this chapter, we have introduced a new type of diagram for representing matrix product states.  We used this to demonstrate that there is a formal equivalence between matrix product states/operators and complex-weighted finite state automata.  This equivalence was used to present a systematic method by which one could factor a matrix product operator.  We then showed how a matrix product factorization of an operator allows one to compute expectations of that operator in $O(N)$ time, and to perform energy minimization at an amortized cost of only $O(1)$ per step.  A generalization of this procedure was presented that allows one to carry the same process through for systems with many dimensions.  A variant of this procedure was presented that allows one to work with infinitely large systems.

As a closing remark, we note that this new formalism is interesting not only because of its practical application in simulating physical systems, but also because it relates our ability to efficiently simulate physical systems with a broader theory (the automata hierarchy/formal language theory) which deals with the fundamental limits of computation.  It would be interesting to see whether there are other insights from this theory that could be used to improve techniques for simulating physical systems.
%@+node:gcross.20110318151522.1487: *3* Practice
\chapter{Practice}
%@+node:gcross.20110331165234.1396: *4* Adiabatic Quantum Computing
\section{Introduction to Adiabatic Quantum Computing}

In the previous chapter we presented an algorithm for simulating finite one-dimensional systems.  In this chapter shall demonstrate how this algorithm can be used to gather important information about physical systems that are important to the field of quantum computing.

Recall that an important consideration when designing a quantum computer is ensuring that the computation is protected from from noise.  Part of the solution to this is to embed the quantum information inside an error-correcting code (as discussed in Part 1 of this thesis), but ideally in addition to performing a continuous error-correction process there would also be some sort of physical barrier that as much as possible prevented errors from occurring in the first place.  For example, one could engineer a physical system whose ground state was a representation of the result of the computation and such that there was a sufficiently large energy gap that it would be difficult for the environment to excite the system out of this state.  This idea, known as \emph{ground state quantum computing}, was first proposed by Ari Mizel, M. W. Mitchell, and Marvin L. Cohen~\cite{PhysRevA.63.040302}.  The down-side of this approach, though, is that it assumes that one can cool down the system down into its ground state, but in general this could be very difficult since many transitions between energy states are forbidden (for example, by conservation of angular momentum) and so it is possible for systems to get stuck in an excited state even when cooled to temperatures far below the excited state energy.

Fortunately, there is a variant on the approach of ground-state quantum computing that solves the problem of cooling, known as \emph{adiabatic quantum computing}.  In this formalism (first introduced by Edward Farhi et al.~\cite{Farhi2000}), one does not start with a Hamiltonian whose ground state represents the result of the compution but rather one starts with a Hamiltonian with the property that it is very easy to cool a system down to the its ground state.   Once the system is in the ground state, one then \emph{adiabatically} (that is, so slowly that the system barely even notices) changes the Hamiltonian into one where the ground state embodies the desired computation;  the Adiabatic Theorem guarantees that if this is done sufficiently slowly then the system will remain in its ground state during this entire process~\cite{JPSJ.5.435}.

The key factor that determines the running time of an adiabatic quantum computation is the size of the gap between the ground and lowest excited states, since this dictates how quickly one can change the Hamiltonian while staying in the adiabatic regime.  In general computing this quantity analytically is very difficult, and this is where the formalism in the previous chapter comes into play.  We have written a code we call \texttt{Nutcracker} which implements the ideas presented in \ref{sec:MPSTheory} in a very efficient manner.  This provides us with a computional tool that we can use to numerically estimate the energy gaps of Hamitlonians and hence evaluate the viability of candidate systems for adiabatic quantum computation.

In this chapter we shall specifically consider two candidates for adiabatic quantum computing that share a similar philosophy in their apprach:  they both contain particles arranged in a one-dimensional chain, and the Hamiltonians are designed such that at the beginning of the computation the information is localized at the left hand of the chain and at the end of the computation the information is localized at the right side of the chain.  That is, as the Hamiltonian is adiabatically transformed from its initial to its final configuration, the quantum information is telported across the system and simultaneously has some number of quantum gates applied to it.  In Section \ref{sec:MizelModel} we shall discuss and numerically analyze the Hamiltonian presented by Mizel in~\cite{Mizel2010}, and in Section \ref{sec:BaconFlammiaModel} we shall discuss and numerically analyze the Hamiltonian presented by Bacon and Flammia in~\cite{Bacon2009}.
%@+node:gcross.20110331165234.1397: *4* Mizel Model
\section{Mizel Model}
\label{sec:MizelModel}

In 2001, Ari Mizel, M. W. Mitchell, and Marvin L. Cohen proposed a model for \emph{ground-state quantum computing}~\cite{PhysRevA.63.040302}.  The basic idea is as follows.  For simplicity, assume that we are modeling a computation consisting of $N$ unitary gates $\{U_1,\dots,U_N\}$ acting on a single qubit in the initial state $\ket{0}$.  We define a Hilbert space with $2(N+1)$ basis vectors which are labeled $\ket{b_t}$ where $t\in\{0\dots N\}$ labels the value of the time and $b\in\{0,1\}$ labels the values of the qubit at time $t$.  Mizel at all demonstrated that there was a Hamiltonian such that the ground state, $\ket{\Psi}$, was given by $$\ket{\Psi}=\sum_{t=0}^N \paren{\prod_{i=1}^{t}[U_i]_i A_{i,i-1}}\ket{0_0},$$ where $[U_i]_i$ is the single-qubit unitary $U_i$ acting non-trivially only on the subspace at time $i$ (that is, $\ket{b_i}$ with $b=0,1$) and $A_{i,j}$ is the time-evolution defined by $A_{i,j} := \ketbra{0_i}{0_j} + \ketbra{1_i}{1_j}$ operator.  $\ket{\Psi}$ is known as a \emph{history} state because it is a superposition over all time steps in a quantum circuit.  Measuring $\ket{\Psi}$ obtains a random time step in the quantum circuit, which in general will \emph{not} be the final state of the circuit;  in fact, the probability of being in the final state of the circuit is only $1/(N+1)$.  This probability can be arbitrarily amplified by adding an arbitrary number of no-op gates to the end of the circuit, but it is still less than ideal.

In 2010, Ari Mizel proposed a modification to this model that added a teleportation step (inspired by \emph{gate-teleportation} first introduced in \cite{Gottesman1999}) to help ensure that each time step actually took place~\cite{Mizel2010}.  To understand how this works, we shall start with a simplified version of the model and then build our way up to the full model.  We start with the model discussed in the previous paragraph with only a single time step --- that is, a Hilbert space with 4 basis vectors which are labeled $\ket{b_t}$ with $b\in\{0,1\}$ and $t\in\{0,1\}$.  Since we want to teleport our quantum information, we need an additional pair of qubits to act as a the entangled pair resource for the teleportation;  hence, we add an additional pair of qubits to our Hilbert space so that it becomes $\mathbb{C}^2\otimes\mathbb{C}^2\otimes\mathbb{C}^4$.  If the teleportation is successful, then the information will have been moved into the leftmost of the three subspaces, and hence after this is done we want the other two spaces to be in a ``null'' state to explicitly indicate that there is no longer any information in them.  Thus, the final Hilbert space for a single gate is $\mathbb{C}^2\otimes\mathbb{C}^3\otimes\mathbb{C}^5\equiv\mathbb{Q}\otimes\mathbb{R}\otimes\mathbb{S}$ where $\mathbb{Q}:=\text{span}\{\ket{0},\ket{1}\}$, $\mathbb{R}:=\text{span}\{\ket{0},\ket{1},\ket{\text{null}}\}$, and $\mathbb{S}:=\text{span}\{\ket{0_0},\ket{1_0},\ket{0_1},\ket{1_1},\ket{\text{null}}\}$;  for $N$ gates, the Hilbert space is $\mathbb{Q}\otimes\paren{\mathbb{R}\otimes\mathbb{S}}^{\otimes N}$.

The Hamiltonian defined by Mizel is $$H(\Lambda) := H_{\text{initialize}} + \sum_{i=1^N}\paren{H_{\text{singlet}}^i + H_{\text{gate}}^i + H_\text{teleport}(\Lambda)}^i,$$ where the terms are defined as follows --- note that we use the notation $\mathbb{R}_i$ and $\mathbb{S}_i$ to denote the $i$th instances of the $\mathbb{R}$ and $\mathbb{S}$ subsystems \emph{from the right}, starting with $i=1$. 
The first term initializes the qubit at time step zero to $\ket{0}$,
$$H_{\text{initialize}} := [\ketbra{1_0}{1_0}]_{\mathbb{S}_0}.$$
The second term forces the teleportation resource state into the Bell singlet state,
$$H_{\text{singlet}}^i := I - \ket{\phi^{-}}^i\bra{\phi^{-}}^i,$$
where
$$\ket{\phi^{-}}^i := \frac{1}{\sqrt{2}}\paren{\ket{0_0}_{\mathbb{S}_{i+1}}\ket{1}_{\mathbb{R}_i}-\ket{1_0}_{\mathbb{S}_{i+1}}\ket{0}_{\mathbb{R}_i}}.$$
The third term applies a gate and advances the time step,
$$\frac{1}{2}\begin{bmatrix}
1 & 0 & -U_{i,00}^* & -U_{i,01}^* & 0\\
0 & 1 & -U_{i,10}^* & -U_{i,11}^* & 0\\
-U_{i,00} & U_{i,10} & 1 & 0 & 0\\
-U_{i,01} & U_{i,11} & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 0\\
\end{bmatrix}_{\mathbb{S}_i},$$
where recall that $\mathbb{S}:=\text{span}\{\ket{0_0},\ket{1_0},\ket{0_1},\ket{1_1},\ket{\text{null}}\}$ is the ordering of the basis vectors.  Finally, the last term teleports the results of applying the gate to the qubit,
$$H_{\text{teleportation}}^i(\Lambda) := \ket{\Lambda}^i\bra{\Lambda}^i + H_{\text{null}}^i,$$
where
$$\ket{\Lambda}^i := \frac{1}{\sqrt{1+\Lambda^2}}\paren{\frac{\Lambda}{\sqrt{2}}(\ket{1_1}_{\mathbb{R}_i}\ket{0}_{\mathbb{S}_i}-\ket{0_1}_{\mathbb{R}_i}\ket{1}_{\mathbb{S}_i})+\ket{\text{null}}_{\mathbb{R}_i}\ket{\text{null}}_{\mathbb{S}_i}}$$
and
$$H_{\text{null}}^i := [\ketbra{\text{null}}{\text{null}}]_{\mathbb{R}_i}\otimes\paren{I-[\ketbra{\text{null}}{\text{null}}]_{\mathbb{S}_i}}+\paren{I-[\ketbra{\text{null}}{\text{null}}]_{\mathbb{R}_i}}\otimes[\ketbra{\text{null}}{\text{null}}]_{\mathbb{S}_i}.$$

It can be shown that the ground state overlaps with a configuration where all of the subsystems save $\mathbb{Q}$ are $\ket{\text{null}}$, and the qubit in $\mathbb{Q}$ is equal to $U_N U_{N-1}\dots U_1\ket{0}$.  Furthermore, this overlap is proportional to $\Lambda$, so the probability of observing it relative to the other configurations can be made arbitrarily large by cranking up $\Lambda$.  Thus, this model can be used for adiabatic quantum computation:  we start with $\Lambda=0$, and then gradually increase $\Lambda$ until we reach some $\Lambda_{\text{max}}$.

Although Mizel provided arguments suggesting that the system had a constant energy gap as a function of $N$, he did not provide concrete evidence.  Thus, I was invited into a collaboration with Steven Flammia to apply my Matrix Product State code to investigate this Hamiltonian numerically.   The encoding of the Hamiltonian into matrix product operator form was relatively straightforwards;  the only additional aspect that was non-trivial was the fact that there were terms that were not expressed as tensor products of single-body matrices, which we easily got around by using singular value decompositions to rewrite them as sums of tensor products of single-body matrices.

\begin{figure}
\includegraphics[width=\columnwidth]{images/mizel}
\caption{Plots of the energy gap for the Mizel model as a function of $\Lambda$ and the number of sites in the system.}
\label{plot:mizel}
\end{figure}

The results are plotted in Figure \ref{plot:mizel}.  They provide strong evidence that the energy gap does indeed asymptotically converge to a constant as the size of the system grows in size.  This was actually somewhat surprising since it is rare to run into this kind of ``good news'' when studying candidates for adiabatic quantum computing.  Unfortunately, these results also show that the energy gap shrinks precipitously as $\Lambda$ increases, which is a large strike against this model.  For this reason and others we eventually decided against putting more resources into studying this particular model.
%@+node:gcross.20110331165234.1398: *4* Bacon-Flammia Model
\section{Bacon-Flammia Model}
\label{sec:BaconFlammiaModel}

In 2001, Robert Raussendorf and Hans J. Briegel introduced a model known as \emph{one-way} or \emph{measurement-based} quantum computation in which a quantum computation is implemented by preparing a system in a special state and then applying a series of measurements~\cite{PhysRevLett.86.5188}.  The advantage of this model over others (such as the standard ``gate'' model) is that state preparation and measurements are the only physical control operations that are needed.  In 2009, Dave Bacon and Steven T. Flammia showed that even the measurement operations in this model were not strictly necessary by demonstrating that one could replace the state preparation with a step that cooled the system into the ground state of an initial Hamiltonian and the sequence of measurements with a sequence of adiabatic transitions that changed the Hamiltonian term by term until it took a specific form~\cite{Bacon2009}.

Specifically what they showed was as followed.  Assume we have a one-dimensional chain of $n$ physical qubits.  Define the following sequence of Hamiltonians $\{H_i\}_{i=0\dots n-1}$ by
$$H_i := -\Delta\sum_{j=1}^{i} [X]_j - \Delta\sum_{j=i+1}^{n-1} S_j$$
where
$$S_j :=
\begin{cases}
[Z]_j\left[e^{-i\theta_jZ/2}Xe^{+i\theta_jZ/2}\right]_{j+1}[Z]_{j+2} & 1 \le j \le n-2 \\
[Z]_j\left[e^{-i\theta_jZ/2}Xe^{+i\theta_jZ/2}\right]_{j+1} & j = n-1 \\
\end{cases}
$$
and $\{\theta_j\}_{j=1\dots n-1}$ are arbitrarily chosen real parameters.  Assume that we have a qubit initialized in some state $\ket{\psi}$ encoded in this chain such that its logical operators are given by $\bar X = [X]_1[Z]_2$ and $\bar Z = [Z]_1$.  Then if we perform a sequence of stepwise adiabatic transitions, $H_0\to H_1\to H_2\to \cdots \to H_{n-1}$, then at time step $t$ the encoded qubit will be in the state $\paren{\prod_{i=1}^te^{-i\theta_iZ/2}H}\ket{\psi}$ and its logical operators will be given by $\bar X = [X]_{t+1}[Z]_{t+2}$ and $\bar Z = [Z]_{t+1}$.  Because the choice of $\{\theta_i\}_i$ is arbitrary, this scheme allows for universal computation on a single qubit.  It is possible to also perform universal computation on multiple qubits using this scheme, but we will not discuss this here;  see \cite{Bacon2009} for the details.

The disadvantage of the scheme just discussed is that it requires a great deal of precise engineering in order perform the requires sequence of local changes to the individual terms of the Hamiltonian.  It would be better if one could instead apply a global control that changed all of the terms at once.  It is not too hard to show that doing so has the same computational effect on the input qubit\cite{TheFuture}, but it is not immediately obvious that the energy gap scales polynomially with the size of the system.  In order to determine whether this is the case, we applied my Matrix Product State code to investigate this model numerically.  We took advantage of the fact that when the choice of the parameters is mirror symmetric about the center of the system --- that is, $\theta_1 = \theta_{n-1}$, $\theta_2=\theta_{n-2}$, etc. --- then the minimum energy gap occurs at the center of the adiabatic transition (as proven in \cite{TheFuture}), and so this is the only point that we needed to examine in our simulations.  For selected values of $n$, we generated 200 random choices of $\{\theta_i\}_{i=1\dots n-1}$, subject to the mirror symmetry condition just discussed.  The results are shown in Figure \ref{plot:qtrans}.  They provide strong numerical evidence that the gap scales inversely with the size of the system, which means that amount of time needed for the adiabatic transition scales only linearly with the size of the system.  This result implies that this model is indeed a viable means of performing adiabatic quantum computation.

\begin{figure}
\includegraphics[width=\columnwidth]{images/qtrans}
\caption{
\label{plot:qtrans}
For selected system sizes we generated 200 random choices of $\{\theta\}_i$ and computed the energy gap.  Above are plotted the minimum, maximum, average, and standard deviation of these data for each system size;  note that the plot uses a log-log scale.  Also plotted is the exact solution for the ``non-twisted'' (i.e., $\forall_i\theta_i=0$) case, as well as the polynomial $4.97n^{-1}$ obtained from a least-squares fit to the data.}
\end{figure}
%@+node:gcross.20110331165234.1399: ** Conclusion
\part{Conclusion}

Quantum computing has the potential to open new frontiers to humanity by unlocking problems that are currently intractible.  However, in order for this potential to be fulfilled we need to be able to engineer systems that can perform quantum computations reliably.  In this thesis we have discussed the theory and practice behind two computation tools that have been created to advance this end.  In Part \ref{part:CodeQuest} we discussed \texttt{CodeQuest}, which assists us in designing error-correcting codes for shielding quantum information from errors, and in Part \ref{part:Matrix Product States} we discussed \emph{matrix product states}, which assist us in designing Hamiltonians with have strong energetic barriers to provide protection against nose.  Althose these two tools are very different in the particular problems that they solve and the techniques used to solve them, they are united in their ultimate goal of advancing our ability to engineer robust systems for quantum computing.

Although we do not yet have quantum computers available to help us solve the engineering difficulties we face in building them, we do have powerful classical computers that we can use to bootstrap our way there.  The ultimate end of the specific techniques advanced in this thesis is to make themselves obselete by enabing the construction of the first quantum computer, which will open the door to the use of quantum algorithms that will hopefully be far more powerful for solving quantum engineering challanges than the classical algorithms we have just discussed.
%@+node:gcross.20110316123044.1278: ** Bibliography
\bibliography{thesis}
\bibliographystyle{plain}
%@-others

\end{document}
%@-leo
